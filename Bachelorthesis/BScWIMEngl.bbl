% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.0 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\datalist[entry]{nyt/global//global/global}
  \entry{borkarMethodConvergenceStochastic2000}{article}{}
    \name{author}{2}{}{%
      {{hash=BV}{%
         family={Borkar},
         familyi={B\bibinitperiod},
         given={V.},
         giveni={V\bibinitperiod},
      }}%
      {{hash=MS}{%
         family={Meyn},
         familyi={M\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
    }
    \strng{namehash}{BVMS1}
    \strng{fullhash}{BVMS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2000}
    \field{labeldatesource}{year}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    It is shown here that stability of the stochastic approximation algorithm
  is implied by the asymptotic stability of the origin for an associated ODE.
  This in turn implies convergence of the algorithm. Several specific classes
  of algorithms are considered as applications. It is found that the results
  provide (i) a simpler derivation of known results for reinforcement learning
  algorithms; (ii) a proof for the first time that a class of asynchronous
  stochastic approximation algorithms are convergent without using any a priori
  assumption of stability; (iii) a proof for the first time that asynchronous
  adaptive critic and Q-learning algorithms are convergent for the average cost
  optimal control problem.%
    }
    \verb{doi}
    \verb 10.1137/S0363012997331639
    \endverb
    \field{issn}{0363-0129}
    \field{number}{2}
    \field{pages}{447\bibrangedash 469}
    \field{shortjournal}{SIAM J. Control Optim.}
    \field{title}{The {{O}}.{{D}}.{{E}}. {{Method}} for {{Convergence}} of
  {{Stochastic Approximation}} and {{Reinforcement Learning}}}
    \field{volume}{38}
    \verb{file}
    \verb C:\\Users\\felix\\Zotero\\storage\\VZ5IS6XX\\Borkar and Meyn - 2000 -
    \verb  The O.D.E. Method for Convergence of Stochastic Ap.pdf;C:\\Users\\fe
    \verb lix\\Zotero\\storage\\6TP8FI9Z\\S0363012997331639.html
    \endverb
    \field{journaltitle}{SIAM Journal on Control and Optimization}
    \field{day}{01}
    \field{month}{01}
    \field{year}{2000}
  \endentry

  \entry{deardenBayesianQLearning1998}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=DR}{%
         family={Dearden},
         familyi={D\bibinitperiod},
         given={Richard},
         giveni={R\bibinitperiod},
      }}%
      {{hash=FN}{%
         family={Friedman},
         familyi={F\bibinitperiod},
         given={Nir},
         giveni={N\bibinitperiod},
      }}%
      {{hash=RS}{%
         family={Russell},
         familyi={R\bibinitperiod},
         given={Stuart},
         giveni={S\bibinitperiod},
      }}%
    }
    \strng{namehash}{DR+1}
    \strng{fullhash}{DRFNRS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1998}
    \field{labeldatesource}{year}
    \field{sortinit}{D}
    \field{sortinithash}{D}
    \field{abstract}{%
    A central problem in learning in complex environments is balancing
  exploration of untested actions against exploitation of actions that are
  known to be good. The beneﬁt of exploration can be estimated using the
  classical notion of Value of Information—the expected improvement in future
  decision quality that might arise from the information acquired by
  exploration. Estimating this quantity requires an assessment of the agent’s
  uncertainty about its current value estimates for states. In this paper, we
  adopt a Bayesian approach to maintaining this uncertain information. We
  extend Watkins’ Q-learning by maintaining and propagating probability
  distributions over the Q-values. These distributions are used to compute a
  myopic approximation to the value of information for each action and hence to
  select the action that best balances exploration and exploitation. We
  establish the convergence properties of our algorithm and show experimentally
  that it can exhibit substantial improvements over other well-known model-free
  exploration strategies.%
    }
    \field{booktitle}{Proceedings of the Fifteenth National Conference on
  Artificial Intelligence ({{AAAI}}-98)}
    \field{pages}{8}
    \field{title}{Bayesian {{Q}}-{{Learning}}}
    \field{langid}{english}
    \verb{file}
    \verb D:\\Google Drive\\ZotFiles\\1998Dearden et al\\Dearden et al_1998_Bay
    \verb esian Q-Learning.pdf
    \endverb
    \field{year}{1998}
  \endentry

  \entry{dvoretzkyStochasticApproximation1956}{report}{}
    \name{author}{1}{}{%
      {{hash=DA}{%
         family={Dvoretzky},
         familyi={D\bibinitperiod},
         given={Aryeh},
         giveni={A\bibinitperiod},
      }}%
    }
    \strng{namehash}{DA1}
    \strng{fullhash}{DA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1956}
    \field{labeldatesource}{year}
    \field{sortinit}{D}
    \field{sortinithash}{D}
    \field{abstract}{%
    Stochastic approximation is concerned with schemes converging to some
  sought valuewhen, due to the stochastic nature of the problem, the
  observations involve errors. Theinteresting schemes are those which are
  self-correcting, that is, in which a mistake alwaystends to be wiped out in
  the limit, and in which the convergence to the desired value isof some
  specified nature, for example, it is mean-square convergence. The typical
  exampleof such a scheme is the original one of Robbins-Monro [7] for
  approximating, undersuitable conditions, the point where a regression
  function assumes a given value. Robbinsand Monro have proved mean-square
  convergence to the root; Wolfowitz [8] showedthat under weaker assumptions
  there is still convergence in probability to the root; andBlum [11
  demonstrated that, under still weaker assumptions, there is not only
  convergencein probability but even convergence with probability 1. Kiefer and
  Wolfowitz [6]have devised a method for approximating the point where the
  maximum of a regressionfunction occurs. They proved that under suitable
  conditions there is convergence in probabilityand Blum [1] has weakened
  somewhat the conditions and strengthened the conclusionto convergence with
  probability 1.%
    }
    \field{title}{On {{Stochastic Approximation}}}
    \verb{url}
    \verb https://apps.dtic.mil/docs/citations/AD1028378
    \endverb
    \field{langid}{english}
    \list{institution}{1}{%
      {{Columbia University, New York City, United States}}%
    }
    \verb{file}
    \verb D:\\Google Drive\\ZotFiles\\1956Dvoretzky\\Dvoretzky_1956_On Stochast
    \verb ic Approximation.pdf;C:\\Users\\felix\\Zotero\\storage\\XCBWQQJ7\\AD1
    \verb 028378.html
    \endverb
    \field{day}{01}
    \field{month}{01}
    \field{year}{1956}
    \field{urlday}{16}
    \field{urlmonth}{03}
    \field{urlyear}{2019}
  \endentry

  \entry{jaakkolaConvergenceStochasticIterative1994a}{article}{}
    \name{author}{3}{}{%
      {{hash=JT}{%
         family={Jaakkola},
         familyi={J\bibinitperiod},
         given={Tommi},
         giveni={T\bibinitperiod},
      }}%
      {{hash=JMI}{%
         family={Jordan},
         familyi={J\bibinitperiod},
         given={Michael\bibnamedelima I.},
         giveni={M\bibinitperiod\bibinitdelim I\bibinitperiod},
      }}%
      {{hash=SSP}{%
         family={Singh},
         familyi={S\bibinitperiod},
         given={Satinder\bibnamedelima P.},
         giveni={S\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
    }
    \strng{namehash}{JT+1}
    \strng{fullhash}{JTJMISSP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1994}
    \field{labeldatesource}{year}
    \field{sortinit}{J}
    \field{sortinithash}{J}
    \field{abstract}{%
    Recent developments in the area of reinforcement learning have yielded a
  number of new algorithms for the prediction and control of Markovian
  environments. These algorithms, including the TD(λ) algorithm of Sutton
  (1988) and the Q-learning algorithm of Watkins (1989), can be motivated
  heuristically as approximations to dynamic programming (DP). In this paper we
  provide a rigorous proof of convergence of these DP-based learning algorithms
  by relating them to the powerful techniques of stochastic approximation
  theory via a new convergence theorem. The theorem establishes a general class
  of convergent algorithms to which both TD(λ) and Q-learning belong.%
    }
    \verb{doi}
    \verb 10.1162/neco.1994.6.6.1185
    \endverb
    \field{issn}{0899-7667}
    \field{number}{6}
    \field{pages}{1185\bibrangedash 1201}
    \field{shortjournal}{Neural Computation}
    \field{title}{On the {{Convergence}} of {{Stochastic Iterative Dynamic
  Programming Algorithms}}}
    \field{volume}{6}
    \verb{file}
    \verb D:\\Google Drive\\ZotFiles\\1994Jaakkola et al\\Jaakkola et al_1994_O
    \verb n the Convergence of Stochastic Iterative Dynamic Programming Algorit
    \verb hms.pdf;C:\\Users\\felix\\Zotero\\storage\\U3ZKMDIZ\\neco.1994.6.6.ht
    \verb ml
    \endverb
    \field{journaltitle}{Neural Computation}
    \field{day}{01}
    \field{month}{11}
    \field{year}{1994}
  \endentry

  \entry{kaelblingLearningEmbeddedSystems1993}{book}{}
    \name{author}{1}{}{%
      {{hash=KLP}{%
         family={Kaelbling},
         familyi={K\bibinitperiod},
         given={Leslie\bibnamedelima Pack},
         giveni={L\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {{MIT press}}%
    }
    \strng{namehash}{KLP1}
    \strng{fullhash}{KLP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1993}
    \field{labeldatesource}{year}
    \field{sortinit}{K}
    \field{sortinithash}{K}
    \field{title}{Learning in Embedded Systems}
    \verb{file}
    \verb D:\\Google Drive\\ZotFiles\\1993Kaelbling\\Kaelbling_1993_Learning in
    \verb  embedded systems2.pdf;C:\\Users\\felix\\Zotero\\storage\\D7YDIQSR\\b
    \verb ooks.html
    \endverb
    \field{annotation}{%
    Interval Learning: ~ Confidence Intervall around Q value -{$>$} Exploration
  bonus%
    }
    \field{year}{1993}
  \endentry

  \entry{kushnerStochasticApproximationAlgorithms1997}{book}{}
    \name{author}{1}{}{%
      {{hash=KHJ}{%
         family={Kushner},
         familyi={K\bibinitperiod},
         given={Harold\bibnamedelima J.},
         giveni={H\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
    }
    \name{editora}{1}{}{%
      {{hash=YG}{%
         family={Yin},
         familyi={Y\bibinitperiod},
         given={George},
         giveni={G\bibinitperiod},
      }}%
    }
    \field{editoratype}{collaborator}
    \list{publisher}{1}{%
      {{u.a. Springer}}%
    }
    \keyw{Stochastic approximation,Stochastic approximation; Stochastic
  approximation,Stochastische Approximation}
    \strng{namehash}{KHJ1}
    \strng{fullhash}{KHJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1997}
    \field{labeldatesource}{year}
    \field{sortinit}{K}
    \field{sortinithash}{K}
    \field{isbn}{978-0-387-94916-1}
    \field{pagetotal}{xxi+417}
    \field{series}{Applications of Mathematics; 35}
    \field{title}{Stochastic Approximation Algorithms and Applications}
    \field{langid}{english}
    \list{location}{1}{%
      {{Berlin Heidelberg [u.a.]}}%
    }
    \field{year}{1997}
  \endentry

  \entry{langeBatchReinforcementLearning2012}{incollection}{useprefix}
    \name{author}{3}{}{%
      {{hash=LS}{%
         family={Lange},
         familyi={L\bibinitperiod},
         given={Sascha},
         giveni={S\bibinitperiod},
      }}%
      {{hash=GT}{%
         family={Gabel},
         familyi={G\bibinitperiod},
         given={Thomas},
         giveni={T\bibinitperiod},
      }}%
      {{hash=RM}{%
         family={Riedmiller},
         familyi={R\bibinitperiod},
         given={Martin},
         giveni={M\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=WM}{%
         family={Wiering},
         familyi={W\bibinitperiod},
         given={Marco},
         giveni={M\bibinitperiod},
      }}%
      {{hash=vOM}{%
         prefix={van},
         prefixi={v\bibinitperiod},
         family={Otterlo},
         familyi={O\bibinitperiod},
         given={Martijn},
         giveni={M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {{Springer Berlin Heidelberg}}%
    }
    \keyw{Batch Learning,Multiagent System,Neural Information Processing
  System,Policy Iteration,Reinforcement Learning}
    \strng{namehash}{LS+1}
    \strng{fullhash}{LSGTRM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2012}
    \field{labeldatesource}{year}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    Batch reinforcement learning is a subfield of dynamic programming-based
  reinforcement learning. Originally defined as the task of learning the best
  possible policy from a fixed set of a priori-known transition samples, the
  (batch) algorithms developed in this field can be easily adapted to the
  classical online case, where the agent interacts with the environment while
  learning. Due to the efficient use of collected data and the stability of the
  learning process, this research area has attracted a lot of attention
  recently. In this chapter, we introduce the basic principles and the theory
  behind batch reinforcement learning, describe the most important algorithms,
  exemplarily discuss ongoing research within this field, and briefly survey
  real-world applications of batch reinforcement learning.%
    }
    \field{booktitle}{Reinforcement {{Learning}}: {{State}}-of-the-{{Art}}}
    \verb{doi}
    \verb 10.1007/978-3-642-27645-3_2
    \endverb
    \field{isbn}{978-3-642-27645-3}
    \field{pages}{45\bibrangedash 73}
    \field{series}{Adaptation, {{Learning}}, and {{Optimization}}}
    \field{title}{Batch {{Reinforcement Learning}}}
    \verb{url}
    \verb https://doi.org/10.1007/978-3-642-27645-3_2
    \endverb
    \field{langid}{english}
    \list{location}{1}{%
      {{Berlin, Heidelberg}}%
    }
    \verb{file}
    \verb D:\\Google Drive\\ZotFiles\\2012Lange et al\\Lange et al_2012_Batch R
    \verb einforcement Learning.pdf
    \endverb
    \field{year}{2012}
    \field{urlday}{22}
    \field{urlmonth}{02}
    \field{urlyear}{2019}
  \endentry

  \entry{linSelfimprovingReactiveAgents1992}{article}{}
    \name{author}{1}{}{%
      {{hash=LLJ}{%
         family={Lin},
         familyi={L\bibinitperiod},
         given={Long-Ji},
         giveni={L\bibinithyphendelim J\bibinitperiod},
      }}%
    }
    \keyw{Reinforcement learning,connectionist networks,planning,teaching}
    \strng{namehash}{LLJ1}
    \strng{fullhash}{LLJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1992}
    \field{labeldatesource}{year}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    To date, reinforcement learning has mostly been studied solving simple
  learning tasks. Reinforcement learning methods that have been studied so far
  typically converge slowly. The purpose of this work is thus two-fold: 1) to
  investigate the utility of reinforcement learning in solving much more
  complicated learning tasks than previously studied, and 2) to investigate
  methods that will speed up reinforcement learning.This paper compares eight
  reinforcement learning frameworks:adaptive heuristic critic (AHC) learning
  due to Sutton,Q-learning due to Watkins, and three extensions to both basic
  methods for speeding up learning. The three extensions are experience replay,
  learning action models for planning, and teaching. The frameworks were
  investigated using connectionism as an approach to generalization. To
  evaluate the performance of different frameworks, a dynamic environment was
  used as a testbed. The environment is moderately complex and
  nondeterministic. This paper describes these frameworks and algorithms in
  detail and presents empirical evaluation of the frameworks.%
    }
    \verb{doi}
    \verb 10.1007/BF00992699
    \endverb
    \field{issn}{1573-0565}
    \field{number}{3}
    \field{pages}{293\bibrangedash 321}
    \field{shortjournal}{Mach Learn}
    \field{title}{Self-Improving Reactive Agents Based on Reinforcement
  Learning, Planning and Teaching}
    \field{volume}{8}
    \field{langid}{english}
    \verb{file}
    \verb D:\\Google Drive\\ZotFiles\\1992Lin\\Lin_1992_Self-improving reactive
    \verb  agents based on reinforcement learning, planning and.pdf
    \endverb
    \field{journaltitle}{Machine Learning}
    \field{day}{01}
    \field{month}{05}
    \field{year}{1992}
  \endentry

  \entry{pathakCuriosityDrivenExplorationSelfSupervised2017}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=PD}{%
         family={Pathak},
         familyi={P\bibinitperiod},
         given={Deepak},
         giveni={D\bibinitperiod},
      }}%
      {{hash=AP}{%
         family={Agrawal},
         familyi={A\bibinitperiod},
         given={Pulkit},
         giveni={P\bibinitperiod},
      }}%
      {{hash=EAA}{%
         family={Efros},
         familyi={E\bibinitperiod},
         given={Alexei\bibnamedelima A.},
         giveni={A\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=DT}{%
         family={Darrell},
         familyi={D\bibinitperiod},
         given={Trevor},
         giveni={T\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {{IEEE}}%
    }
    \strng{namehash}{PD+1}
    \strng{fullhash}{PDAPEAADT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2017}
    \field{labeldatesource}{year}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{abstract}{%
    In many real-world scenarios, rewards extrinsic to the agent are extremely
  sparse, or absent altogether. In such cases, curiosity can serve as an
  intrinsic reward signal to enable the agent to explore its environment and
  learn skills that might be useful later in its life. We formulate curiosity
  as the error in an agent’s ability to predict the consequence of its own
  actions in a visual feature space learned by a self-supervised inverse
  dynamics model. Our formulation scales to high-dimensional continuous state
  spaces like images, bypasses the difﬁculties of directly predicting pixels,
  and, critically, ignores the aspects of the environment that cannot affect
  the agent. The proposed approach is evaluated in two environments: VizDoom
  and Super Mario Bros. Three broad settings are investigated: 1) sparse
  extrinsic reward, where curiosity allows for far fewer interactions with the
  environment to reach the goal; 2) exploration with no extrinsic reward, where
  curiosity pushes the agent to explore more efﬁciently; and 3)
  generalization to unseen scenarios (e.g. new levels of the same game) where
  the knowledge gained from earlier experience helps the agent explore new
  places much faster than starting from scratch.%
    }
    \field{booktitle}{2017 {{IEEE Conference}} on {{Computer Vision}} and
  {{Pattern Recognition Workshops}} ({{CVPRW}})}
    \verb{doi}
    \verb 10.1109/CVPRW.2017.70
    \endverb
    \field{eventtitle}{2017 {{IEEE Conference}} on {{Computer Vision}} and
  {{Pattern Recognition Workshops}} ({{CVPRW}})}
    \field{isbn}{978-1-5386-0733-6}
    \field{pages}{488\bibrangedash 489}
    \field{title}{Curiosity-{{Driven Exploration}} by {{Self}}-{{Supervised
  Prediction}}}
    \field{langid}{english}
    \list{location}{1}{%
      {{Honolulu, HI, USA}}%
    }
    \verb{file}
    \verb D:\\Google Drive\\ZotFiles\\2017Pathak et al\\Pathak et al_2017_Curio
    \verb sity-Driven Exploration by Self-Supervised Prediction.pdf
    \endverb
    \field{month}{07}
    \field{year}{2017}
  \endentry

  \entry{pengIncrementalMultiStepQLearning1994}{incollection}{}
    \name{author}{2}{}{%
      {{hash=PJ}{%
         family={Peng},
         familyi={P\bibinitperiod},
         given={Jing},
         giveni={J\bibinitperiod},
      }}%
      {{hash=WRJ}{%
         family={Williams},
         familyi={W\bibinitperiod},
         given={Ronald\bibnamedelima J.},
         giveni={R\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=CWW}{%
         family={Cohen},
         familyi={C\bibinitperiod},
         given={William\bibnamedelima W.},
         giveni={W\bibinitperiod\bibinitdelim W\bibinitperiod},
      }}%
      {{hash=HH}{%
         family={Hirsh},
         familyi={H\bibinitperiod},
         given={Haym},
         giveni={H\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {{Morgan Kaufmann}}%
    }
    \strng{namehash}{PJWRJ1}
    \strng{fullhash}{PJWRJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1994}
    \field{labeldatesource}{year}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{abstract}{%
    This paper presents a novel incremental algorithm that combines Q-learning,
  a well-known dynamic programming-based reinforcement learning method, with
  the TD(A) return estimation process, which is typically used in actor-critic
  learning, another well-known dynamic programming-based reinforcement learning
  method. The parameter A is used to distribute credit throughout sequences of
  actions, leading to faster learning and also helping to alleviate the
  non-Markovian effect of coarse state-space quantization. The resulting
  algorithm, Q(λ)-learning, thus combines some of the best features of the
  Q-learning and actor-critic learning paradigms. The behavior of this
  algorithm is demonstrated through computer simulations of the standard
  benchmark control problem of learning to balance a pole on a cart.%
    }
    \field{booktitle}{Machine {{Learning Proceedings}} 1994}
    \verb{doi}
    \verb 10.1016/B978-1-55860-335-6.50035-0
    \endverb
    \field{isbn}{978-1-55860-335-6}
    \field{pages}{226\bibrangedash 232}
    \field{title}{Incremental {{Multi}}-{{Step Q}}-{{Learning}}}
    \verb{url}
    \verb http://www.sciencedirect.com/science/article/pii/B9781558603356500350
    \endverb
    \list{location}{1}{%
      {{San Francisco (CA)}}%
    }
    \verb{file}
    \verb C:\\Users\\felix\\Zotero\\storage\\MVDKEDD6\\B9781558603356500350.htm
    \verb l
    \endverb
    \field{day}{01}
    \field{month}{01}
    \field{year}{1994}
    \field{urlday}{28}
    \field{urlmonth}{04}
    \field{urlyear}{2019}
  \endentry

  \entry{putermanMarkovDecisionProcesses2005}{book}{}
    \name{author}{1}{}{%
      {{hash=PML}{%
         family={Puterman},
         familyi={P\bibinitperiod},
         given={Martin\bibnamedelima L.},
         giveni={M\bibinitperiod\bibinitdelim L\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {{Wiley-Interscience}}%
    }
    \strng{namehash}{PML1}
    \strng{fullhash}{PML1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelyear}{2005}
    \field{labeldatesource}{year}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{isbn}{978-0-471-72782-8}
    \field{note}{OCLC: 254152847}
    \field{pagetotal}{649}
    \field{series}{Wiley Series in Probability and Statistics}
    \field{shorttitle}{Markov Decision Processes}
    \field{title}{Markov {{Decision Processes}}: {{Discrete Stochastic Dynamic
  Programming}}}
    \field{langid}{english}
    \list{location}{1}{%
      {{Hoboken, NJ}}%
    }
    \verb{file}
    \verb C:\\Users\\felix\\Zotero\\storage\\VNB8FB4L\\Puterman - 2005 - Markov
    \verb  decision processes discrete stochastic dyn.pdf
    \endverb
    \field{year}{2005}
  \endentry

  \entry{robbinsStochasticApproximationMethod1951}{article}{}
    \name{author}{2}{}{%
      {{hash=RH}{%
         family={Robbins},
         familyi={R\bibinitperiod},
         given={Herbert},
         giveni={H\bibinitperiod},
      }}%
      {{hash=MS}{%
         family={Monro},
         familyi={M\bibinitperiod},
         given={Sutton},
         giveni={S\bibinitperiod},
      }}%
    }
    \strng{namehash}{RHMS1}
    \strng{fullhash}{RHMS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1951}
    \field{labeldatesource}{year}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \field{abstract}{%
    [Let M(x) denote the expected value at level x of the response to a certain
  experiment. M(x) is assumed to be a monotone function of x but is unknown to
  the experimenter, and it is desired to find the solution x = θ of the
  equation M(x) = α, where α is a given constant. We give a method for making
  successive experiments at levels x1,x2,⋯ in such a way that xn will tend to
  θ in probability.]%
    }
    \field{issn}{0003-4851}
    \field{number}{3}
    \field{pages}{400\bibrangedash 407}
    \field{title}{A {{Stochastic Approximation Method}}}
    \verb{url}
    \verb https://www.jstor.org/stable/2236626
    \endverb
    \field{volume}{22}
    \verb{file}
    \verb C:\\Users\\felix\\Zotero\\storage\\4CSGQDU7\\Robbins and Monro - 1951
    \verb  - A Stochastic Approximation Method.pdf
    \endverb
    \field{journaltitle}{The Annals of Mathematical Statistics}
    \field{year}{1951}
    \field{urlday}{27}
    \field{urlmonth}{02}
    \field{urlyear}{2019}
  \endentry

  \entry{singhReinforcementLearningReplacing1996}{article}{}
    \name{author}{2}{}{%
      {{hash=SSP}{%
         family={Singh},
         familyi={S\bibinitperiod},
         given={Satinder\bibnamedelima P.},
         giveni={S\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
      {{hash=SRS}{%
         family={Sutton},
         familyi={S\bibinitperiod},
         given={Richard\bibnamedelima S.},
         giveni={R\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
    }
    \keyw{CMAC,eligibility trace,Markov chain,Monte Carlo method,reinforcement
  learning,temporal difference learning}
    \strng{namehash}{SSPSRS1}
    \strng{fullhash}{SSPSRS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1996}
    \field{labeldatesource}{year}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    The eligibility trace is one of the basic mechanisms used in reinforcement
  learning to handle delayed reward. In this paper we introduce a new kind of
  eligibility trace, the replacing trace, analyze it theoretically, and show
  that it results in faster, more reliable learning than the conventional
  trace. Both kinds of trace assign credit to prior events according to how
  recently they occurred, but only the conventional trace gives greater credit
  to repeated events. Our analysis is for conventional and replace-trace
  versions of the offline TD(1) algorithm applied to undiscounted absorbing
  Markov chains. First, we show that these methods converge under repeated
  presentations of the training set to the same predictions as two well known
  Monte Carlo methods. We then analyze the relative efficiency of the two Monte
  Carlo methods. We show that the method corresponding to conventional TD is
  biased, whereas the method corresponding to replace-trace TD is unbiased. In
  addition, we show that the method corresponding to replacing traces is
  closely related to the maximum likelihood solution for these tasks, and that
  its mean squared error is always lower in the long run. Computational results
  confirm these analyses and show that they are applicable more generally. In
  particular, we show that replacing traces significantly improve performance
  and reduce parameter sensitivity on the "Mountain-Car" task, a full
  reinforcement-learning problem with a continuous state space, when using a
  feature-based function approximator.%
    }
    \verb{doi}
    \verb 10.1023/A:1018012322525
    \endverb
    \field{issn}{1573-0565}
    \field{number}{1}
    \field{pages}{123\bibrangedash 158}
    \field{shortjournal}{Machine Learning}
    \field{title}{Reinforcement {{Learning}} with {{Replacing Eligibility
  Traces}}}
    \field{volume}{22}
    \field{langid}{english}
    \verb{file}
    \verb D:\\Google Drive\\ZotFiles\\1996Singh_Sutton\\Singh_Sutton_1996_Reinf
    \verb orcement Learning with Replacing Eligibility Traces.pdf
    \endverb
    \field{journaltitle}{Machine Learning}
    \field{day}{01}
    \field{month}{01}
    \field{year}{1996}
  \endentry

  \entry{strehlAnalysisModelbasedInterval2008}{article}{}
    \name{author}{2}{}{%
      {{hash=SAL}{%
         family={Strehl},
         familyi={S\bibinitperiod},
         given={Alexander\bibnamedelima L.},
         giveni={A\bibinitperiod\bibinitdelim L\bibinitperiod},
      }}%
      {{hash=LML}{%
         family={Littman},
         familyi={L\bibinitperiod},
         given={Michael\bibnamedelima L.},
         giveni={M\bibinitperiod\bibinitdelim L\bibinitperiod},
      }}%
    }
    \keyw{Reinforcement learning,Learning theory,Markov Decision Processes}
    \strng{namehash}{SALLML1}
    \strng{fullhash}{SALLML1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2008}
    \field{labeldatesource}{year}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    Several algorithms for learning near-optimal policies in Markov Decision
  Processes have been analyzed and proven efficient. Empirical results have
  suggested that Model-based Interval Estimation (MBIE) learns efficiently in
  practice, effectively balancing exploration and exploitation. This paper
  presents a theoretical analysis of MBIE and a new variation called MBIE-EB,
  proving their efficiency even under worst-case conditions. The paper also
  introduces a new performance metric, average loss, and relates it to its less
  “online” cousins from the literature.%
    }
    \verb{doi}
    \verb 10.1016/j.jcss.2007.08.009
    \endverb
    \field{issn}{0022-0000}
    \field{number}{8}
    \field{pages}{1309\bibrangedash 1331}
    \field{series}{Learning {{Theory}} 2005}
    \field{shortjournal}{Journal of Computer and System Sciences}
    \field{title}{An Analysis of Model-Based {{Interval Estimation}} for
  {{Markov Decision Processes}}}
    \field{volume}{74}
    \verb{file}
    \verb D:\\Google Drive\\ZotFiles\\2008Strehl_Littman\\Strehl_Littman_2008_A
    \verb n analysis of model-based Interval Estimation for Markov Decision Pro
    \verb cesses.pdf;C:\\Users\\felix\\Zotero\\storage\\722XY68T\\S002200000800
    \verb 0767.html
    \endverb
    \field{journaltitle}{Journal of Computer and System Sciences}
    \field{day}{01}
    \field{month}{12}
    \field{year}{2008}
  \endentry

  \entry{suttonLearningPredictMethods1988}{article}{}
    \name{author}{1}{}{%
      {{hash=SRS}{%
         family={Sutton},
         familyi={S\bibinitperiod},
         given={Richard\bibnamedelima S.},
         giveni={R\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
    }
    \keyw{connectionism,credit assignment,evaluation functions,Incremental
  learning,prediction}
    \strng{namehash}{SRS1}
    \strng{fullhash}{SRS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1988}
    \field{labeldatesource}{year}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    This article introduces a class of incremental learning procedures
  specialized for prediction-that is, for using past experience with an
  incompletely known system to predict its future behavior. Whereas
  conventional prediction-learning methods assign credit by means of the
  difference between predicted and actual outcomes, the new methods assign
  credit by means of the difference between temporally successive predictions.
  Although such temporal-difference methods have been used in Samuel's checker
  player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic,
  they have remained poorly understood. Here we prove their convergence and
  optimality for special cases and relate them to supervised-learning methods.
  For most real-world prediction problems, temporal-difference methods require
  less memory and less peak computation than conventional methods and they
  produce more accurate predictions. We argue that most problems to which
  supervised learning is currently applied are really prediction problems of
  the sort to which temporal-difference methods can be applied to advantage.%
    }
    \verb{doi}
    \verb 10.1007/BF00115009
    \endverb
    \field{issn}{1573-0565}
    \field{number}{1}
    \field{pages}{9\bibrangedash 44}
    \field{shortjournal}{Mach Learn}
    \field{title}{Learning to Predict by the Methods of Temporal Differences}
    \field{volume}{3}
    \field{langid}{english}
    \verb{file}
    \verb D:\\Google Drive\\ZotFiles\\1988Sutton\\Sutton_1988_Learning to predi
    \verb ct by the methods of temporal differences.pdf
    \endverb
    \field{journaltitle}{Machine Learning}
    \field{day}{01}
    \field{month}{08}
    \field{year}{1988}
  \endentry

  \entry{suttonReinforcementLearningIntroduction1998}{book}{}
    \name{author}{2}{}{%
      {{hash=SRS}{%
         family={Sutton},
         familyi={S\bibinitperiod},
         given={Richard\bibnamedelima S.},
         giveni={R\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
      {{hash=BAG}{%
         family={Barto},
         familyi={B\bibinitperiod},
         given={Andrew\bibnamedelima G.},
         giveni={A\bibinitperiod\bibinitdelim G\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {{MIT Press}}%
    }
    \keyw{Reinforcement learning,Lernen,Reinforcement learning (Machine
  learning),Bestärkendes Lernen,Operantes Lernen,Reinforcement learning
  (Machine learning); Reinforcement learning (Machine learning),Verstärkendes
  Lernen,Verstärkungslernen}
    \strng{namehash}{SRSBAG1}
    \strng{fullhash}{SRSBAG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelyear}{1998}
    \field{labeldatesource}{year}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{isbn}{978-0-262-19398-6}
    \field{pagetotal}{xviii+322}
    \field{series}{Adaptive Computation and Machine Learning}
    \field{shorttitle}{Reinforcement Learning}
    \field{title}{Reinforcement Learning: An Introduction}
    \field{langid}{english}
    \list{location}{1}{%
      {{Cambridge, Mass. [u.a.]}}%
    }
    \field{year}{1998}
  \endentry

  \entry{suttonReinforcementLearningIntroduction2018a}{book}{}
    \name{author}{2}{}{%
      {{hash=SRS}{%
         family={Sutton},
         familyi={S\bibinitperiod},
         given={Richard\bibnamedelima S.},
         giveni={R\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
      {{hash=BAG}{%
         family={Barto},
         familyi={B\bibinitperiod},
         given={Andrew\bibnamedelima G.},
         giveni={A\bibinitperiod\bibinitdelim G\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {{The MIT Press}}%
    }
    \keyw{Reinforcement learning}
    \strng{namehash}{SRSBAG1}
    \strng{fullhash}{SRSBAG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelyear}{2018}
    \field{labeldatesource}{year}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    "Reinforcement learning, one of the most active research areas in
  artificial intelligence, is a computational approach to learning whereby an
  agent tries to maximize the total amount of reward it receives while
  interacting with a complex, uncertain environment. In Reinforcement Learning,
  Richard Sutton and Andrew Barto provide a clear and simple account of the
  field's key ideas and algorithms."--%
    }
    \field{edition}{Second edition}
    \field{isbn}{978-0-262-03924-6}
    \field{pagetotal}{526}
    \field{series}{Adaptive Computation and Machine Learning Series}
    \field{shorttitle}{Reinforcement Learning}
    \field{title}{Reinforcement Learning: An Introduction}
    \field{langid}{english}
    \list{location}{1}{%
      {{Cambridge, Massachusetts}}%
    }
    \verb{file}
    \verb D:\\Google Drive\\ZotFiles\\2018Sutton_Barto\\Sutton_Barto_2018_Reinf
    \verb orcement learning.pdf
    \endverb
    \field{year}{2018}
  \endentry

  \entry{szepesvariAlgorithmsReinforcementLearning2010}{article}{}
    \name{author}{1}{}{%
      {{hash=SC}{%
         family={Szepesvári},
         familyi={S\bibinitperiod},
         given={Csaba},
         giveni={C\bibinitperiod},
      }}%
    }
    \strng{namehash}{SC1}
    \strng{fullhash}{SC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2010}
    \field{labeldatesource}{year}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \verb{doi}
    \verb 10.2200/S00268ED1V01Y201005AIM009
    \endverb
    \field{issn}{1939-4608}
    \field{number}{1}
    \field{pages}{1\bibrangedash 103}
    \field{shortjournal}{Synthesis Lectures on Artificial Intelligence and
  Machine Learning}
    \field{title}{Algorithms for {{Reinforcement Learning}}}
    \field{volume}{4}
    \verb{file}
    \verb C:\\Users\\felix\\Zotero\\storage\\9BG3XKG8\\Szepesvári - 2010 - Alg
    \verb orithms for Reinforcement Learning.pdf;C:\\Users\\felix\\Zotero\\stor
    \verb age\\XFRVYKZ3\\Algorithms_ReinforcementLearning.pdf;C:\\Users\\felix\
    \verb \Zotero\\storage\\3HQTSX4N\\S00268ED1V01Y201005AIM009.html
    \endverb
    \field{journaltitle}{Synthesis Lectures on Artificial Intelligence and
  Machine Learning}
    \field{day}{01}
    \field{month}{01}
    \field{year}{2010}
  \endentry

  \entry{tsitsiklisAnalysisTemporaldifferenceLearning1997}{article}{}
    \name{author}{2}{}{%
      {{hash=TJN}{%
         family={Tsitsiklis},
         familyi={T\bibinitperiod},
         given={J.\bibnamedelima N.},
         giveni={J\bibinitperiod\bibinitdelim N\bibinitperiod},
      }}%
      {{hash=RBV}{%
         family={Roy},
         familyi={R\bibinitperiod},
         given={B.\bibnamedelima Van},
         giveni={B\bibinitperiod\bibinitdelim V\bibinitperiod},
      }}%
    }
    \keyw{Algorithm design and analysis,Approximation algorithms,Approximation
  error,convergence,Convergence,Cost function,cost-to-go function,Dynamic
  programming,Error analysis,finite state space,function approximation,Function
  approximation,infinite state space,infinite-horizon discounted Markov
  chain,irreducible aperiodic Markov chain,learning (artificial
  intelligence),Linear approximation,linear function approximation,Markov
  processes,nonlinear function approximators,State-space
  methods,temporal-difference learning}
    \strng{namehash}{TJNRBV1}
    \strng{fullhash}{TJNRBV1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1997}
    \field{labeldatesource}{year}
    \field{sortinit}{T}
    \field{sortinithash}{T}
    \field{abstract}{%
    We discuss the temporal-difference learning algorithm, as applied to
  approximating the cost-to-go function of an infinite-horizon discounted
  Markov chain. The algorithm we analyze updates parameters of a linear
  function approximator online during a single endless trajectory of an
  irreducible aperiodic Markov chain with a finite or infinite state space. We
  present a proof of convergence (with probability one), a characterization of
  the limit of convergence, and a bound on the resulting approximation error.
  Furthermore, our analysis is based on a new line of reasoning that provides
  new intuition about the dynamics of temporal-difference learning. In addition
  to proving new and stronger positive results than those previously available,
  we identify the significance of online updating and potential hazards
  associated with the use of nonlinear function approximators. First, we prove
  that divergence may occur when updates are not based on trajectories of the
  Markov chain. This fact reconciles positive and negative results that have
  been discussed in the literature, regarding the soundness of
  temporal-difference learning. Second, we present an example illustrating the
  possibility of divergence when temporal difference learning is used in the
  presence of a nonlinear function approximator.%
    }
    \verb{doi}
    \verb 10.1109/9.580874
    \endverb
    \field{issn}{0018-9286}
    \field{number}{5}
    \field{pages}{674\bibrangedash 690}
    \field{title}{An Analysis of Temporal-Difference Learning with Function
  Approximation}
    \field{volume}{42}
    \verb{file}
    \verb D:\\Google Drive\\ZotFiles\\1997Tsitsiklis_Roy\\Tsitsiklis_Roy_1997_A
    \verb n analysis of temporal-difference learning with function approximatio
    \verb n.pdf;C:\\Users\\felix\\Zotero\\storage\\URLNN84W\\580874.html
    \endverb
    \field{journaltitle}{IEEE Transactions on Automatic Control}
    \field{month}{05}
    \field{year}{1997}
  \endentry

  \entry{tsitsiklisAsynchronousStochasticApproximation1994}{article}{}
    \name{author}{1}{}{%
      {{hash=TJN}{%
         family={Tsitsiklis},
         familyi={T\bibinitperiod},
         given={John\bibnamedelima N.},
         giveni={J\bibinitperiod\bibinitdelim N\bibinitperiod},
      }}%
    }
    \keyw{Reinforcement learning,dynamic programming,Q-learning,stochastic
  approximation}
    \strng{namehash}{TJN1}
    \strng{fullhash}{TJN1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1994}
    \field{labeldatesource}{year}
    \field{sortinit}{T}
    \field{sortinithash}{T}
    \field{abstract}{%
    We provide some general results on the convergence of a class of stochastic
  approximation algorithms and their parallel and asynchronous variants. We
  then use these results to study the Q-learning algorithm, a reinforcement
  learning method for solving Markov decision problems, and establish its
  convergence under conditions more general than previously available.%
    }
    \verb{doi}
    \verb 10.1007/BF00993306
    \endverb
    \field{issn}{1573-0565}
    \field{number}{3}
    \field{pages}{185\bibrangedash 202}
    \field{shortjournal}{Mach Learn}
    \field{title}{Asynchronous Stochastic Approximation and {{Q}}-Learning}
    \field{volume}{16}
    \field{langid}{english}
    \verb{file}
    \verb D:\\Google Drive\\ZotFiles\\1994Tsitsiklis\\Tsitsiklis_1994_Asynchron
    \verb ous stochastic approximation and Q-learning.pdf
    \endverb
    \field{journaltitle}{Machine Learning}
    \field{day}{01}
    \field{month}{09}
    \field{year}{1994}
  \endentry

  \entry{vanseijenTrueOnlineTD2014}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=VSH}{%
         family={Van\bibnamedelima Seijen},
         familyi={V\bibinitperiod\bibinitdelim S\bibinitperiod},
         given={Harm},
         giveni={H\bibinitperiod},
      }}%
      {{hash=SRS}{%
         family={Sutton},
         familyi={S\bibinitperiod},
         given={Richard\bibnamedelima S.},
         giveni={R\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {{JMLR.org}}%
    }
    \strng{namehash}{VSHSRS1}
    \strng{fullhash}{VSHSRS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2014}
    \field{labeldatesource}{year}
    \field{sortinit}{V}
    \field{sortinithash}{V}
    \field{abstract}{%
    TD(λ) is a core algorithm of modern reinforcement learning. Its appeal
  comes from its equivalence to a clear and conceptually simple forward view,
  and the fact that it can be implemented online in an inexpensive manner.
  However, the equivalence between TD(λ) and the forward view is exact only
  for the off-line version of the algorithm (in which updates are made only at
  the end of each episode). In the online version of TD(λ) (in which updates
  are made at each step, which generally performs better and is always used in
  applications) the match to the forward view is only approximate. In a sense
  this is unavoidable for the conventional forward view, as it itself presumes
  that the estimates are unchanging during an episode. In this paper we
  introduce a new forward view that takes into account the possibility of
  changing estimates and a new variant of TD(λ) that exactlyachieves it. Our
  algorithm uses a new form of eligibility trace similar to but different from
  conventional accumulating and replacing traces. The overall computational
  complexity is the same as TD(λ), even when using function approximation. In
  our empirical comparisons, our algorithm outperformed TD(λ) in all of its
  variations. It seems, by adhering more truly to the original goal of
  TD(λ)--matching an intuitively clear forward view even in the online
  case--that we have found a new algorithm that simply improves on classical
  TD(λ).%
    }
    \field{booktitle}{Proceedings of the 31st {{International Conference}} on
  {{Machine Learning}} - {{Volume}} 32}
    \field{pages}{I\bibrangedash 692\bibrangedash I\bibrangedash 700}
    \field{series}{{{ICML}}'14}
    \field{title}{True {{Online TD}}(\$\textbackslash{}lambda\$)}
    \verb{url}
    \verb http://dl.acm.org/citation.cfm?id=3044805.3044884
    \endverb
    \field{venue}{Beijing, China}
    \verb{file}
    \verb D:\\Google Drive\\ZotFiles\\2014Van Seijen_Sutton\\Van Seijen_Sutton_
    \verb 2014_True Online TD($-lambda$).pdf
    \endverb
    \field{year}{2014}
    \field{urlday}{08}
    \field{urlmonth}{03}
    \field{urlyear}{2019}
  \endentry

  \entry{watkinsLearningDelayedRewards1989}{thesis}{}
    \name{author}{1}{}{%
      {{hash=WCJCH}{%
         family={Watkins},
         familyi={W\bibinitperiod},
         given={Christopher J. C.\bibnamedelima H.},
         giveni={C\bibinitperiod\bibinitdelim J\bibinitperiod\bibinitdelim
  C\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
    }
    \strng{namehash}{WCJCH1}
    \strng{fullhash}{WCJCH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1989}
    \field{labeldatesource}{year}
    \field{sortinit}{W}
    \field{sortinithash}{W}
    \field{title}{Learning from Delayed Rewards}
    \list{institution}{1}{%
      {{King's College, Cambridge}}%
    }
    \verb{file}
    \verb D:\\Google Drive\\ZotFiles\\1989Watkins\\Watkins_1989_Learning from d
    \verb elayed rewards.pdf
    \endverb
    \field{type}{PhD Thesis}
    \field{year}{1989}
  \endentry

  \entry{watkinsQlearning1992}{article}{}
    \name{author}{2}{}{%
      {{hash=WCJCH}{%
         family={Watkins},
         familyi={W\bibinitperiod},
         given={Christopher J. C.\bibnamedelima H.},
         giveni={C\bibinitperiod\bibinitdelim J\bibinitperiod\bibinitdelim
  C\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
      {{hash=DP}{%
         family={Dayan},
         familyi={D\bibinitperiod},
         given={Peter},
         giveni={P\bibinitperiod},
      }}%
    }
    \strng{namehash}{WCJCHDP1}
    \strng{fullhash}{WCJCHDP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1992}
    \field{labeldatesource}{year}
    \field{sortinit}{W}
    \field{sortinithash}{W}
    \field{abstract}{%
    Q-learning (Watkins, 1989) is a simple way for agents to learn how to act
  optimally in controlled Markovian domains. It amounts to an incremental
  method for dynamic programming which imposes limited...%
    }
    \verb{doi}
    \verb 10.1007/BF00992698
    \endverb
    \field{issn}{0885-6125, 1573-0565}
    \field{number}{3-4}
    \field{pages}{279\bibrangedash 292}
    \field{shortjournal}{Mach Learn}
    \field{title}{Q-Learning}
    \field{volume}{8}
    \field{langid}{english}
    \verb{file}
    \verb C:\\Users\\felix\\Zotero\\storage\\PF3CU2VT\\Q-learning SpringerLink.
    \verb pdf;C:\\Users\\felix\\Zotero\\storage\\WCA8JCF4\\Watkins and Dayan -
    \verb 1992 - Emphasis Type=ItalicQEmphasis-learning.pdf;C:\\Users\\felix\\Z
    \verb otero\\storage\\22WQBT9Y\\BF00992698.html
    \endverb
    \field{journaltitle}{Machine Learning}
    \field{day}{01}
    \field{month}{05}
    \field{year}{1992}
  \endentry

  \entry{whiteRealApplicationsMarkov1985}{article}{}
    \name{author}{1}{}{%
      {{hash=WDJ}{%
         family={White},
         familyi={W\bibinitperiod},
         given={Douglas\bibnamedelima J.},
         giveni={D\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
    }
    \strng{namehash}{WDJ1}
    \strng{fullhash}{WDJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1985}
    \field{labeldatesource}{year}
    \field{sortinit}{W}
    \field{sortinithash}{W}
    \verb{doi}
    \verb 10.1287/inte.15.6.73
    \endverb
    \field{issn}{0092-2102, 1526-551X}
    \field{number}{6}
    \field{pages}{73\bibrangedash 83}
    \field{title}{Real {{Applications}} of {{Markov Decision Processes}}}
    \field{volume}{15}
    \field{langid}{english}
    \verb{file}
    \verb C:\\Users\\felix\\Zotero\\storage\\DWX2M69R\\White - 1985 - Real Appl
    \verb ications of Markov Decision Processes.pdf
    \endverb
    \field{journaltitle}{Interfaces}
    \field{month}{12}
    \field{year}{1985}
  \endentry

  \entry{wieringEfficientModelbasedExploration1998}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=WM}{%
         family={Wiering},
         familyi={W\bibinitperiod},
         given={Marco},
         giveni={M\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Schmidhuber},
         familyi={S\bibinitperiod},
         given={Jürgen},
         giveni={J\bibinitperiod},
      }}%
    }
    \strng{namehash}{WMSJ1}
    \strng{fullhash}{WMSJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1998}
    \field{labeldatesource}{year}
    \field{sortinit}{W}
    \field{sortinithash}{W}
    \field{booktitle}{Proceedings of the {{Sixth International Conference}} on
  {{Simulation}} of {{Adaptive Behavior}}: {{From Animals}} to {{Animats}}}
    \field{pages}{223\bibrangedash 228}
    \field{title}{Efficient Model-Based Exploration}
    \field{volume}{6}
    \verb{file}
    \verb D:\\Google Drive\\ZotFiles\\1998Wiering_Schmidhuber\\Wiering_Schmidhu
    \verb ber_1998_Efficient model-based exploration.pdf;C:\\Users\\felix\\Zote
    \verb ro\\storage\\VKDPQRCB\\books.html
    \endverb
    \field{year}{1998}
  \endentry
\enddatalist
\endinput
