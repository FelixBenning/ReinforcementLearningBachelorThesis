% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.0 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nyt/global//global/global}
    \entry{borkarMethodConvergenceStochastic2000}{article}{}
      \name{author}{2}{}{%
        {{uniquename=0,uniquepart=base,hash=3cc6211f904ecb5c60f4a2faa50b6be3}{%
           family={Borkar},
           familyi={B\bibinitperiod},
           given={V.},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{uniquename=0,uniquepart=base,hash=88bb7b430bf616aa1f184701f54cf744}{%
           family={Meyn},
           familyi={M\bibinitperiod},
           given={S.},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{dff4e182e1177bd64d8456c0a4ce58f2}
      \strng{fullhash}{dff4e182e1177bd64d8456c0a4ce58f2}
      \strng{bibnamehash}{dff4e182e1177bd64d8456c0a4ce58f2}
      \strng{authorbibnamehash}{dff4e182e1177bd64d8456c0a4ce58f2}
      \strng{authornamehash}{dff4e182e1177bd64d8456c0a4ce58f2}
      \strng{authorfullhash}{dff4e182e1177bd64d8456c0a4ce58f2}
      \field{sortinit}{B}
      \field{sortinithash}{276475738cc058478c1677046f857703}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{It is shown here that stability of the stochastic approximation algorithm is implied by the asymptotic stability of the origin for an associated ODE. This in turn implies convergence of the algorithm. Several specific classes of algorithms are considered as applications. It is found that the results provide (i) a simpler derivation of known results for reinforcement learning algorithms; (ii) a proof for the first time that a class of asynchronous stochastic approximation algorithms are convergent without using any a priori assumption of stability; (iii) a proof for the first time that asynchronous adaptive critic and Q-learning algorithms are convergent for the average cost optimal control problem.}
      \field{day}{1}
      \field{issn}{0363-0129}
      \field{journaltitle}{SIAM Journal on Control and Optimization}
      \field{month}{1}
      \field{number}{2}
      \field{shortjournal}{SIAM J. Control Optim.}
      \field{title}{The {{O}}.{{D}}.{{E}}. {{Method}} for {{Convergence}} of {{Stochastic Approximation}} and {{Reinforcement Learning}}}
      \field{volume}{38}
      \field{year}{2000}
      \field{dateera}{ce}
      \field{pages}{447\bibrangedash 469}
      \range{pages}{23}
      \verb{doi}
      \verb 10.1137/S0363012997331639
      \endverb
      \verb{file}
      \verb C:\\Users\\felix\\Zotero\\storage\\VZ5IS6XX\\Borkar and Meyn - 2000 - The O.D.E. Method for Convergence of Stochastic Ap.pdf;C:\\Users\\felix\\Zotero\\storage\\6TP8FI9Z\\S0363012997331639.html
      \endverb
    \endentry
    \entry{deardenBayesianQLearning1998}{article}{}
      \name{author}{3}{}{%
        {{uniquename=0,uniquepart=base,hash=5eac58f71e450d96418f25079668603b}{%
           family={Dearden},
           familyi={D\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{uniquename=0,uniquepart=base,hash=987ee418ae32e261a8a5385e14cda479}{%
           family={Friedman},
           familyi={F\bibinitperiod},
           given={Nir},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{uniquename=0,uniquepart=base,hash=143fa183327d9fcd9de18eec99d6ca97}{%
           family={Russell},
           familyi={R\bibinitperiod},
           given={Stuart},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{6d310e17699a36ec6e6f2aaf556d6f97}
      \strng{fullhash}{80450527277e2a29451d686bb0e0d879}
      \strng{bibnamehash}{80450527277e2a29451d686bb0e0d879}
      \strng{authorbibnamehash}{80450527277e2a29451d686bb0e0d879}
      \strng{authornamehash}{6d310e17699a36ec6e6f2aaf556d6f97}
      \strng{authorfullhash}{80450527277e2a29451d686bb0e0d879}
      \field{sortinit}{D}
      \field{sortinithash}{2ef1bd9a78cc71eb74d7231c635177b8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A central problem in learning in complex environments is balancing exploration of untested actions against exploitation of actions that are known to be good. The beneﬁt of exploration can be estimated using the classical notion of Value of Information—the expected improvement in future decision quality that might arise from the information acquired by exploration. Estimating this quantity requires an assessment of the agent’s uncertainty about its current value estimates for states. In this paper, we adopt a Bayesian approach to maintaining this uncertain information. We extend Watkins’ Q-learning by maintaining and propagating probability distributions over the Q-values. These distributions are used to compute a myopic approximation to the value of information for each action and hence to select the action that best balances exploration and exploitation. We establish the convergence properties of our algorithm and show experimentally that it can exhibit substantial improvements over other well-known model-free exploration strategies.}
      \field{langid}{english}
      \field{title}{Bayesian {{Q}}-{{Learning}}}
      \field{year}{1998}
      \field{dateera}{ce}
      \field{pages}{8}
      \range{pages}{1}
      \verb{file}
      \verb D:\\Google Drive\\ZotFiles\\1998Dearden et al\\Dearden et al_1998_Bayesian Q-Learning.pdf
      \endverb
    \endentry
    \entry{dvoretzkyStochasticApproximation1956}{report}{}
      \name{author}{1}{}{%
        {{uniquename=0,uniquepart=base,hash=4b78c338b8672f0d52378eb8b42c11a8}{%
           family={Dvoretzky},
           familyi={D\bibinitperiod},
           given={Aryeh},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \list{institution}{1}{%
        {Columbia University, New York City, United States}%
      }
      \strng{namehash}{4b78c338b8672f0d52378eb8b42c11a8}
      \strng{fullhash}{4b78c338b8672f0d52378eb8b42c11a8}
      \strng{bibnamehash}{4b78c338b8672f0d52378eb8b42c11a8}
      \strng{authorbibnamehash}{4b78c338b8672f0d52378eb8b42c11a8}
      \strng{authornamehash}{4b78c338b8672f0d52378eb8b42c11a8}
      \strng{authorfullhash}{4b78c338b8672f0d52378eb8b42c11a8}
      \field{sortinit}{D}
      \field{sortinithash}{2ef1bd9a78cc71eb74d7231c635177b8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Stochastic approximation is concerned with schemes converging to some sought valuewhen, due to the stochastic nature of the problem, the observations involve errors. Theinteresting schemes are those which are self-correcting, that is, in which a mistake alwaystends to be wiped out in the limit, and in which the convergence to the desired value isof some specified nature, for example, it is mean-square convergence. The typical exampleof such a scheme is the original one of Robbins-Monro [7] for approximating, undersuitable conditions, the point where a regression function assumes a given value. Robbinsand Monro have proved mean-square convergence to the root; Wolfowitz [8] showedthat under weaker assumptions there is still convergence in probability to the root; andBlum [11 demonstrated that, under still weaker assumptions, there is not only convergencein probability but even convergence with probability 1. Kiefer and Wolfowitz [6]have devised a method for approximating the point where the maximum of a regressionfunction occurs. They proved that under suitable conditions there is convergence in probabilityand Blum [1] has weakened somewhat the conditions and strengthened the conclusionto convergence with probability 1.}
      \field{day}{1}
      \field{langid}{english}
      \field{month}{1}
      \field{title}{On {{Stochastic Approximation}}}
      \field{urlday}{16}
      \field{urlmonth}{3}
      \field{urlyear}{2019}
      \field{year}{1956}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb D:\\Google Drive\\ZotFiles\\1956Dvoretzky\\Dvoretzky_1956_On Stochastic Approximation.pdf;C:\\Users\\felix\\Zotero\\storage\\XCBWQQJ7\\AD1028378.html
      \endverb
      \verb{urlraw}
      \verb https://apps.dtic.mil/docs/citations/AD1028378
      \endverb
      \verb{url}
      \verb https://apps.dtic.mil/docs/citations/AD1028378
      \endverb
    \endentry
    \entry{jaakkolaConvergenceStochasticIterative1994}{incollection}{}
      \name{author}{3}{}{%
        {{uniquename=0,uniquepart=base,hash=521085652a07e67bdff5323e2febc138}{%
           family={Jaakkola},
           familyi={J\bibinitperiod},
           given={Tommi},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{uniquename=0,uniquepart=base,hash=8a36116840c7ee55901618c95fd08a58}{%
           family={Jordan},
           familyi={J\bibinitperiod},
           given={Michael\bibnamedelima I.},
           giveni={M\bibinitperiod\bibinitdelim I\bibinitperiod},
           givenun=0}}%
        {{uniquename=0,uniquepart=base,hash=0778520109d58859c98d71f3b7e75837}{%
           family={Singh},
           familyi={S\bibinitperiod},
           given={Satinder\bibnamedelima P.},
           giveni={S\bibinitperiod\bibinitdelim P\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{3}{}{%
        {{hash=4f26380d9d1d486f010a79c26655f578}{%
           family={Cowan},
           familyi={C\bibinitperiod},
           given={J.\bibnamedelimi D.},
           giveni={J\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=e99ea83bf0b2aa481e8339676e479945}{%
           family={Tesauro},
           familyi={T\bibinitperiod},
           given={G.},
           giveni={G\bibinitperiod}}}%
        {{hash=2c8f544b6de560764fbe2cf8df6d4d3a}{%
           family={Alspector},
           familyi={A\bibinitperiod},
           given={J.},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Morgan-Kaufmann}%
      }
      \strng{namehash}{c147aea00d57d8906308e81e27de9993}
      \strng{fullhash}{c472cf2b2187f20a7da30bcef9ceab5d}
      \strng{bibnamehash}{c472cf2b2187f20a7da30bcef9ceab5d}
      \strng{authorbibnamehash}{c472cf2b2187f20a7da30bcef9ceab5d}
      \strng{authornamehash}{c147aea00d57d8906308e81e27de9993}
      \strng{authorfullhash}{c472cf2b2187f20a7da30bcef9ceab5d}
      \strng{editorbibnamehash}{81369fca149846346458bacd250f0ea6}
      \strng{editornamehash}{c3b95e24d31fc0e844d1d3d948049ee9}
      \strng{editorfullhash}{81369fca149846346458bacd250f0ea6}
      \field{extraname}{1}
      \field{sortinit}{J}
      \field{sortinithash}{fce5f8d0bd05e8d93f3dbe21c78897ca}
      \field{extradate}{1}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}} 6}
      \field{title}{Convergence of {{Stochastic Iterative Dynamic Programming Algorithms}}}
      \field{urlday}{26}
      \field{urlmonth}{2}
      \field{urlyear}{2019}
      \field{year}{1994}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{703\bibrangedash 710}
      \range{pages}{8}
      \verb{file}
      \verb D:\\Google Drive\\ZotFiles\\1994Jaakkola et al\\Jaakkola et al_1994_Convergence of Stochastic Iterative Dynamic Programming Algorithms.pdf;C:\\Users\\felix\\Zotero\\storage\\PK2RJG45\\764-convergence-of-stochastic-iterative-dynamic-programming-algorithms.html
      \endverb
      \verb{urlraw}
      \verb http://papers.nips.cc/paper/764-convergence-of-stochastic-iterative-dynamic-programming-algorithms.pdf
      \endverb
      \verb{url}
      \verb http://papers.nips.cc/paper/764-convergence-of-stochastic-iterative-dynamic-programming-algorithms.pdf
      \endverb
    \endentry
    \entry{jaakkolaConvergenceStochasticIterative1994a}{article}{}
      \name{author}{3}{}{%
        {{uniquename=0,uniquepart=base,hash=521085652a07e67bdff5323e2febc138}{%
           family={Jaakkola},
           familyi={J\bibinitperiod},
           given={Tommi},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{uniquename=0,uniquepart=base,hash=8a36116840c7ee55901618c95fd08a58}{%
           family={Jordan},
           familyi={J\bibinitperiod},
           given={Michael\bibnamedelima I.},
           giveni={M\bibinitperiod\bibinitdelim I\bibinitperiod},
           givenun=0}}%
        {{uniquename=0,uniquepart=base,hash=0778520109d58859c98d71f3b7e75837}{%
           family={Singh},
           familyi={S\bibinitperiod},
           given={Satinder\bibnamedelima P.},
           giveni={S\bibinitperiod\bibinitdelim P\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{c147aea00d57d8906308e81e27de9993}
      \strng{fullhash}{c472cf2b2187f20a7da30bcef9ceab5d}
      \strng{bibnamehash}{c472cf2b2187f20a7da30bcef9ceab5d}
      \strng{authorbibnamehash}{c472cf2b2187f20a7da30bcef9ceab5d}
      \strng{authornamehash}{c147aea00d57d8906308e81e27de9993}
      \strng{authorfullhash}{c472cf2b2187f20a7da30bcef9ceab5d}
      \field{extraname}{2}
      \field{sortinit}{J}
      \field{sortinithash}{fce5f8d0bd05e8d93f3dbe21c78897ca}
      \field{extradate}{2}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent developments in the area of reinforcement learning have yielded a number of new algorithms for the prediction and control of Markovian environments. These algorithms, including the TD(λ) algorithm of Sutton (1988) and the Q-learning algorithm of Watkins (1989), can be motivated heuristically as approximations to dynamic programming (DP). In this paper we provide a rigorous proof of convergence of these DP-based learning algorithms by relating them to the powerful techniques of stochastic approximation theory via a new convergence theorem. The theorem establishes a general class of convergent algorithms to which both TD(λ) and Q-learning belong.}
      \field{day}{1}
      \field{issn}{0899-7667}
      \field{journaltitle}{Neural Computation}
      \field{month}{11}
      \field{number}{6}
      \field{shortjournal}{Neural Computation}
      \field{title}{On the {{Convergence}} of {{Stochastic Iterative Dynamic Programming Algorithms}}}
      \field{volume}{6}
      \field{year}{1994}
      \field{dateera}{ce}
      \field{pages}{1185\bibrangedash 1201}
      \range{pages}{17}
      \verb{doi}
      \verb 10.1162/neco.1994.6.6.1185
      \endverb
      \verb{file}
      \verb D:\\Google Drive\\ZotFiles\\1994Jaakkola et al\\Jaakkola et al_1994_On the Convergence of Stochastic Iterative Dynamic Programming Algorithms.pdf;C:\\Users\\felix\\Zotero\\storage\\U3ZKMDIZ\\neco.1994.6.6.html
      \endverb
    \endentry
    \entry{kaelblingLearningEmbeddedSystems1993}{book}{}
      \name{author}{1}{}{%
        {{uniquename=0,uniquepart=base,hash=72fdc7bf3120e1cff12e3ecf7e892b9a}{%
           family={Kaelbling},
           familyi={K\bibinitperiod},
           given={Leslie\bibnamedelima Pack},
           giveni={L\bibinitperiod\bibinitdelim P\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {MIT press}%
      }
      \strng{namehash}{72fdc7bf3120e1cff12e3ecf7e892b9a}
      \strng{fullhash}{72fdc7bf3120e1cff12e3ecf7e892b9a}
      \strng{bibnamehash}{72fdc7bf3120e1cff12e3ecf7e892b9a}
      \strng{authorbibnamehash}{72fdc7bf3120e1cff12e3ecf7e892b9a}
      \strng{authornamehash}{72fdc7bf3120e1cff12e3ecf7e892b9a}
      \strng{authorfullhash}{72fdc7bf3120e1cff12e3ecf7e892b9a}
      \field{sortinit}{K}
      \field{sortinithash}{9fd838a31ba64d981e8f44562bd33f89}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{annotation}{Interval Learning: ~ Confidence Intervall around Q value -{$>$} Exploration bonus}
      \field{title}{Learning in Embedded Systems}
      \field{year}{1993}
      \field{dateera}{ce}
      \verb{file}
      \verb D:\\Google Drive\\ZotFiles\\1993Kaelbling\\Kaelbling_1993_Learning in embedded systems2.pdf;C:\\Users\\felix\\Zotero\\storage\\D7YDIQSR\\books.html
      \endverb
    \endentry
    \entry{kushnerStochasticApproximationAlgorithms1997}{book}{}
      \name{author}{1}{}{%
        {{uniquename=0,uniquepart=base,hash=1ed29ca8d2b16f090f280df6bab6e0f6}{%
           family={Kushner},
           familyi={K\bibinitperiod},
           given={Harold\bibnamedelima J.},
           giveni={H\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
      }
      \name{editora}{1}{}{%
        {{hash=bb039264a668968d0116bd91a9c250e1}{%
           family={Yin},
           familyi={Y\bibinitperiod},
           given={George},
           giveni={G\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Berlin Heidelberg [u.a.}%
      }
      \list{publisher}{1}{%
        {Berlin Heidelberg uaSpringer}%
      }
      \strng{namehash}{1ed29ca8d2b16f090f280df6bab6e0f6}
      \strng{fullhash}{1ed29ca8d2b16f090f280df6bab6e0f6}
      \strng{bibnamehash}{1ed29ca8d2b16f090f280df6bab6e0f6}
      \strng{authorbibnamehash}{1ed29ca8d2b16f090f280df6bab6e0f6}
      \strng{authornamehash}{1ed29ca8d2b16f090f280df6bab6e0f6}
      \strng{authorfullhash}{1ed29ca8d2b16f090f280df6bab6e0f6}
      \strng{editorabibnamehash}{bb039264a668968d0116bd91a9c250e1}
      \strng{editoranamehash}{bb039264a668968d0116bd91a9c250e1}
      \strng{editorafullhash}{bb039264a668968d0116bd91a9c250e1}
      \field{sortinit}{K}
      \field{sortinithash}{9fd838a31ba64d981e8f44562bd33f89}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{editoratype}{collaborator}
      \field{isbn}{978-0-387-94916-1}
      \field{langid}{english}
      \field{pagetotal}{xxi+417}
      \field{series}{Applications of Mathematics; 35}
      \field{title}{Stochastic Approximation Algorithms and Applications}
      \field{year}{1997}
      \field{dateera}{ce}
      \keyw{Stochastic approximation,Stochastic approximation; Stochastic approximation,Stochastische Approximation}
    \endentry
    \entry{langeBatchReinforcementLearning2012}{incollection}{useprefix=true}
      \name{author}{3}{}{%
        {{uniquename=0,uniquepart=base,hash=eb0c3521aaad8bb21bb052214ae15910}{%
           family={Lange},
           familyi={L\bibinitperiod},
           given={Sascha},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{uniquename=0,uniquepart=base,hash=e2ac534a758a01d2f71dca4f0de0a9d6}{%
           family={Gabel},
           familyi={G\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{uniquename=0,uniquepart=base,hash=9449802bcb467309c0ebced658096818}{%
           family={Riedmiller},
           familyi={R\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{2}{}{%
        {{hash=a6855779cbaaf0280e06a4d858e5136d}{%
           family={Wiering},
           familyi={W\bibinitperiod},
           given={Marco},
           giveni={M\bibinitperiod}}}%
        {{hash=dde7efb52fcf5e2fad203f10f1ba90ce}{%
           family={Otterlo},
           familyi={O\bibinitperiod},
           given={Martijn},
           giveni={M\bibinitperiod},
           prefix={van},
           prefixi={v\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer Berlin Heidelberg}%
      }
      \strng{namehash}{9ccfbfeda5044fe2109c77ef1c67ad9e}
      \strng{fullhash}{d4a6cb97cdf04d3b08e0bcaa6f4c1750}
      \strng{bibnamehash}{d4a6cb97cdf04d3b08e0bcaa6f4c1750}
      \strng{authorbibnamehash}{d4a6cb97cdf04d3b08e0bcaa6f4c1750}
      \strng{authornamehash}{9ccfbfeda5044fe2109c77ef1c67ad9e}
      \strng{authorfullhash}{d4a6cb97cdf04d3b08e0bcaa6f4c1750}
      \strng{editorbibnamehash}{b30b771391670cd7ae6157e722016025}
      \strng{editornamehash}{b30b771391670cd7ae6157e722016025}
      \strng{editorfullhash}{b30b771391670cd7ae6157e722016025}
      \field{sortinit}{L}
      \field{sortinithash}{2c7981aaabc885868aba60f0c09ee20f}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Batch reinforcement learning is a subfield of dynamic programming-based reinforcement learning. Originally defined as the task of learning the best possible policy from a fixed set of a priori-known transition samples, the (batch) algorithms developed in this field can be easily adapted to the classical online case, where the agent interacts with the environment while learning. Due to the efficient use of collected data and the stability of the learning process, this research area has attracted a lot of attention recently. In this chapter, we introduce the basic principles and the theory behind batch reinforcement learning, describe the most important algorithms, exemplarily discuss ongoing research within this field, and briefly survey real-world applications of batch reinforcement learning.}
      \field{booktitle}{Reinforcement {{Learning}}: {{State}}-of-the-{{Art}}}
      \field{isbn}{978-3-642-27645-3}
      \field{langid}{english}
      \field{series}{Adaptation, {{Learning}}, and {{Optimization}}}
      \field{title}{Batch {{Reinforcement Learning}}}
      \field{urlday}{22}
      \field{urlmonth}{2}
      \field{urlyear}{2019}
      \field{year}{2012}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{45\bibrangedash 73}
      \range{pages}{29}
      \verb{doi}
      \verb 10.1007/978-3-642-27645-3_2
      \endverb
      \verb{file}
      \verb D:\\Google Drive\\ZotFiles\\2012Lange et al\\Lange et al_2012_Batch Reinforcement Learning.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-3-642-27645-3_2
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-3-642-27645-3_2
      \endverb
      \keyw{Batch Learning,Multiagent System,Neural Information Processing System,Policy Iteration,Reinforcement Learning}
    \endentry
    \entry{linSelfimprovingReactiveAgents1992}{article}{}
      \name{author}{1}{}{%
        {{uniquename=0,uniquepart=base,hash=fbe995b88920a196d852d2d613da0c32}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Long-Ji},
           giveni={L\bibinithyphendelim J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{fbe995b88920a196d852d2d613da0c32}
      \strng{fullhash}{fbe995b88920a196d852d2d613da0c32}
      \strng{bibnamehash}{fbe995b88920a196d852d2d613da0c32}
      \strng{authorbibnamehash}{fbe995b88920a196d852d2d613da0c32}
      \strng{authornamehash}{fbe995b88920a196d852d2d613da0c32}
      \strng{authorfullhash}{fbe995b88920a196d852d2d613da0c32}
      \field{sortinit}{L}
      \field{sortinithash}{2c7981aaabc885868aba60f0c09ee20f}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.This paper compares eight reinforcement learning frameworks:adaptive heuristic critic (AHC) learning due to Sutton,Q-learning due to Watkins, and three extensions to both basic methods for speeding up learning. The three extensions are experience replay, learning action models for planning, and teaching. The frameworks were investigated using connectionism as an approach to generalization. To evaluate the performance of different frameworks, a dynamic environment was used as a testbed. The environment is moderately complex and nondeterministic. This paper describes these frameworks and algorithms in detail and presents empirical evaluation of the frameworks.}
      \field{day}{1}
      \field{issn}{1573-0565}
      \field{journaltitle}{Machine Learning}
      \field{langid}{english}
      \field{month}{5}
      \field{number}{3}
      \field{shortjournal}{Mach Learn}
      \field{title}{Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching}
      \field{volume}{8}
      \field{year}{1992}
      \field{dateera}{ce}
      \field{pages}{293\bibrangedash 321}
      \range{pages}{29}
      \verb{doi}
      \verb 10.1007/BF00992699
      \endverb
      \verb{file}
      \verb D:\\Google Drive\\ZotFiles\\1992Lin\\Lin_1992_Self-improving reactive agents based on reinforcement learning, planning and.pdf
      \endverb
      \keyw{Reinforcement learning,connectionist networks,planning,teaching}
    \endentry
    \entry{pathakCuriosityDrivenExplorationSelfSupervised2017}{inproceedings}{}
      \name{author}{4}{}{%
        {{uniquename=0,uniquepart=base,hash=09e431093c8637ade01037714cfc992c}{%
           family={Pathak},
           familyi={P\bibinitperiod},
           given={Deepak},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{uniquename=0,uniquepart=base,hash=8386009211558835d056bfe419103c76}{%
           family={Agrawal},
           familyi={A\bibinitperiod},
           given={Pulkit},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{uniquename=0,uniquepart=base,hash=5a663b27298722834a8cf09bb93d8c94}{%
           family={Efros},
           familyi={E\bibinitperiod},
           given={Alexei\bibnamedelima A.},
           giveni={A\bibinitperiod\bibinitdelim A\bibinitperiod},
           givenun=0}}%
        {{uniquename=0,uniquepart=base,hash=90180e1a30742e0d15328bfe637c2ef4}{%
           family={Darrell},
           familyi={D\bibinitperiod},
           given={Trevor},
           giveni={T\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {Honolulu, HI, USA}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{a353c16f2bb5e07676f9be9b2c866db4}
      \strng{fullhash}{5e51aa99fa833fb4f48cba575f1027fd}
      \strng{bibnamehash}{a353c16f2bb5e07676f9be9b2c866db4}
      \strng{authorbibnamehash}{a353c16f2bb5e07676f9be9b2c866db4}
      \strng{authornamehash}{a353c16f2bb5e07676f9be9b2c866db4}
      \strng{authorfullhash}{5e51aa99fa833fb4f48cba575f1027fd}
      \field{sortinit}{P}
      \field{sortinithash}{8d51b3d5b78d75b54308d706b9bbe285}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent’s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difﬁculties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efﬁciently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.}
      \field{booktitle}{2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})}
      \field{eventtitle}{2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})}
      \field{isbn}{978-1-5386-0733-6}
      \field{langid}{english}
      \field{month}{7}
      \field{title}{Curiosity-{{Driven Exploration}} by {{Self}}-{{Supervised Prediction}}}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{pages}{488\bibrangedash 489}
      \range{pages}{2}
      \verb{doi}
      \verb 10.1109/CVPRW.2017.70
      \endverb
      \verb{file}
      \verb D:\\Google Drive\\ZotFiles\\2017Pathak et al\\Pathak et al_2017_Curiosity-Driven Exploration by Self-Supervised Prediction.pdf
      \endverb
    \endentry
    \entry{putermanMarkovDecisionProcesses2005}{book}{}
      \name{author}{1}{}{%
        {{uniquename=0,uniquepart=base,hash=b4258b9291d903d97b69d703eebf790a}{%
           family={Puterman},
           familyi={P\bibinitperiod},
           given={Martin\bibnamedelima L.},
           giveni={M\bibinitperiod\bibinitdelim L\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {Hoboken, NJ}%
      }
      \list{publisher}{1}{%
        {Wiley-Interscience}%
      }
      \strng{namehash}{b4258b9291d903d97b69d703eebf790a}
      \strng{fullhash}{b4258b9291d903d97b69d703eebf790a}
      \strng{bibnamehash}{b4258b9291d903d97b69d703eebf790a}
      \strng{authorbibnamehash}{b4258b9291d903d97b69d703eebf790a}
      \strng{authornamehash}{b4258b9291d903d97b69d703eebf790a}
      \strng{authorfullhash}{b4258b9291d903d97b69d703eebf790a}
      \field{sortinit}{P}
      \field{sortinithash}{8d51b3d5b78d75b54308d706b9bbe285}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{isbn}{978-0-471-72782-8}
      \field{langid}{english}
      \field{note}{OCLC: 254152847}
      \field{pagetotal}{649}
      \field{series}{Wiley Series in Probability and Statistics}
      \field{shorttitle}{Markov Decision Processes}
      \field{title}{Markov {{Decision Processes}}: {{Discrete Stochastic Dynamic Programming}}}
      \field{year}{2005}
      \field{dateera}{ce}
      \verb{file}
      \verb C:\\Users\\felix\\Zotero\\storage\\VNB8FB4L\\Puterman - 2005 - Markov decision processes discrete stochastic dyn.pdf
      \endverb
    \endentry
    \entry{robbinsStochasticApproximationMethod1951}{article}{}
      \name{author}{2}{}{%
        {{uniquename=0,uniquepart=base,hash=163566b92b332258782e61e3d217a2bb}{%
           family={Robbins},
           familyi={R\bibinitperiod},
           given={Herbert},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{uniquename=0,uniquepart=base,hash=95751f7f614bc2af42f1a078b3e2ba61}{%
           family={Monro},
           familyi={M\bibinitperiod},
           given={Sutton},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{5d85b58c5cbfcc8fc266cf4f5f6fbdbb}
      \strng{fullhash}{5d85b58c5cbfcc8fc266cf4f5f6fbdbb}
      \strng{bibnamehash}{5d85b58c5cbfcc8fc266cf4f5f6fbdbb}
      \strng{authorbibnamehash}{5d85b58c5cbfcc8fc266cf4f5f6fbdbb}
      \strng{authornamehash}{5d85b58c5cbfcc8fc266cf4f5f6fbdbb}
      \strng{authorfullhash}{5d85b58c5cbfcc8fc266cf4f5f6fbdbb}
      \field{sortinit}{R}
      \field{sortinithash}{da6b42bd3ab22fee61abed031ee405f7}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{[Let M(x) denote the expected value at level x of the response to a certain experiment. M(x) is assumed to be a monotone function of x but is unknown to the experimenter, and it is desired to find the solution x = θ of the equation M(x) = α, where α is a given constant. We give a method for making successive experiments at levels x1,x2,⋯ in such a way that xn will tend to θ in probability.]}
      \field{issn}{0003-4851}
      \field{journaltitle}{The Annals of Mathematical Statistics}
      \field{number}{3}
      \field{title}{A {{Stochastic Approximation Method}}}
      \field{urlday}{27}
      \field{urlmonth}{2}
      \field{urlyear}{2019}
      \field{volume}{22}
      \field{year}{1951}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{400\bibrangedash 407}
      \range{pages}{8}
      \verb{file}
      \verb C:\\Users\\felix\\Zotero\\storage\\4CSGQDU7\\Robbins and Monro - 1951 - A Stochastic Approximation Method.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.jstor.org/stable/2236626
      \endverb
      \verb{url}
      \verb https://www.jstor.org/stable/2236626
      \endverb
    \endentry
    \entry{strehlAnalysisModelbasedInterval2008}{article}{}
      \name{author}{2}{}{%
        {{uniquename=0,uniquepart=base,hash=ac0ed7f5b15d0617e232f18c3153085b}{%
           family={Strehl},
           familyi={S\bibinitperiod},
           given={Alexander\bibnamedelima L.},
           giveni={A\bibinitperiod\bibinitdelim L\bibinitperiod},
           givenun=0}}%
        {{uniquename=0,uniquepart=base,hash=211e529e9f10ad1bfacff2284da10ea7}{%
           family={Littman},
           familyi={L\bibinitperiod},
           given={Michael\bibnamedelima L.},
           giveni={M\bibinitperiod\bibinitdelim L\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{e126a6b8fa5583cb704698d42e538fa6}
      \strng{fullhash}{e126a6b8fa5583cb704698d42e538fa6}
      \strng{bibnamehash}{e126a6b8fa5583cb704698d42e538fa6}
      \strng{authorbibnamehash}{e126a6b8fa5583cb704698d42e538fa6}
      \strng{authornamehash}{e126a6b8fa5583cb704698d42e538fa6}
      \strng{authorfullhash}{e126a6b8fa5583cb704698d42e538fa6}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Several algorithms for learning near-optimal policies in Markov Decision Processes have been analyzed and proven efficient. Empirical results have suggested that Model-based Interval Estimation (MBIE) learns efficiently in practice, effectively balancing exploration and exploitation. This paper presents a theoretical analysis of MBIE and a new variation called MBIE-EB, proving their efficiency even under worst-case conditions. The paper also introduces a new performance metric, average loss, and relates it to its less “online” cousins from the literature.}
      \field{day}{1}
      \field{issn}{0022-0000}
      \field{journaltitle}{Journal of Computer and System Sciences}
      \field{month}{12}
      \field{number}{8}
      \field{series}{Learning {{Theory}} 2005}
      \field{shortjournal}{Journal of Computer and System Sciences}
      \field{title}{An Analysis of Model-Based {{Interval Estimation}} for {{Markov Decision Processes}}}
      \field{volume}{74}
      \field{year}{2008}
      \field{dateera}{ce}
      \field{pages}{1309\bibrangedash 1331}
      \range{pages}{23}
      \verb{doi}
      \verb 10.1016/j.jcss.2007.08.009
      \endverb
      \verb{file}
      \verb D:\\Google Drive\\ZotFiles\\2008Strehl_Littman\\Strehl_Littman_2008_An analysis of model-based Interval Estimation for Markov Decision Processes.pdf;C:\\Users\\felix\\Zotero\\storage\\722XY68T\\S0022000008000767.html
      \endverb
      \keyw{Reinforcement learning,Learning theory,Markov Decision Processes}
    \endentry
    \entry{suttonLearningPredictMethods1988}{article}{}
      \name{author}{1}{}{%
        {{uniquename=0,uniquepart=base,hash=eb920e5277d3d5fd0903f3cd41e11871}{%
           family={Sutton},
           familyi={S\bibinitperiod},
           given={Richard\bibnamedelima S.},
           giveni={R\bibinitperiod\bibinitdelim S\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{eb920e5277d3d5fd0903f3cd41e11871}
      \strng{fullhash}{eb920e5277d3d5fd0903f3cd41e11871}
      \strng{bibnamehash}{eb920e5277d3d5fd0903f3cd41e11871}
      \strng{authorbibnamehash}{eb920e5277d3d5fd0903f3cd41e11871}
      \strng{authornamehash}{eb920e5277d3d5fd0903f3cd41e11871}
      \strng{authorfullhash}{eb920e5277d3d5fd0903f3cd41e11871}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.}
      \field{day}{1}
      \field{issn}{1573-0565}
      \field{journaltitle}{Machine Learning}
      \field{langid}{english}
      \field{month}{8}
      \field{number}{1}
      \field{shortjournal}{Mach Learn}
      \field{title}{Learning to Predict by the Methods of Temporal Differences}
      \field{volume}{3}
      \field{year}{1988}
      \field{dateera}{ce}
      \field{pages}{9\bibrangedash 44}
      \range{pages}{36}
      \verb{doi}
      \verb 10.1007/BF00115009
      \endverb
      \verb{file}
      \verb D:\\Google Drive\\ZotFiles\\1988Sutton\\Sutton_1988_Learning to predict by the methods of temporal differences.pdf
      \endverb
      \keyw{connectionism,credit assignment,evaluation functions,Incremental learning,prediction}
    \endentry
    \entry{suttonReinforcementLearningIntroduction1998}{book}{}
      \name{author}{2}{}{%
        {{uniquename=0,uniquepart=base,hash=eb920e5277d3d5fd0903f3cd41e11871}{%
           family={Sutton},
           familyi={S\bibinitperiod},
           given={Richard\bibnamedelima S.},
           giveni={R\bibinitperiod\bibinitdelim S\bibinitperiod},
           givenun=0}}%
        {{uniquename=0,uniquepart=base,hash=32a0a208f8bcf56a13b0a8e618aa806a}{%
           family={Barto},
           familyi={B\bibinitperiod},
           given={Andrew\bibnamedelima G.},
           giveni={A\bibinitperiod\bibinitdelim G\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {Cambridge, Mass. [u.a.]}%
      }
      \list{publisher}{1}{%
        {MIT Press}%
      }
      \strng{namehash}{c6212f1a1407d96a3d9f4fefbb07eade}
      \strng{fullhash}{c6212f1a1407d96a3d9f4fefbb07eade}
      \strng{bibnamehash}{c6212f1a1407d96a3d9f4fefbb07eade}
      \strng{authorbibnamehash}{c6212f1a1407d96a3d9f4fefbb07eade}
      \strng{authornamehash}{c6212f1a1407d96a3d9f4fefbb07eade}
      \strng{authorfullhash}{c6212f1a1407d96a3d9f4fefbb07eade}
      \field{extraname}{1}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{isbn}{978-0-262-19398-6}
      \field{langid}{english}
      \field{pagetotal}{xviii+322}
      \field{series}{Adaptive Computation and Machine Learning}
      \field{shorttitle}{Reinforcement Learning}
      \field{title}{Reinforcement Learning: An Introduction}
      \field{year}{1998}
      \field{dateera}{ce}
      \keyw{Reinforcement learning,Lernen,Reinforcement learning (Machine learning),Bestärkendes Lernen,Operantes Lernen,Reinforcement learning (Machine learning); Reinforcement learning (Machine learning),Verstärkendes Lernen,Verstärkungslernen}
    \endentry
    \entry{suttonReinforcementLearningIntroduction2018a}{book}{}
      \name{author}{2}{}{%
        {{uniquename=0,uniquepart=base,hash=eb920e5277d3d5fd0903f3cd41e11871}{%
           family={Sutton},
           familyi={S\bibinitperiod},
           given={Richard\bibnamedelima S.},
           giveni={R\bibinitperiod\bibinitdelim S\bibinitperiod},
           givenun=0}}%
        {{uniquename=0,uniquepart=base,hash=32a0a208f8bcf56a13b0a8e618aa806a}{%
           family={Barto},
           familyi={B\bibinitperiod},
           given={Andrew\bibnamedelima G.},
           giveni={A\bibinitperiod\bibinitdelim G\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {Cambridge, Massachusetts}%
      }
      \list{publisher}{1}{%
        {The MIT Press}%
      }
      \strng{namehash}{c6212f1a1407d96a3d9f4fefbb07eade}
      \strng{fullhash}{c6212f1a1407d96a3d9f4fefbb07eade}
      \strng{bibnamehash}{c6212f1a1407d96a3d9f4fefbb07eade}
      \strng{authorbibnamehash}{c6212f1a1407d96a3d9f4fefbb07eade}
      \strng{authornamehash}{c6212f1a1407d96a3d9f4fefbb07eade}
      \strng{authorfullhash}{c6212f1a1407d96a3d9f4fefbb07eade}
      \field{extraname}{2}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--}
      \field{edition}{Second edition}
      \field{isbn}{978-0-262-03924-6}
      \field{langid}{english}
      \field{pagetotal}{526}
      \field{series}{Adaptive Computation and Machine Learning Series}
      \field{shorttitle}{Reinforcement Learning}
      \field{title}{Reinforcement Learning: An Introduction}
      \field{year}{2018}
      \field{dateera}{ce}
      \verb{file}
      \verb D:\\Google Drive\\ZotFiles\\2018Sutton_Barto\\Sutton_Barto_2018_Reinforcement learning.pdf
      \endverb
      \keyw{Reinforcement learning}
    \endentry
    \entry{szepesvariAlgorithmsReinforcementLearning2010}{article}{}
      \name{author}{1}{}{%
        {{uniquename=0,uniquepart=base,hash=98b0d29966f947460b3eb25590bfa7b5}{%
           family={Szepesvári},
           familyi={S\bibinitperiod},
           given={Csaba},
           giveni={C\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{98b0d29966f947460b3eb25590bfa7b5}
      \strng{fullhash}{98b0d29966f947460b3eb25590bfa7b5}
      \strng{bibnamehash}{98b0d29966f947460b3eb25590bfa7b5}
      \strng{authorbibnamehash}{98b0d29966f947460b3eb25590bfa7b5}
      \strng{authornamehash}{98b0d29966f947460b3eb25590bfa7b5}
      \strng{authorfullhash}{98b0d29966f947460b3eb25590bfa7b5}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{day}{1}
      \field{issn}{1939-4608}
      \field{journaltitle}{Synthesis Lectures on Artificial Intelligence and Machine Learning}
      \field{month}{1}
      \field{number}{1}
      \field{shortjournal}{Synthesis Lectures on Artificial Intelligence and Machine Learning}
      \field{title}{Algorithms for {{Reinforcement Learning}}}
      \field{volume}{4}
      \field{year}{2010}
      \field{dateera}{ce}
      \field{pages}{1\bibrangedash 103}
      \range{pages}{103}
      \verb{doi}
      \verb 10.2200/S00268ED1V01Y201005AIM009
      \endverb
      \verb{file}
      \verb C:\\Users\\felix\\Zotero\\storage\\9BG3XKG8\\Szepesvári - 2010 - Algorithms for Reinforcement Learning.pdf;C:\\Users\\felix\\Zotero\\storage\\XFRVYKZ3\\Algorithms_ReinforcementLearning.pdf;C:\\Users\\felix\\Zotero\\storage\\3HQTSX4N\\S00268ED1V01Y201005AIM009.html
      \endverb
    \endentry
    \entry{tsitsiklisAsynchronousStochasticApproximation1994}{article}{}
      \name{author}{1}{}{%
        {{uniquename=0,uniquepart=base,hash=c68e0327319b548306afacf417d06ebf}{%
           family={Tsitsiklis},
           familyi={T\bibinitperiod},
           given={John\bibnamedelima N.},
           giveni={J\bibinitperiod\bibinitdelim N\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{c68e0327319b548306afacf417d06ebf}
      \strng{fullhash}{c68e0327319b548306afacf417d06ebf}
      \strng{bibnamehash}{c68e0327319b548306afacf417d06ebf}
      \strng{authorbibnamehash}{c68e0327319b548306afacf417d06ebf}
      \strng{authornamehash}{c68e0327319b548306afacf417d06ebf}
      \strng{authorfullhash}{c68e0327319b548306afacf417d06ebf}
      \field{sortinit}{T}
      \field{sortinithash}{6f7aff9db9dcfeb7f95fd5bbd2f78df9}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We provide some general results on the convergence of a class of stochastic approximation algorithms and their parallel and asynchronous variants. We then use these results to study the Q-learning algorithm, a reinforcement learning method for solving Markov decision problems, and establish its convergence under conditions more general than previously available.}
      \field{day}{1}
      \field{issn}{1573-0565}
      \field{journaltitle}{Machine Learning}
      \field{langid}{english}
      \field{month}{9}
      \field{number}{3}
      \field{shortjournal}{Mach Learn}
      \field{title}{Asynchronous Stochastic Approximation and {{Q}}-Learning}
      \field{volume}{16}
      \field{year}{1994}
      \field{dateera}{ce}
      \field{pages}{185\bibrangedash 202}
      \range{pages}{18}
      \verb{doi}
      \verb 10.1007/BF00993306
      \endverb
      \verb{file}
      \verb D:\\Google Drive\\ZotFiles\\1994Tsitsiklis\\Tsitsiklis_1994_Asynchronous stochastic approximation and Q-learning.pdf
      \endverb
      \keyw{Reinforcement learning,dynamic programming,Q-learning,stochastic approximation}
    \endentry
    \entry{watkinsQlearning1992}{article}{}
      \name{author}{2}{}{%
        {{uniquename=2,uniquepart=given,hash=6522b6087fe013d303faca6da91596ca}{%
           family={Watkins},
           familyi={W\bibinitperiod},
           given={Christopher\bibnamedelimb J.\bibnamedelimi C.\bibnamedelimi H.},
           giveni={C\bibinitperiod\bibinitdelim J\bibinitperiod\bibinitdelim C\bibinitperiod\bibinitdelim H\bibinitperiod},
           givenun=2}}%
        {{uniquename=0,uniquepart=base,hash=ef0afbe0b4e15059c963e026fe213c34}{%
           family={Dayan},
           familyi={D\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{1f776b9a46cb729efd9599e3af8beeef}
      \strng{fullhash}{1f776b9a46cb729efd9599e3af8beeef}
      \strng{bibnamehash}{1f776b9a46cb729efd9599e3af8beeef}
      \strng{authorbibnamehash}{1f776b9a46cb729efd9599e3af8beeef}
      \strng{authornamehash}{1f776b9a46cb729efd9599e3af8beeef}
      \strng{authorfullhash}{1f776b9a46cb729efd9599e3af8beeef}
      \field{sortinit}{W}
      \field{sortinithash}{ecb89ff85896a47dc313960773ac311d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited...}
      \field{day}{1}
      \field{issn}{0885-6125, 1573-0565}
      \field{journaltitle}{Machine Learning}
      \field{langid}{english}
      \field{month}{5}
      \field{number}{3-4}
      \field{shortjournal}{Mach Learn}
      \field{title}{Q-Learning}
      \field{volume}{8}
      \field{year}{1992}
      \field{dateera}{ce}
      \field{pages}{279\bibrangedash 292}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1007/BF00992698
      \endverb
      \verb{file}
      \verb C:\\Users\\felix\\Zotero\\storage\\PF3CU2VT\\Q-learning SpringerLink.pdf;C:\\Users\\felix\\Zotero\\storage\\WCA8JCF4\\Watkins and Dayan - 1992 - Emphasis Type=ItalicQEmphasis-learning.pdf;C:\\Users\\felix\\Zotero\\storage\\22WQBT9Y\\BF00992698.html
      \endverb
    \endentry
    \entry{watkinsLearningDelayedRewards1989}{thesis}{}
      \name{author}{1}{}{%
        {{uniquename=2,uniquepart=given,hash=95237318637755e27714e3782c884a8b}{%
           family={Watkins},
           familyi={W\bibinitperiod},
           given={Christopher\bibnamedelimb John\bibnamedelimb Cornish\bibnamedelima Hellaby},
           giveni={C\bibinitperiod\bibinitdelim J\bibinitperiod\bibinitdelim C\bibinitperiod\bibinitdelim H\bibinitperiod},
           givenun=2}}%
      }
      \list{institution}{1}{%
        {King's College, Cambridge}%
      }
      \strng{namehash}{95237318637755e27714e3782c884a8b}
      \strng{fullhash}{95237318637755e27714e3782c884a8b}
      \strng{bibnamehash}{95237318637755e27714e3782c884a8b}
      \strng{authorbibnamehash}{95237318637755e27714e3782c884a8b}
      \strng{authornamehash}{95237318637755e27714e3782c884a8b}
      \strng{authorfullhash}{95237318637755e27714e3782c884a8b}
      \field{sortinit}{W}
      \field{sortinithash}{ecb89ff85896a47dc313960773ac311d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Learning from Delayed Rewards}
      \field{type}{PhD Thesis}
      \field{year}{1989}
      \field{dateera}{ce}
      \verb{file}
      \verb D:\\Google Drive\\ZotFiles\\1989Watkins\\Watkins_1989_Learning from delayed rewards.pdf
      \endverb
    \endentry
    \entry{whiteRealApplicationsMarkov1985}{article}{}
      \name{author}{1}{}{%
        {{uniquename=0,uniquepart=base,hash=aa320bd099c1a6093a33260ddcecd3f3}{%
           family={White},
           familyi={W\bibinitperiod},
           given={Douglas\bibnamedelima J.},
           giveni={D\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{aa320bd099c1a6093a33260ddcecd3f3}
      \strng{fullhash}{aa320bd099c1a6093a33260ddcecd3f3}
      \strng{bibnamehash}{aa320bd099c1a6093a33260ddcecd3f3}
      \strng{authorbibnamehash}{aa320bd099c1a6093a33260ddcecd3f3}
      \strng{authornamehash}{aa320bd099c1a6093a33260ddcecd3f3}
      \strng{authorfullhash}{aa320bd099c1a6093a33260ddcecd3f3}
      \field{sortinit}{W}
      \field{sortinithash}{ecb89ff85896a47dc313960773ac311d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0092-2102, 1526-551X}
      \field{journaltitle}{Interfaces}
      \field{langid}{english}
      \field{month}{12}
      \field{number}{6}
      \field{title}{Real {{Applications}} of {{Markov Decision Processes}}}
      \field{volume}{15}
      \field{year}{1985}
      \field{dateera}{ce}
      \field{pages}{73\bibrangedash 83}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1287/inte.15.6.73
      \endverb
      \verb{file}
      \verb C:\\Users\\felix\\Zotero\\storage\\DWX2M69R\\White - 1985 - Real Applications of Markov Decision Processes.pdf
      \endverb
    \endentry
    \entry{wieringEfficientModelbasedExploration1998}{inproceedings}{}
      \name{author}{2}{}{%
        {{uniquename=0,uniquepart=base,hash=a6855779cbaaf0280e06a4d858e5136d}{%
           family={Wiering},
           familyi={W\bibinitperiod},
           given={Marco},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{uniquename=0,uniquepart=base,hash=288bdbcfe1b91ad7484d7a24f74f99ed}{%
           family={Schmidhuber},
           familyi={S\bibinitperiod},
           given={Jürgen},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{d68bf8f74182ac49ad7ac6462f2064dd}
      \strng{fullhash}{d68bf8f74182ac49ad7ac6462f2064dd}
      \strng{bibnamehash}{d68bf8f74182ac49ad7ac6462f2064dd}
      \strng{authorbibnamehash}{d68bf8f74182ac49ad7ac6462f2064dd}
      \strng{authornamehash}{d68bf8f74182ac49ad7ac6462f2064dd}
      \strng{authorfullhash}{d68bf8f74182ac49ad7ac6462f2064dd}
      \field{sortinit}{W}
      \field{sortinithash}{ecb89ff85896a47dc313960773ac311d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Proceedings of the {{Sixth International Conference}} on {{Simulation}} of {{Adaptive Behavior}}: {{From Animals}} to {{Animats}}}
      \field{title}{Efficient Model-Based Exploration}
      \field{volume}{6}
      \field{year}{1998}
      \field{dateera}{ce}
      \field{pages}{223\bibrangedash 228}
      \range{pages}{6}
      \verb{file}
      \verb D:\\Google Drive\\ZotFiles\\1998Wiering_Schmidhuber\\Wiering_Schmidhuber_1998_Efficient model-based exploration.pdf;C:\\Users\\felix\\Zotero\\storage\\VKDPQRCB\\books.html
      \endverb
    \endentry
  \enddatalist
\endrefsection
\endinput

