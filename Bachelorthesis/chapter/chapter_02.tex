% !TEX root = ../BScWIMEngl.tex

\chapter{Reinforcement Learning Algorithms}

\section{Introduction}

Dynamic Programming usually breaks down in the real world for two reasons:
\begin{enumerate}
    \item The transition probabilities and immediate rewards are not known or hard to calculate.
    \item The state and action space is too large to even compute one iteration of Dynamic Programming for every state-action tuple (e.g. possible positions and possible moves in every position in chess).
\end{enumerate}

This is where \emph{Reinforcement Learning} algorithms come in, which try to find solutions without having to sweep the entire state space. In this chapter based on \textcite{suttonReinforcementLearningIntroduction2018a} we will examine advantages and disadvantages of various algorithms and discuss possible variations and extensions. In the next chapter we will show almost sure convergence of the basic algorithms introduced in this chapter and illustrate their connection to sotchastic approximation. \fxwarning{check outline of the plan} 

We separate their introduction and the convergence proofs, because -- while the guarantee of almost sure convergence is reassuring -- it is of little use for comparing various algorithms. Since one of the reasons for moving away from Dynamic Programming in the first place was, that we could not calculate the value function for the entire state space within a reasonable time frame. As the size of the state space is often too large for that.  Therefore almost sure convergence should not be viewed as more than an entry requirement. For this reason most papers compare algorithms empirically on various example problems. And for some of the more complex algorithms convergence proofs simply do not exist yet. 

So since the theoretical convergence properties are usually only ever an afterthought, it is more natural to introduce the various algorithms heuristically, explaining what specific problems they try to address with examples.


But let us ignore the second problem for a moment and consider the case where we only have the first problem (\(p\) and \(r\) unknown). Then we can learn from a sample \(((X_t, A_t, Y_t, R_t), t\in\{0,\dots, T\})\) with
\[
	(Y_t,R_t)\sim \cP(\cdot \mid X_t, A_t)
\]
and with \(Y_t=X_{t+1}\) if the sample is generated sequentially by a behavior. But there is no reason not to allow this more general sample which might be useful in cases where you can jump around in the statespace and try different transitions at will. 

We can then use this batch of transitions to calculate estimators \(\hat{p},\hat{r}\) for the state transitions and immediate rewards \(\hat{r}\). And use Dynamic Programming on these estimators.

%Since the transition probabilities and immediate rewards are not known, these algorithms first need to create samples from which to estimate them. These samples are created using a behavior resulting in a state-action-reward sequence \((X_t,A_t,R_t,t\in\N_0)\). This behavior has to explore the state-action space, trying to generate data of interesting transitions. From this the algorithm then has to estimate the value functions. 


\begin{algorithm}
	\caption{Naive Batch Learning Algorithm}
	\begin{enumerate}
		\item Generate the history \((X_t, A_t, Y_t, R_t, t\in\{0,\dots, T\})\)
		\begin{algorithmic}[1]
			\For{ \((y,x,a)\in\cX\times\cX\times\cA \)} \Comment{initialize variables}
				\State rewards[\(x,a\)]\(\gets\)list() 
				\State stateTransitions[\(y\mid x,a\)]\(\gets\) 0
				\State totalTransitions[\(x,a\)]\(\gets\) 0
			\EndFor
			\For{\(t=0,\dots, T\)}
				\State rewards[\(X_t,A_t\)].append(\(R_{t+1}\))
				\State stateTransitions[\(Y_t\mid X_t,A_t\)]\(++\)
				\State totalTransitions[\(X_t,A_t\)]\(++\)
			\EndFor
			\For{\((x,a)\in\cX\times\cA\)}
				\State \(\hat{r}(x,a) \gets\) average(rewards[x,a])
				\For{\(y\in\cX\)}
					\State \(\hat{p}(y\mid x,a)\gets\) stateTransitions[\(y\mid x,a\)]\(/\)totalTransitions[\(x,a\)]
				\EndFor
			\EndFor
		\end{algorithmic}
		\item Use Dynamic Programming on \(\hat{r}\), \(\hat{p}\) 
	\end{enumerate}
\end{algorithm}
\fxnote{add code/ ref code Dynamic Programming}
\fxnote{numerical stability analysis?}

If the batch was generated by an exploration policy we separated the \emph{exploration} from the later \emph{exploitation}. The methods which do that are often grouped under the term \emph{Batch Reinforcement Learning} or \emph{off-line} learning. 

This method works fine, if the state space is small and one can sample enough observations for every state and action in a reasonable timeframe.
But if our state space is too large for that, it is impractical to wait for this. 

%%%%%%%%%%%%%%%%%%%
\section{Monte Carlo}
%%%%%%%%%%%%%%%%%

One idea to tackle larger state spaces is, that it is often unneccessary to know the value function in every state.

To visualize this idea it is useful to imagine the state space to be a room the agent has to navigate. 

\[
\begin{tikzpicture}
	\foreach \x in {0,1,2,3,4}{
		\foreach \y in {0,1,2,3,4}{
			\draw (\x,\y) rectangle +(1,1);
		}
	}
	\node at (1.5,0.5){S};
	\node at (2.5,4.5) {\textcolor{gray}{\(\gamma\)}};
	\foreach \value in {2,3,4,5}{
		\tikzmath{
			int \power;
			\power=7-\value;
		}
		\node at (1.5, \value-0.5) {\textcolor{gray}{\(\gamma^\power\)}};
	}
	\node at (3.5, 4.5){G};
	

	\node at (8.5,2.5) {\parbox[t]{6cm}{The state space \(\cX\) are the tiles on the floor, the actions \(\cA\) are the adjacent tiles, where the next state is with probability one equal to the action, and the reward is 0 for every transition but the transition to the goal (G) where the reward is 1 and the game ends. The start state is S. The value of choosing a tile is indicated in grey on the tile.}};
\end{tikzpicture}
\]

It is often enough for the agent to know the action value function for the states on his path, without knowing the value of states in the corners. Since he can then follow these ``breadcrumbs'' to find the goal reliably again. And it will quickly stop walking in circles if it goes into the direction of the highest value next time. 

This idea is the basis for Monte Carlo algorithms. To make notation more brief we will introduce the algorithm to calculate the value function, the action value function will be analogous. Consider an episodic MDP and a behavior \(\pi\) then
\[
	\sum_{t=0}^\infty\gamma^tR_{t+1}=\sum_{t=0}^T\gamma^tR_{t+1}
\]
is a bias free estimator for \(V^\pi(X_0)\) for episode lenght T. As the definition was
\[
	V^\pi(x)=\E\left[\sum_{t=0}^\infty\gamma^tR_{t+1} \;\middle|\; X_0=x \right] 
\]
And because of the markov property
\[
	\sum_{t=k}^T\gamma^tR_{t+1}
\]
is a bias free estimator for \(V^\pi(X_k)\). \emph{First-visit Monte Carlo} uses the return after the first visit of a state \(x\in\{X_1,\dots,X_t\}\) as an estimator for \(V^\pi(x)\).

\begin{algorithm}
	\caption{First-visit Monte Carlo}
	\begin{algorithmic}[1]
		\For{\(x\in\cX\)} \Comment{initializing}
			\State Returns\((x)\gets\) list()
			\State \(V^\pi(x)\gets 0\)
		\EndFor
		\While{true} (forever) \Comment{learning}
			\State Generate an episode \(((X_t,R_{t+1}), t\in\{0,\dots,T\})\) with behavior \(\pi\)
			\For{\(x\in\{X_0,\dots,X_T\}\)}
				\State \(k\gets \min\{t \mid X_t=x\}\)
				\State Returns\((x)\).append(\(\sum_{t=k}^T\gamma^tR_{t+1}\))
				\State \(V^\pi(x)\gets\) average(Returns(x))
			\EndFor
		\EndWhile
	\end{algorithmic}
\end{algorithm}

Because of the strong law of large numbers, first-visit Monte Carlo algorithm causes the value function estimation \(\hat{V}^\pi(x)\) to converge with probability 1 to \(V^\pi(x)\) for every state \(x\in\cX\) which is visited with positive probability given behavior \(\pi\) and starting distribution \(X_0\). 

\emph{Every-visit} Monte Carlo uses the Return after every visit of a state \(x\) to estimate \(V^\pi(x)\). Since this means that the tail of the rewards can be included in multiple returns, the returns are not independent from each other anymore which make proving convergence a little bit more difficult than simply applying the strong law of large numbers. But every visit Monte Carlo will turn out to be a special case of \(TD(\lambda)\).\fxnote{will TD proof work?} 

The same method can be applied to learn the action value function \(Q^\pi\). In this case we take the returns following a state \emph{and} action as estimators for \(Q^\pi\). But if the model is known and our problem is just a large state space calculating \(V^\pi\) is preferrable, since \(|\cX|\le |\cX\times\cA|\) means that the first algorithm needs fewer observations to converge and \(Q^\pi\) can be calculated with \(V^\pi\) given \(r\) and \(p\).

\subsection{From \(\pi\) to \(\pi^*\)}
Let us assume exploring starts 
\[
	\Pr(X_0=x)>0 \qquad \forall x\in\cX
\] 
for a moment. Then Monte Carlo converges whatever policy we select. Similar to policy iteration (\ref{policy iteration}) we can then alternate between calculating \(Q^{\pi_n}\) and selecting \(\pi_{n+1}\) as a greedy policy with regard to \(Q^{\pi_n}\). This is referred to as generalized policy iteration (GPI). If we would wait for our Monte Carlo approximation of \(Q^{\pi_n}\) to converge, \(\pi_n\) would converge to \(\pi^*\) for the same reason as policy iteration converges. But remember we are doing Monte Carlo in the first place, because the state space is too large to wait for an evaluation of every state. Which means in practice algorithms alternate beteen choosing a greedy policy with regard to \({V^{\pi_n}}\) and generating an episode with policy \(\pi_n\), averaging the estimates from this episode with the estimates of \(V^{\pi_{n-1}}\). But since we have to assume
\[
	V^{\pi_{n-1}}\neq V^{\pi_n}
\] 
in general, using these old estimates is not bias free. It is easy to see that this procedure can only converge to \(\pi^*\) if it converges at all. Since there are only a finite number of deterministic stationary policies it would have to stay constant at some point, but for a constant policy Monte Carlo will converge, and then the policy can only stay constant if it is greedy with regards to its own value function, which forces it to be optimal (\ref{real improvement or optimal}). But it is still an open problem whether or not this alternating procedure converges at all \parencite[99]{suttonReinforcementLearningIntroduction2018a}.

If we remove the assumption of exporing starts, we still need to ensure that every state is visited with positive probability for MC to converge. This requires the policy to do the exploring. There are multiple approaches to exploration \fxnote*{check claim}{which we will discuss later}. We will discuss the most straightforward example here, under the assumption that we can calculate \(Q^{\pi_n}\) somehow.  

\begin{definition}
	A stationary policy \(\pi\) is called 
	\begin{description}[font=\normalfont]
		\item[\emph{soft}] if it fulfills 
		\[
			\pi(a\mid x)>0 \qquad\;\; \forall (x,a)\in\cX\times\cA
		\]
		\item[\emph{\(\vep\)-soft}] if it fulfills 
		\[
			\pi(a\mid x)>\frac{\vep}{|\cA_x|} \quad \forall (x,a)\in\cX\times\cA
		\]
		\item[\emph{\(\vep\)-greedy}] with regard to Q, if it selects the greedy action w.r.t. Q with probability \((1-\vep)\) and a (uniform) random action with probability \(\vep\), i.e. \[\pi(a\mid x)=
			\begin{cases}
				(1-\vep)+ \frac{\vep}{|\cA_x|} & a \text{ is greedy\footnotemark[1]}\\
				\frac{\vep}{|\cA_x|} & a \text{ is not greedy}
			\end{cases}
		\]
	\end{description}\footnotetext[1]{w.l.o.g. only one greedy action}
	Note that an \(\vep\)-greedy policy is \(\vep\)-soft.

	An \(\vep\)-soft policy \(\pi^*\) is called \emph{\(\vep\)-soft optimal} if 
	\[
		V^{\pi^*}(x)=\sup_{\pi\; \vep\text{-soft}}(x) V^\pi \eqqcolon \tilde{V}^*(x)
	\]
\end{definition}

\begin{prop}
	Generalized policy iteration with \(\vep\)-greedy policies converges to a \(\vep\)-soft optimal policy
\end{prop}
\begin{proof}
	% Let \(\pi_{n+1}\) be \(\vep\)-greedy with regard to \(Q^{\pi_n}\). Since \(\pi_n\) is an \(\vep\)-soft policy we know that
	% \[
	% 	\pi_n(a\mid x) -\frac{\vep}{|\cA_x|} \ge 0
	% \]
	% which together with the equality
	% \[
	% 	\sum_{a\in\cA_x}\frac{\pi_n(a\mid x) -\frac{\vep}{|\cA_x|}}{1-\vep} 
	% 	= \frac{\left(\sum_{a\in\cA_x}\pi_n(a\mid x)\right)- \vep}{1-\vep}
	% 	= 1
	% \]
	% implies the inequality (\ref{weighted sum}). Since the maximum is larger than the weighted average.
	% \begin{align}
	% 	&T^{\pi_{n+1}}V^{\pi_n}(x)=Q^{\pi_n}(x,\pi_{n+1}(x))
	% 	=\sum_{a\in\cA_x} \pi_{n+1}(a\mid x) Q^{\pi_n}(x,a)
	% 	\nonumber\\
	% 	&\lxeq{\vep\text{-greedy}}\frac{\vep}{|\cA_x|}\sum_{a\in\cA_x}Q^{\pi_n}(x,a)
	% 	+(1-\vep)\max_{a\in\cA_x} Q^{\pi_n}(x,a)
	% 	\nonumber\\
	% 	&\ge\frac{\vep}{|\cA_x|}\sum_{a\in\cA_x}Q^{\pi_n}(x,a) 
	% 	+(1-\vep)\sum_{a\in\cA_x}
	% 	\frac{\pi_n(a\mid x) -\frac{\vep}{|\cA_x|}}{1-\vep}Q^{\pi_n}(x,a)  
	% 	\label{weighted sum}\\
	% 	&=\frac{\vep}{|\cA_x|}\sum_{a\in\cA_x}Q^{\pi_n}(x,a) 
	% 	-\frac{\vep}{|\cA_x|}\sum_{a\in\cA_x}Q^{\pi_n}(x,a)
	% 	+\sum_{a\in\cA_x}\pi_n(a\mid x)Q^{\pi_n}(x,a)
	% 	\nonumber\\
	% 	&=V^{\pi_n}(x)\nonumber
	% \end{align}
	% This implies \(V^{\pi_{n+1}}\ge V^{\pi_n} \). 
	To use the same argument as with the standard policy iteration, we would need
	\[
		T^*(V^{\pi_n})=T^{\pi_{n+1}}(V^{\pi_n})
	\]
	But this is not true. To circumvent this problem we ``move the requirement of \(\vep\)-softness into the environment''. I.e. we define an MDP \(\tilde{M}\) where
	\[
		\tilde{\cP}(\cdot \mid x,a) \coloneqq (1-\vep) \cP(\cdot \mid x,a) 
		+ \frac{\vep}{|\cA_x|}\sum_{b\in\cA_x} \cP(\cdot \mid x,b)
	\]
	This means, that with probability \(\vep\) the transition kernel will ignore the selected action and behave as if an uniformly random action was chosen.	
	
	We can transform \(\vep\)-soft policies \(\pi\) from the old MDP to stationary policies \(\tilde{\pi}\) of \(\tilde{M}\) with a mapping \(f\), where
	\[
		f(\pi)(a\mid x)=\tilde{\pi}(a\mid x)\coloneqq \frac{\pi(a\mid x)-\frac{\vep}{|\cA_x|}}{1-\vep}\ge 0
	\]
	And for every stationary policy \(\tilde{\pi}\) of \(\tilde{M}\) we can define the \(\vep\)-soft policy \(\pi\) by
	\[
		\pi(a\mid x)\coloneqq (1-\vep)\tilde{\pi}(a\mid x) + \frac{\vep}{|\cA_x|}
	\]
	which is the inverse mapping. Therefore \(f\) is a bijection between the \(\vep\)-soft policies in the old MDP and all stationary policies in the new MDP. We now show that the Value functions \(V^\pi\) stay invariant with regard to this mapping. 
	First note that
	\begin{align*}
		\tilde{p}(y\mid x,a)
		&=\tilde{\cP}(\{y\}\times \R\mid x,a)\\
		&=(1-\vep)p(y\mid x,a)+\frac{\vep}{|\cA_x|}\sum_{b\in\cA_x}p(y\mid x,b)
	\end{align*}
	and together with \(q(B\mid x,a)\coloneqq \cP(\cX\times B \mid x,a)\) the complementary marginal distribution we can show
	\begin{align*}
		\tilde{r}(x,a)&=\int t\ d\tilde{q}(t \mid x,a)=\int t\ 
		d\left((1-\vep)q(\cdot\mid x,a) + \frac{\vep}{|\cA_x|}\sum_{b\in\cA_x}q(\cdot\mid x,b)\right)(t) \\
		&=(1-\vep)\int t\ dq(t\mid x,a) 
		+ \frac{\vep}{|\cA_x|}\sum_{b\in\cA_x} \int t\ dq(t\mid x,a)\\
		&=(1-\vep)r(x,a) + \frac{\vep}{|\cA_x|}\sum_{b\in\cA_x}r(x,b)
	\end{align*}
	Therefore we know
	\begin{align*}
		&\sum_{a\in\cA_x}\tilde{\pi}(a\mid x)\tilde{r}(x,a)\\
		&=\sum_{a\in\cA_x} \left(\frac{\pi(a\mid x)-\frac{\vep}{|\cA_x|}}{1-\vep} \right)
		\left((1-\vep)r(x,a) + \frac{\vep}{|\cA_x|}\sum_{b\in\cA_x}r(x,b)\right)\\
		&=\sum_{a\in\cA_x}\left(\pi(a\mid x)-\frac{\vep}{|\cA_x|}\right)r(x,a) 
		+ \frac{\vep}{|\cA_x|}\sum_{b\in\cA_x}r(x,b) 
		\underbracket{\sum_{a\in\cA_x}\frac{\pi(a\mid x)-\frac{\vep}{|\cA_x|}}{1-\vep}}_{
			=1
		}\\
		&=\sum_{a\in\cA_x}\pi(a\mid x)r(x,a)
	\end{align*}
	and similarly
	\begin{align*}
		&\Pr^{\tilde{\pi}}(\tilde{X}_1=y\mid \tilde{X}_0=x) 
		=\sum_{a\in\cA_x}\tilde{\pi}(a\mid x)\tilde{p}(y\mid x,a)\\
		&=\sum_{a\in\cA_x}\left(\frac{\pi(a\mid x)-\frac{\vep}{|\cA_x|}}{1-\vep} \right)
		\left((1-\vep)p(y\mid x,a)+\frac{\vep}{|\cA_x|}\sum_{b\in\cA_x}p(y\mid x,b)\right)\\
		&=\dotsc = \sum_{a\in\cA_x} \pi(a\mid x)p(y\mid x,a) =\Pr^\pi(X_1=y\mid X_0=x)
	\end{align*}
	The last two equations together imply
	\begin{align*}
		\tilde{T}^{\tilde{\pi}}V^\pi(x)&=\sum_{a\in\cA_x}\tilde{\pi}(a\mid x)\tilde{r}(x,a) 
		+ \gamma \sum_{y\in\cX}\Pr^{\tilde{\pi}}(\tilde{X}_1=y\mid X_0=x)V^\pi(y)\\
		&=\sum_{a\in\cA_x}\pi(a\mid x)r(x,a) + \gamma \sum_{y\in\cX}\Pr^\pi(X_1=y\mid X_0=x)V^\pi(y)\\
		&=T^\pi V^\pi(x)=V^\pi(x)
	\end{align*}
	Since the fixpoint is unique it follows that
	\[
		V^{\tilde{\pi}}=V^\pi
	\]
	Since f is bijective this implies
	\[
		\sup_{\tilde{\pi}\in \tilde{\Pi}_S}V^{\tilde{\pi}}(x)=\sup_{\pi\ \vep\text{-soft}} V^\pi(x)
	\]
	as well as
	\begin{align*}
		Q^{\tilde{\pi}}(x,a)&=\tilde{r}(x,a)+\gamma \sum_{y\in\cX} \tilde{p}(y\mid x,a) V^\pi(y)\\
		&=\begin{aligned}[t]
			&(1-\vep)\left(r(x,a)+\gamma \sum_{y\in\cX}p(y\mid x,a) V^\pi(y) \right)\\
			&\qquad + \frac{\vep}{|\cA_x|}\sum_{b\in\cA_x}\left(r(x,b)+\gamma \sum_{y\in\cX}p(y\mid x,b) \right)
		\end{aligned}\\
		&=(1-\vep)Q^\pi(x,a)
		+ \frac{\vep}{|\cA_x|}\sum_{b\in\cA_x}\left(r(x,b)+\gamma \sum_{y\in\cX}p(y\mid x,b) \right)
	\end{align*}
	Which implies that
	\[
		\arg\max_{a\in\cA_x} Q^{\tilde{\pi}}(x,a)=\arg\max_{a\in\cA_x} Q^\pi(x,a)
	\]
	Threfore greedy with regard to \(Q^\pi\) and \(Q^{\tilde{\pi}}\) is the same. Let \(\pi_n\) be an \(\vep\)-soft policy, and let \(\pi_{n+1}\) be \(\vep\)-greedy with regard to 
	\(Q^{\pi_n}\). Then \(\tilde{\pi}_{n+1}\coloneqq f(\pi_{n+1})\) is greedy w.r.t. 
	\(Q^{\tilde{\pi}_n}\)
	\begin{align*}
		\tilde{\pi}_{n+1}(a\mid x)&=f(\pi_{n+1})(a\mid x)
		=\frac{\pi(a\mid x)-\frac{\vep}{|\cA_x|}}{1-\vep}\\
		&=\begin{cases}
			\frac{\left((1-\vep) + \frac{\vep}{|\cA_x|}\right) -\frac{\vep}{|\cA_x|}}{1-\vep}=1 & a \text{ is greedy}\\
			\frac{\frac{\vep}{|\cA_x|}- \frac{\vep}{|\cA_x|}}{1-\vep}=0 & a \text{ is not greedy}
		\end{cases}
	\end{align*}
	This finishes our proof
	\[
		\sup_{\pi\ \vep\text{-soft}} V^\pi(x)
		=\sup_{\tilde{\pi}\in \tilde{\Pi}_S}V^{\tilde{\pi}}(x)
		=\lim_{n\to\infty} V^{\tilde{\pi}_n}(x)
		=\lim_{n\to\infty} V^{\pi_n}(x)
		\qedhere
	\]
\end{proof}


\subsection{The Weakness of Monte Carlo}
\citeauthor{suttonReinforcementLearningIntroduction2018a} provides a very instructive example for the weakness of Monte Carlo. Recall that Monte Carlo tries to estimate \(V^\pi\). So the example assumes a given behavior and tries to evaluate the value of a situation.

\begin{example}[Driving Home]
	Let us assume that John Doe works in city A and drives to his home in city B after work every day. Since he wants to get home as quickly as possible, the value of the state is antiproportional to the time it will take him from a certain position home. This means that estimating the value of a certain state is equivalent to estimating the time left to drive. The longer John drives this route the better he will get at estimating the time it will take him to drive certain sections of the road. If there is a delay in an earlier section he has a good estimate for the remaining time once he cleared the obstruction. Now imagine he is driving home from a doctors appointment from city A. As soon as he enters the highway to city B, he is able to use his experience driving this route to estime the remaining time quite precisely.

	The Monte Carlo algorithm on the other hand \emph{never} uses existing value estimations to estimate the value of a different state. If John Doe uses Monte Carlo estimates to guess the remaining time from the doctor, the accuracy of his estimates will only ever increase with the times he actually starts driving from the doctor's office. 
\end{example}

Since Monte Carlo has more possible "starting points" or generally earlier points in the chain in case of estimating \(Q^\pi\), it is worse in this case. An extreme example would be two actions in the same state which have the same effect. Even though Monte Carlo might have a good estimate of the value of the first action it will start completely anew for the second action. 
\fxnote{illustrations?}

%The behavior needs to get updated with the information gathered continuously while the behavior has to balance exploration of interesting states with exploitation of high valued states. The algorithm needs to be what is called \emph{on-line}.\footnote{since Batch Reinforcement Learning algorithms often utilized little information more efficiently than their online counterparts, there were some efforts to modify these algorithms to allow for continuous updates of the estimates of the value functions. These were categorized as \emph{growing batch reinforcement learning} algorithms in contrast to pure batch learning \parencite{langeBatchReinforcementLearning2012} blurring the lines in the process.}

%\section{Asynchronous Dynamic Programming}



\section{Temporal Difference Learning TD}


\section{Mixing Both -- The Generalization TD(\(\lambda\))}
\section{Q-learning}
\section{Exploration}
%optimism
%uniform
%boltzman
%interval
%bayes

%intrinsic rewards


%%%%%%%%%%%%%%%%%%%%%%%%%
\endinput
