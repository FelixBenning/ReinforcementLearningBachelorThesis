% !TEX root = ../BScWIMEngl.tex  

\chapter{Reinforcement Learning Algorithms}

\section{Introduction}

Dynamic Programming usually breaks down in the real world for two reasons:
\begin{enumerate}
    \item The transition probabilities and immediate rewards are not known or hard to calculate.
    \item The state and action space is too large to even compute one iteration of Dynamic Programming for every state-action tuple (e.g. possible positions and possible moves in every position in chess).
\end{enumerate}

This is where algorithms which try to find good solutions without having to sweep the entire state space come in. The collective term for these various algorithms is \emph{Reinforcement Learning}.

Since the transition probabilities and immediate rewards are not known, these algorithms first need to create samples from which to estimate them. These samples are created using a behavior. So this behavior has to explore the state-action space. And from the random variables generated by this behavior, the algorithm has to estimate the value functions. 

If the state space is small and we only have the first problem, we can separate the exploration from the later exploitation of our knowledge about the MDP. These methods are grouped under the term \emph{Batch Reinforcement Learning}. But if our state space is too large for that, it is impractical to wait for this. So the behavior needs to get updated with the information gathered continuously; the algorithm needs to be what is called \emph{online}. But note that since Batch Reinforcement Learning algorithms often utilized little information more efficiently than their online counterparts, there were some efforts to modify these algorithms to allow for continuous updates of the estimates of the value functions. These were categorized as \emph{growing batch reinforcement learning} algorithms in contrast to pure batch learning \parencite{langeBatchReinforcementLearning2012} blurring the lines in the process.

So one of the main problems in online learning is balancing the exploration with the exploitation of the knowledge gathered. 

%\section{Asynchronous Dynamic Programming}
\section{Monte-Carlo}
\section{Temporal Difference Learning TD}
\section{Mixing Both -- The Generalization TD(\(\gamma\))}

\endinput
