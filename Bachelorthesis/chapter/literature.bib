
@online{ProofVerificationBlackwell,
  title = {Proof Verification - {{Blackwell}}'s Condition for a Contraction: {{Why}} Is Boundedness Neccessary?},
  url = {https://math.stackexchange.com/questions/1087885/blackwells-condition-for-a-contraction-why-is-boundedness-neccessary?rq=1},
  shorttitle = {Proof Verification - {{Blackwell}}'s Condition for a Contraction},
  journaltitle = {Mathematics Stack Exchange},
  urldate = {2019-02-04},
  file = {C:\\Users\\felix\\Zotero\\storage\\D88I6ZBY\\blackwells-condition-for-a-contraction-why-is-boundedness-neccessary.html}
}

@article{whiteRealApplicationsMarkov1985,
  langid = {english},
  title = {Real {{Applications}} of {{Markov Decision Processes}}},
  volume = {15},
  issn = {0092-2102, 1526-551X},
  doi = {10.1287/inte.15.6.73},
  number = {6},
  journaltitle = {Interfaces},
  date = {1985-12},
  pages = {73-83},
  author = {White, Douglas J.},
  file = {C:\\Users\\felix\\Zotero\\storage\\DWX2M69R\\White - 1985 - Real Applications of Markov Decision Processes.pdf}
}

@article{reesBiblatexCheatSheet,
  langid = {english},
  title = {Biblatex {{Cheat Sheet}}},
  pages = {2},
  author = {Rees, Clea F},
  file = {C:\\Users\\felix\\Zotero\\storage\\DVSDHAWK\\Rees - Biblatex Cheat Sheet.pdf}
}

@inreference{ReinforcementLearning2019,
  langid = {english},
  title = {Reinforcement Learning},
  url = {https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&oldid=880988096},
  abstract = {Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. The problem, due to its generality, is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In the operations research and control literature, reinforcement learning is called approximate dynamic programming, or neuro-dynamic programming.
The problems of interest in reinforcement learning have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation, particularly in the absence of a mathematical model of the environment. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.
In machine learning, the environment is typically formulated as a Markov Decision Process (MDP), as many reinforcement learning algorithms for this context utilize dynamic programming techniques. The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.Reinforcement learning is considered as one of three machine learning paradigms, alongside supervised learning and unsupervised learning. It differs from supervised learning in that correct input/output pairs need not be presented, and sub-optimal actions need not be explicitly corrected. Instead the focus is on performance, which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). The exploration vs. exploitation trade-off has been most thoroughly studied through the multi-armed bandit problem and in finite MDPs.},
  booktitle = {Wikipedia},
  urldate = {2019-02-06},
  date = {2019-01-30T19:10:08Z},
  file = {C:\\Users\\felix\\Zotero\\storage\\59E98V4E\\index.html},
  note = {Page Version ID: 880988096}
}

@article{szepesvariAlgorithmsReinforcementLearning2010,
  title = {Algorithms for {{Reinforcement Learning}}},
  volume = {4},
  issn = {1939-4608},
  doi = {10.2200/S00268ED1V01Y201005AIM009},
  number = {1},
  journaltitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  shortjournal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  date = {2010-01-01},
  pages = {1-103},
  author = {Szepesvári, Csaba},
  file = {C:\\Users\\felix\\Zotero\\storage\\9BG3XKG8\\Szepesvári - 2010 - Algorithms for Reinforcement Learning.pdf;C:\\Users\\felix\\Zotero\\storage\\XFRVYKZ3\\Algorithms_ReinforcementLearning.pdf;C:\\Users\\felix\\Zotero\\storage\\3HQTSX4N\\S00268ED1V01Y201005AIM009.html}
}

@book{putermanMarkovDecisionProcesses2005,
  langid = {english},
  location = {{Hoboken, NJ}},
  title = {Markov {{Decision Processes}}: {{Discrete Stochastic Dynamic Programming}}},
  isbn = {978-0-471-72782-8},
  shorttitle = {Markov Decision Processes},
  pagetotal = {649},
  series = {Wiley Series in Probability and Statistics},
  publisher = {{Wiley-Interscience}},
  date = {2005},
  author = {Puterman, Martin L.},
  file = {C:\\Users\\felix\\Zotero\\storage\\VNB8FB4L\\Puterman - 2005 - Markov decision processes discrete stochastic dyn.pdf},
  note = {OCLC: 254152847}
}

@article{watkinsQlearning1992,
  langid = {english},
  title = {Q-Learning},
  volume = {8},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00992698},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited...},
  number = {3-4},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  date = {1992-05-01},
  pages = {279-292},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  file = {C:\\Users\\felix\\Zotero\\storage\\PF3CU2VT\\Q-learning  SpringerLink.pdf;C:\\Users\\felix\\Zotero\\storage\\WCA8JCF4\\Watkins and Dayan - 1992 - Emphasis Type=ItalicQEmphasis-learning.pdf;C:\\Users\\felix\\Zotero\\storage\\22WQBT9Y\\BF00992698.html}
}

@incollection{even-darLearningRatesQLearning2001,
  langid = {english},
  location = {{Berlin, Heidelberg}},
  title = {Learning {{Rates}} for {{Q}}-{{Learning}}},
  volume = {2111},
  isbn = {978-3-540-42343-0 978-3-540-44581-4},
  url = {http://link.springer.com/10.1007/3-540-44581-1_39},
  abstract = {In this paper we derive convergence rates for Q-learning. We show an interesting relationship between the convergence rate and the learning rate used in Q-learning. For a polynomial learning rate, one which is 1/tω at time t where ω ∈ (1/2, 1), we show that the convergence rate is polynomial in 1/(1 − γ), where γ is the discount factor. In contrast we show that for a linear learning rate, one which is 1/t at time t, the convergence rate has an exponential dependence on 1/(1 − γ). In addition we show a simple example that proves this exponential behavior is inherent for linear learning rates.},
  booktitle = {Computational {{Learning Theory}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2019-02-21},
  date = {2001},
  pages = {589-604},
  author = {Even-Dar, Eyal and Mansour, Yishay},
  editor = {Helmbold, David and Williamson, Bob},
  editorb = {Goos, G. and Hartmanis, J. and van Leeuwen, J.},
  editorbtype = {redactor},
  options = {useprefix=true},
  file = {C:\\Users\\felix\\Zotero\\storage\\BRU858KT\\Even-Dar and Mansour - 2001 - Learning Rates for Q-Learning.pdf},
  doi = {10.1007/3-540-44581-1_39}
}

@incollection{ghavamzadehSpeedyQLearning2011,
  title = {Speedy {{Q}}-{{Learning}}},
  url = {http://papers.nips.cc/paper/4251-speedy-q-learning.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 24},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-02-21},
  date = {2011},
  pages = {2411--2419},
  author = {Ghavamzadeh, Mohammad and Kappen, Hilbert J. and Azar, Mohammad G. and Munos, Rémi},
  editor = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
  file = {C:\\Users\\felix\\Zotero\\storage\\W543EWTN\\Ghavamzadeh et al. - 2011 - Speedy Q-Learning.pdf;C:\\Users\\felix\\Zotero\\storage\\H5DWZPNW\\4251-speedy-q-learning.html}
}

@article{deardenBayesianQLearning1998,
  langid = {english},
  title = {Bayesian {{Q}}-{{Learning}}},
  abstract = {A central problem in learning in complex environments is balancing exploration of untested actions against exploitation of actions that are known to be good. The beneﬁt of exploration can be estimated using the classical notion of Value of Information—the expected improvement in future decision quality that might arise from the information acquired by exploration. Estimating this quantity requires an assessment of the agent’s uncertainty about its current value estimates for states. In this paper, we adopt a Bayesian approach to maintaining this uncertain information. We extend Watkins’ Q-learning by maintaining and propagating probability distributions over the Q-values. These distributions are used to compute a myopic approximation to the value of information for each action and hence to select the action that best balances exploration and exploitation. We establish the convergence properties of our algorithm and show experimentally that it can exhibit substantial improvements over other well-known model-free exploration strategies.},
  date = {1998},
  pages = {8},
  author = {Dearden, Richard and Friedman, Nir and Russell, Stuart},
  file = {C:\\Users\\felix\\Zotero\\storage\\PUCHM243\\Dearden et al. - Bayesian Q-Learning.pdf}
}

@book{kaelblingLearningEmbeddedSystems1993,
  title = {Learning in Embedded Systems},
  publisher = {{MIT press}},
  date = {1993},
  author = {Kaelbling, Leslie Pack},
  file = {C:\\Users\\felix\\Zotero\\storage\\KKKGCHAX\\Kaelbling - 1993 - Learning in embedded systems.pdf;C:\\Users\\felix\\Zotero\\storage\\D7YDIQSR\\books.html},
  annotation = {Interval Learning:

~

Confidence Intervall around Q value -{$>$} Exploration bonus}
}

@incollection{kearnsFiniteSampleConvergenceRates1999,
  title = {Finite-{{Sample Convergence Rates}} for {{Q}}-{{Learning}} and {{Indirect Algorithms}}},
  url = {http://papers.nips.cc/paper/1531-finite-sample-convergence-rates-for-q-learning-and-indirect-algorithms.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 11},
  publisher = {{MIT Press}},
  urldate = {2019-02-21},
  date = {1999},
  pages = {996--1002},
  author = {Kearns, Michael J. and Singh, Satinder P.},
  editor = {Kearns, M. J. and Solla, S. A. and Cohn, D. A.},
  file = {C:\\Users\\felix\\Zotero\\storage\\ZJVVL9QT\\Kearns and Singh - 1999 - Finite-Sample Convergence Rates for Q-Learning and.pdf;C:\\Users\\felix\\Zotero\\storage\\IRELKWUT\\1531-finite-sample-convergence-rates-for-q-learning-and-indirect-algorithms.html}
}

@incollection{langeBatchReinforcementLearning2012,
  langid = {english},
  location = {{Berlin, Heidelberg}},
  title = {Batch {{Reinforcement Learning}}},
  isbn = {978-3-642-27645-3},
  abstract = {Batch reinforcement learning is a subfield of dynamic programming-based reinforcement learning. Originally defined as the task of learning the best possible policy from a fixed set of a priori-known transition samples, the (batch) algorithms developed in this field can be easily adapted to the classical online case, where the agent interacts with the environment while learning. Due to the efficient use of collected data and the stability of the learning process, this research area has attracted a lot of attention recently. In this chapter, we introduce the basic principles and the theory behind batch reinforcement learning, describe the most important algorithms, exemplarily discuss ongoing research within this field, and briefly survey real-world applications of batch reinforcement learning.},
  booktitle = {Reinforcement {{Learning}}: {{State}}-of-the-{{Art}}},
  series = {Adaptation, {{Learning}}, and {{Optimization}}},
  publisher = {{Springer Berlin Heidelberg}},
  date = {2012},
  pages = {45-73},
  keywords = {Batch Learning,Multiagent System,Neural Information Processing System,Policy Iteration,Reinforcement Learning},
  author = {Lange, Sascha and Gabel, Thomas and Riedmiller, Martin},
  editor = {Wiering, Marco and van Otterlo, Martijn},
  options = {useprefix=true},
  file = {C:\\Users\\felix\\Zotero\\storage\\P3KVYLAG\\Lange et al. - 2012 - Batch Reinforcement Learning.pdf},
  doi = {10.1007/978-3-642-27645-3_2}
}

@article{schmidhuberDeepLearningNeural2015,
  title = {Deep Learning in Neural Networks: {{An}} Overview},
  volume = {61},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2014.09.003},
  shorttitle = {Deep Learning in Neural Networks},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  date = {2015-01-01},
  pages = {85-117},
  keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning},
  author = {Schmidhuber, Jürgen},
  file = {C:\\Users\\felix\\Zotero\\storage\\6HMK9TRC\\Schmidhuber - 2015 - Deep learning in neural networks An overview.pdf;C:\\Users\\felix\\Zotero\\storage\\4MQKCMJ8\\S0893608014002135.html}
}

@inproceedings{riedmillerNeuralFittedIteration2005,
  langid = {english},
  title = {Neural {{Fitted Q Iteration}} – {{First Experiences}} with a {{Data Efficient Neural Reinforcement Learning Method}}},
  isbn = {978-3-540-31692-3},
  abstract = {This paper introduces NFQ, an algorithm for efficient and effective training of a Q-value function represented by a multi-layer perceptron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.},
  booktitle = {Machine {{Learning}}: {{ECML}} 2005},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer Berlin Heidelberg}},
  date = {2005},
  pages = {317-328},
  author = {Riedmiller, Martin},
  editor = {Gama, João and Camacho, Rui and Brazdil, Pavel B. and Jorge, Alípio Mário and Torgo, Luís},
  file = {C:\\Users\\felix\\Zotero\\storage\\FB2I6A25\\Riedmiller - 2005 - Neural Fitted Q Iteration – First Experiences with.pdf}
}

@inproceedings{pathakCuriosityDrivenExplorationSelfSupervised2017,
  langid = {english},
  location = {{Honolulu, HI, USA}},
  title = {Curiosity-{{Driven Exploration}} by {{Self}}-{{Supervised Prediction}}},
  isbn = {978-1-5386-0733-6},
  doi = {10.1109/CVPRW.2017.70},
  abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent’s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difﬁculties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efﬁciently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  publisher = {{IEEE}},
  date = {2017-07},
  pages = {488-489},
  author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
  file = {C:\\Users\\felix\\Zotero\\storage\\4LQFLK29\\Pathak et al. - 2017 - Curiosity-Driven Exploration by Self-Supervised Pr.pdf}
}

@article{borkarMethodConvergenceStochastic2000,
  title = {The {{O}}.{{D}}.{{E}}. {{Method}} for {{Convergence}} of {{Stochastic Approximation}} and {{Reinforcement Learning}}},
  volume = {38},
  issn = {0363-0129},
  doi = {10.1137/S0363012997331639},
  abstract = {It is shown here that stability of the stochastic approximation algorithm is implied by the asymptotic stability of the origin for an associated ODE. This in turn implies convergence of the algorithm. Several specific classes of algorithms are considered as applications. It is found that the results provide (i) a simpler derivation of known results for reinforcement learning algorithms; (ii) a proof for the first time that a class of asynchronous stochastic approximation algorithms are convergent without using any a priori assumption of stability; (iii) a proof for the first time that asynchronous adaptive critic and Q-learning algorithms are convergent for the average cost optimal control problem.},
  number = {2},
  journaltitle = {SIAM Journal on Control and Optimization},
  shortjournal = {SIAM J. Control Optim.},
  date = {2000-01-01},
  pages = {447-469},
  author = {Borkar, V. and Meyn, S.},
  file = {C:\\Users\\felix\\Zotero\\storage\\VZ5IS6XX\\Borkar and Meyn - 2000 - The O.D.E. Method for Convergence of Stochastic Ap.pdf;C:\\Users\\felix\\Zotero\\storage\\6TP8FI9Z\\S0363012997331639.html}
}

@book{suttonReinforcementLearningIntroduction1998,
  langid = {english},
  location = {{Cambridge, Mass. [u.a.]}},
  title = {Reinforcement Learning: An Introduction},
  isbn = {978-0-262-19398-6},
  shorttitle = {Reinforcement Learning},
  pagetotal = {xviii+322},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{MIT Press}},
  date = {1998},
  keywords = {Reinforcement learning,Lernen,Reinforcement learning (Machine learning),Bestärkendes Lernen,Operantes Lernen,Reinforcement learning (Machine learning); Reinforcement learning (Machine learning),Verstärkendes Lernen,Verstärkungslernen},
  author = {Sutton, Richard S.},
  editora = {Barto, Andrew},
  editoratype = {collaborator}
}

@incollection{jaakkolaConvergenceStochasticIterative1994,
  title = {Convergence of {{Stochastic Iterative Dynamic Programming Algorithms}}},
  url = {http://papers.nips.cc/paper/764-convergence-of-stochastic-iterative-dynamic-programming-algorithms.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 6},
  publisher = {{Morgan-Kaufmann}},
  urldate = {2019-02-26},
  date = {1994},
  pages = {703--710},
  author = {Jaakkola, Tommi and Jordan, Michael I. and Singh, Satinder P.},
  editor = {Cowan, J. D. and Tesauro, G. and Alspector, J.},
  file = {C:\\Users\\felix\\Zotero\\storage\\PWH5X7YF\\Jaakkola et al. - 1994 - Convergence of Stochastic Iterative Dynamic Progra.pdf;C:\\Users\\felix\\Zotero\\storage\\PK2RJG45\\764-convergence-of-stochastic-iterative-dynamic-programming-algorithms.html}
}

@book{suttonReinforcementLearningIntroduction2018a,
  langid = {english},
  location = {{Cambridge, Massachusetts}},
  title = {Reinforcement Learning: An Introduction},
  edition = {Second edition},
  isbn = {978-0-262-03924-6},
  shorttitle = {Reinforcement Learning},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  pagetotal = {526},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {{The MIT Press}},
  date = {2018},
  keywords = {Reinforcement learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  file = {C:\\Users\\felix\\Zotero\\storage\\6YKDX3JJ\\RLbook2018.pdf}
}

@article{robbinsStochasticApproximationMethod1951,
  title = {A {{Stochastic Approximation Method}}},
  volume = {22},
  issn = {0003-4851},
  url = {https://www.jstor.org/stable/2236626},
  abstract = {[Let M(x) denote the expected value at level x of the response to a certain experiment. M(x) is assumed to be a monotone function of x but is unknown to the experimenter, and it is desired to find the solution x = θ of the equation M(x) = α, where α is a given constant. We give a method for making successive experiments at levels x1,x2,⋯ in such a way that xn will tend to θ in probability.]},
  number = {3},
  journaltitle = {The Annals of Mathematical Statistics},
  urldate = {2019-02-27},
  date = {1951},
  pages = {400-407},
  author = {Robbins, Herbert and Monro, Sutton},
  file = {C:\\Users\\felix\\Zotero\\storage\\4CSGQDU7\\Robbins and Monro - 1951 - A Stochastic Approximation Method.pdf}
}

@article{dermanDvoretzkyStochasticApproximation1959a,
  title = {On {{Dvoretzky}}'s {{Stochastic Approximation Theorem}}},
  volume = {30},
  issn = {0003-4851},
  url = {https://www.jstor.org/stable/2237107},
  number = {2},
  journaltitle = {The Annals of Mathematical Statistics},
  urldate = {2019-02-28},
  date = {1959},
  pages = {601-606},
  author = {Derman, C. and Sacks, J.},
  file = {D:\\Google Drive\\ZotFiles\\1959Derman_Sacks\\Derman_Sacks_1959_On Dvoretzky's Stochastic Approximation Theorem.pdf}
}

@article{blumApproximationMethodsWhich1954,
  langid = {english},
  title = {Approximation {{Methods}} Which {{Converge}} with {{Probability}} One},
  volume = {25},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177728794},
  abstract = {Let H(y∣x)H(y∣x)H(y\textbackslash{}mid x) be a family of distribution functions depending upon a real parameter x,x,x, and let M(x)=∫∞−∞ydH(y∣x)M(x)=∫−∞∞ydH(y∣x)M(x) = \textbackslash{}int\^\textbackslash{}infty\_\{-\textbackslash{}infty\} y dH(y \textbackslash{}mid x) be the corresponding regression function. It is assumed M(x)M(x)M(x) is unknown to the experimenter, who is, however, allowed to take observations on H(y∣x)H(y∣x)H(y\textbackslash{}mid x) for any value x.x.x. Robbins and Monro [1] give a method for defining successively a sequence \{xn\}\{xn\}\textbackslash\{x\_n\textbackslash\} such that xnxnx\_n converges to θθ\textbackslash{}theta in probability, where θθ\textbackslash{}theta is a root of the equation M(x)=αM(x)=αM(x) = \textbackslash{}alpha and αα\textbackslash{}alpha is a given number. Wolfowitz [2] generalizes these results, and Kiefer and Wolfowitz [3], solve a similar problem in the case when M(x)M(x)M(x) has a maximum at x=θ.x=θ.x = \textbackslash{}theta. Using a lemma due to Loeve [4], we show that in both cases xnxnx\_n converges to θθ\textbackslash{}theta with probability one, under weaker conditions than those imposed in [2] and [3]. Further we solve a similar problem in the case when M(x)M(x)M(x) is the median of H(y∣x).H(y∣x).H(y \textbackslash{}mid x).},
  number = {2},
  journaltitle = {The Annals of Mathematical Statistics},
  shortjournal = {Ann. Math. Statist.},
  date = {1954-06},
  pages = {382-386},
  author = {Blum, Julius R.},
  file = {D:\\Google Drive\\ZotFiles\\1954Blum\\Blum_1954_Approximation Methods which Converge with Probability one.pdf;C:\\Users\\felix\\Zotero\\storage\\DKVXAE9V\\1177728794.html}
}

@article{tsitsiklisAsynchronousStochasticApproximation1994,
  langid = {english},
  title = {Asynchronous Stochastic Approximation and {{Q}}-Learning},
  volume = {16},
  issn = {1573-0565},
  doi = {10.1007/BF00993306},
  abstract = {We provide some general results on the convergence of a class of stochastic approximation algorithms and their parallel and asynchronous variants. We then use these results to study the Q-learning algorithm, a reinforcement learning method for solving Markov decision problems, and establish its convergence under conditions more general than previously available.},
  number = {3},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  date = {1994-09-01},
  pages = {185-202},
  keywords = {Reinforcement learning,dynamic programming,Q-learning,stochastic approximation},
  author = {Tsitsiklis, John N.},
  file = {D:\\Google Drive\\ZotFiles\\1994Tsitsiklis\\Tsitsiklis_1994_Asynchronous stochastic approximation and Q-learning.pdf}
}

@inproceedings{loeveAlmostSureConvergence1951,
  title = {On Almost Sure Convergence},
  booktitle = {Proceedings of the {{Second Berkeley Symposium}} on {{Mathematical Statistics}} and {{Probability}}},
  publisher = {{The Regents of the University of California}},
  date = {1951},
  author = {Loève, Michel},
  file = {D:\\Google Drive\\ZotFiles\\1951Loève\\Loève_1951_On almost sure convergence.pdf}
}

@article{jaakkolaConvergenceStochasticIterative1994a,
  title = {On the {{Convergence}} of {{Stochastic Iterative Dynamic Programming Algorithms}}},
  volume = {6},
  issn = {0899-7667},
  doi = {10.1162/neco.1994.6.6.1185},
  abstract = {Recent developments in the area of reinforcement learning have yielded a number of new algorithms for the prediction and control of Markovian environments. These algorithms, including the TD(λ) algorithm of Sutton (1988) and the Q-learning algorithm of Watkins (1989), can be motivated heuristically as approximations to dynamic programming (DP). In this paper we provide a rigorous proof of convergence of these DP-based learning algorithms by relating them to the powerful techniques of stochastic approximation theory via a new convergence theorem. The theorem establishes a general class of convergent algorithms to which both TD(λ) and Q-learning belong.},
  number = {6},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  date = {1994-11-01},
  pages = {1185-1201},
  author = {Jaakkola, Tommi and Jordan, Michael I. and Singh, Satinder P.},
  file = {D:\\Google Drive\\ZotFiles\\1994Jaakkola et al\\Jaakkola et al_1994_On the Convergence of Stochastic Iterative Dynamic Programming Algorithms.pdf;C:\\Users\\felix\\Zotero\\storage\\U3ZKMDIZ\\neco.1994.6.6.html}
}

@inproceedings{vanseijenTrueOnlineTD2014,
  title = {True {{Online TD}}(\$\textbackslash{}lambda\$)},
  url = {http://dl.acm.org/citation.cfm?id=3044805.3044884},
  abstract = {TD(λ) is a core algorithm of modern reinforcement learning. Its appeal comes from its equivalence to a clear and conceptually simple forward view, and the fact that it can be implemented online in an inexpensive manner. However, the equivalence between TD(λ) and the forward view is exact only for the off-line version of the algorithm (in which updates are made only at the end of each episode). In the online version of TD(λ) (in which updates are made at each step, which generally performs better and is always used in applications) the match to the forward view is only approximate. In a sense this is unavoidable for the conventional forward view, as it itself presumes that the estimates are unchanging during an episode. In this paper we introduce a new forward view that takes into account the possibility of changing estimates and a new variant of TD(λ) that exactlyachieves it. Our algorithm uses a new form of eligibility trace similar to but different from conventional accumulating and replacing traces. The overall computational complexity is the same as TD(λ), even when using function approximation. In our empirical comparisons, our algorithm outperformed TD(λ) in all of its variations. It seems, by adhering more truly to the original goal of TD(λ)--matching an intuitively clear forward view even in the online case--that we have found a new algorithm that simply improves on classical TD(λ).},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 32},
  series = {{{ICML}}'14},
  publisher = {{JMLR.org}},
  urldate = {2019-03-08},
  date = {2014},
  pages = {I-692--I-700},
  author = {Van Seijen, Harm and Sutton, Richard S.},
  file = {D:\\Google Drive\\ZotFiles\\2014Van Seijen_Sutton\\Van Seijen_Sutton_2014_True Online TD($-lambda$).pdf},
  venue = {Beijing, China}
}

@article{blackwellDiscountedDynamicProgramming1965,
  langid = {english},
  title = {Discounted {{Dynamic Programming}}},
  volume = {36},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177700285},
  abstract = {Project Euclid - mathematics and statistics online},
  number = {1},
  journaltitle = {The Annals of Mathematical Statistics},
  shortjournal = {Ann. Math. Statist.},
  date = {1965-02},
  pages = {226-235},
  author = {Blackwell, David},
  file = {D:\\Google Drive\\ZotFiles\\1965Blackwell\\Blackwell_1965_Discounted Dynamic Programming.pdf;C:\\Users\\felix\\Zotero\\storage\\63YH8YGS\\1177700285.html}
}

@report{dvoretzkyStochasticApproximation1956,
  langid = {english},
  title = {On {{Stochastic Approximation}}},
  url = {https://apps.dtic.mil/docs/citations/AD1028378},
  abstract = {Stochastic approximation is concerned with schemes converging to some sought valuewhen, due to the stochastic nature of the problem, the observations involve errors. Theinteresting schemes are those which are self-correcting, that is, in which a mistake alwaystends to be wiped out in the limit, and in which the convergence to the desired value isof some specified nature, for example, it is mean-square convergence. The typical exampleof such a scheme is the original one of Robbins-Monro [7] for approximating, undersuitable conditions, the point where a regression function assumes a given value. Robbinsand Monro have proved mean-square convergence to the root; Wolfowitz [8] showedthat under weaker assumptions there is still convergence in probability to the root; andBlum [11 demonstrated that, under still weaker assumptions, there is not only convergencein probability but even convergence with probability 1. Kiefer and Wolfowitz [6]have devised a method for approximating the point where the maximum of a regressionfunction occurs. They proved that under suitable conditions there is convergence in probabilityand Blum [1] has weakened somewhat the conditions and strengthened the conclusionto convergence with probability 1.},
  institution = {{COLUMBIA UNIVERSITY New York City United States}},
  urldate = {2019-03-16},
  date = {1956-01-01},
  author = {Dvoretzky, Aryeh},
  file = {D:\\Google Drive\\ZotFiles\\1956Dvoretzky\\Dvoretzky_1956_On Stochastic Approximation.pdf;C:\\Users\\felix\\Zotero\\storage\\XCBWQQJ7\\AD1028378.html}
}

@article{suttonLearningPredictMethods1988,
  langid = {english},
  title = {Learning to Predict by the Methods of Temporal Differences},
  volume = {3},
  issn = {1573-0565},
  doi = {10.1007/BF00115009},
  abstract = {This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
  number = {1},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  date = {1988-08-01},
  pages = {9-44},
  keywords = {connectionism,credit assignment,evaluation functions,Incremental learning,prediction},
  author = {Sutton, Richard S.},
  file = {D:\\Google Drive\\ZotFiles\\1988Sutton\\Sutton_1988_Learning to predict by the methods of temporal differences.pdf}
}

@article{linSelfimprovingReactiveAgents1992,
  langid = {english},
  title = {Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching},
  volume = {8},
  issn = {1573-0565},
  doi = {10.1007/BF00992699},
  abstract = {To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.This paper compares eight reinforcement learning frameworks:adaptive heuristic critic (AHC) learning due to Sutton,Q-learning due to Watkins, and three extensions to both basic methods for speeding up learning. The three extensions are experience replay, learning action models for planning, and teaching. The frameworks were investigated using connectionism as an approach to generalization. To evaluate the performance of different frameworks, a dynamic environment was used as a testbed. The environment is moderately complex and nondeterministic. This paper describes these frameworks and algorithms in detail and presents empirical evaluation of the frameworks.},
  number = {3},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  date = {1992-05-01},
  pages = {293-321},
  keywords = {Reinforcement learning,connectionist networks,planning,teaching},
  author = {Lin, Long-Ji},
  file = {D:\\Google Drive\\ZotFiles\\1992Lin\\Lin_1992_Self-improving reactive agents based on reinforcement learning, planning and.pdf}
}

@incollection{suttonIntegratedArchitecturesLearning1990,
  location = {{San Francisco (CA)}},
  title = {Integrated {{Architectures}} for {{Learning}}, {{Planning}}, and {{Reacting Based}} on {{Approximating Dynamic Programming}}},
  isbn = {978-1-55860-141-3},
  url = {http://www.sciencedirect.com/science/article/pii/B9781558601413500304},
  abstract = {This paper extends previous work with Dyna, a class of architectures for intelligent systems based on approximating dynamic programming methods. Dyna architectures integrate trial-and-error (reinforcement) learning and execution-time planning into a single process operating alternately on the world and on a learned model of the world. In this paper, I present and show results for two Dyna architectures. The Dyna-PI architecture is based on dynamic programming's policy iteration method and can be related to existing AI ideas such as evaluation functions and universal plans (reactive systems). Using a navigation task, results are shown for a simple Dyna-PI system that simultaneously learns by trial and error, learns a world model, and plans optimal routes using the evolving world model. The Dyna-Q architecture is based on Watkins's Q-learning, a new kind of reinforcement learning. Dyna-Q uses a less familiar set of data structures than does Dyna-PI, but is arguably simpler to implement and use. We show that Dyna-Q architectures are easy to adapt for use in changing environments.},
  booktitle = {Machine {{Learning Proceedings}} 1990},
  publisher = {{Morgan Kaufmann}},
  urldate = {2019-03-21},
  date = {1990-01-01},
  pages = {216-224},
  author = {Sutton, Richard S.},
  editor = {Porter, Bruce and Mooney, Raymond},
  file = {D:\\Google Drive\\ZotFiles\\1990Sutton\\Sutton_1990_Integrated Architectures for Learning, Planning, and Reacting Based on.pdf;C:\\Users\\felix\\Zotero\\storage\\MZFI3CDR\\B9781558601413500304.html},
  doi = {10.1016/B978-1-55860-141-3.50030-4}
}


