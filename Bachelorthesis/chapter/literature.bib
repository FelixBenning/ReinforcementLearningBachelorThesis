
@online{ProofVerificationBlackwell,
  title = {Proof Verification - {{Blackwell}}'s Condition for a Contraction: {{Why}} Is Boundedness Neccessary?},
  url = {https://math.stackexchange.com/questions/1087885/blackwells-condition-for-a-contraction-why-is-boundedness-neccessary?rq=1},
  shorttitle = {Proof Verification - {{Blackwell}}'s Condition for a Contraction},
  journaltitle = {Mathematics Stack Exchange},
  urldate = {2019-02-04},
  file = {C:\\Users\\felix\\Zotero\\storage\\D88I6ZBY\\blackwells-condition-for-a-contraction-why-is-boundedness-neccessary.html}
}

@article{whiteRealApplicationsMarkov1985,
  langid = {english},
  title = {Real {{Applications}} of {{Markov Decision Processes}}},
  volume = {15},
  issn = {0092-2102, 1526-551X},
  url = {http://pubsonline.informs.org/doi/abs/10.1287/inte.15.6.73},
  doi = {10.1287/inte.15.6.73},
  number = {6},
  journaltitle = {Interfaces},
  urldate = {2019-02-05},
  date = {1985-12},
  pages = {73-83},
  author = {White, Douglas J.},
  file = {C:\\Users\\felix\\Zotero\\storage\\DWX2M69R\\White - 1985 - Real Applications of Markov Decision Processes.pdf}
}

@article{reesBiblatexCheatSheet,
  langid = {english},
  title = {Biblatex {{Cheat Sheet}}},
  pages = {2},
  author = {Rees, Clea F},
  file = {C:\\Users\\felix\\Zotero\\storage\\DVSDHAWK\\Rees - Biblatex Cheat Sheet.pdf}
}

@inreference{ReinforcementLearning2019,
  langid = {english},
  title = {Reinforcement Learning},
  url = {https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&oldid=880988096},
  abstract = {Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. The problem, due to its generality, is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In the operations research and control literature, reinforcement learning is called approximate dynamic programming, or neuro-dynamic programming.
The problems of interest in reinforcement learning have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation, particularly in the absence of a mathematical model of the environment. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.
In machine learning, the environment is typically formulated as a Markov Decision Process (MDP), as many reinforcement learning algorithms for this context utilize dynamic programming techniques. The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.Reinforcement learning is considered as one of three machine learning paradigms, alongside supervised learning and unsupervised learning. It differs from supervised learning in that correct input/output pairs need not be presented, and sub-optimal actions need not be explicitly corrected. Instead the focus is on performance, which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). The exploration vs. exploitation trade-off has been most thoroughly studied through the multi-armed bandit problem and in finite MDPs.},
  booktitle = {Wikipedia},
  urldate = {2019-02-06},
  date = {2019-01-30T19:10:08Z},
  file = {C:\\Users\\felix\\Zotero\\storage\\59E98V4E\\index.html},
  note = {Page Version ID: 880988096}
}

@article{szepesvariAlgorithmsReinforcementLearning2010,
  title = {Algorithms for {{Reinforcement Learning}}},
  volume = {4},
  issn = {1939-4608},
  url = {https://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009},
  doi = {10.2200/S00268ED1V01Y201005AIM009},
  number = {1},
  journaltitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  shortjournal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  urldate = {2019-02-06},
  date = {2010-01-01},
  pages = {1-103},
  author = {Szepesvári, Csaba},
  file = {C:\\Users\\felix\\Zotero\\storage\\9BG3XKG8\\Szepesvári - 2010 - Algorithms for Reinforcement Learning.pdf;C:\\Users\\felix\\Zotero\\storage\\XFRVYKZ3\\Algorithms_ReinforcementLearning.pdf;C:\\Users\\felix\\Zotero\\storage\\3HQTSX4N\\S00268ED1V01Y201005AIM009.html}
}

@incollection{putermanChapterMarkovDecision1990,
  title = {Chapter 8 {{Markov}} Decision Processes},
  volume = {2},
  url = {http://www.sciencedirect.com/science/article/pii/S0927050705801720},
  abstract = {This chapter presents theory, applications, and computational methods for Markov Decision Processes (MDP's). MDP's are a class of stochastic sequential decision processes in which the cost and transition functions depend only on the current state of the system and the current action. These models have been applied in a wide range of subject areas, most notably in queueing and inventory control. A sequential decision process is a model for dynamic system under the control of a decision maker. Sequential decision processes are classified according to the times (epochs) at which decisions are made, the length of the decision making horizon, the mathematical properties of the state and action spaces, and the optimality criteria. The focus of this chapter is problems in which decisions are made periodically at discrete time points. The state and action sets are either finite, countable, compact or Borel; their characteristics determine the form of the reward and transition probability functions. The optimality criteria considered in the chapter include finite and infinite horizon expected total reward, infinite horizon expected total discounted reward, and average expected reward. The main objectives in analyzing sequential decision processes in general and MDP's in particular include (1) providing an optimality equation that characterizes the supremal value of the objective function, (2) characterizing the form of an optimal policy if it exists, (3) developing efficient computational procedures for finding policies thatare optimal or close to optimal. The optimality or Bellman equation is the basic entity in MDP theory and almost all existence, characterization, and computational results are based on its analysis.},
  booktitle = {Handbooks in {{Operations Research}} and {{Management Science}}},
  series = {Stochastic {{Models}}},
  publisher = {{Elsevier}},
  urldate = {2019-02-06},
  date = {1990-01-01},
  pages = {331-434},
  author = {Puterman, Martin L.},
  file = {C:\\Users\\felix\\Zotero\\storage\\TRVY63HZ\\Puterman - 1990 - Chapter 8 Markov decision processes.pdf;C:\\Users\\felix\\Zotero\\storage\\LIDEZKNR\\S0927050705801720.html},
  doi = {10.1016/S0927-0507(05)80172-0}
}

@book{putermanMarkovDecisionProcesses2014,
  langid = {german},
  title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  isbn = {978-1-118-62587-3},
  shorttitle = {Markov Decision Processes},
  abstract = {The Wiley-Interscience Paperback Series consists of selected books that have been made more accessible to consumers in an effort to increase global appeal and general circulation. With these new unabridged softcover volumes, Wiley hopes to extend the lives of these works by making them available to future generations of statisticians, mathematicians, and scientists. "This text is unique in bringing together so many results hitherto found only in part in other texts and papers. . . . The text is fairly self-contained, inclusive of some basic mathematical results needed, and provides a rich diet of examples, applications, and exercises. The bibliographical material at the end of each chapter is excellent, not only from a historical perspective, but because it is valuable for researchers in acquiring a good perspective of the MDP research potential." —Zentralblatt fur Mathematik ". . . it is of great value to advanced-level students, researchers, and professional practitioners of this field to have now a complete volume (with more than 600 pages) devoted to this topic. . . . Markov Decision Processes: Discrete Stochastic Dynamic Programming represents an up-to-date, unified, and rigorous treatment of theoretical and computational aspects of discrete-time Markov decision processes." —Journal of the American Statistical Association},
  pagetotal = {615},
  publisher = {{John Wiley \& Sons}},
  date = {2014-08-28},
  keywords = {Mathematics / Probability & Statistics / General,Mathematics / Probability & Statistics / Stochastic Processes},
  author = {Puterman, Martin L.},
  eprinttype = {googlebooks},
  annotation = {see 2.1.3 Definition}
}

@article{hashemiCompositionalReasoningInterval2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1607.08484},
  primaryClass = {cs},
  title = {Compositional {{Reasoning}} for {{Interval Markov Decision Processes}}},
  url = {http://arxiv.org/abs/1607.08484},
  abstract = {Model checking probabilistic CTL properties of Markov decision processes with convex uncertainties has been recently investigated by Puggelli et al. Such model checking algorithms typically suffer from the state space explosion. In this paper, we address probabilistic bisimulation to reduce the size of such an MDP while preserving the probabilistic CTL properties it satisfies. In particular, we discuss the key ingredients to build up the operations of parallel composition for composing interval MDP components at run-time. More precisely, we investigate how the parallel composition operator for interval MDPs can be defined so as to arrive at a congruence closure. As a result, we show that probabilistic bisimulation for interval MDPs is congruence with respect to two facets of parallelism, namely synchronous product and interleaving.},
  urldate = {2019-02-06},
  date = {2016-07-28},
  keywords = {68Q10; 68Q60; 68Q85; 68Q25,Computer Science - Logic in Computer Science},
  author = {Hashemi, Vahid and Hermanns, Holger and Turrini, Andrea},
  file = {C:\\Users\\felix\\Zotero\\storage\\S3ZVAJ5Z\\Hashemi et al. - 2016 - Compositional Reasoning for Interval Markov Decisi.pdf;C:\\Users\\felix\\Zotero\\storage\\MZG6XMMG\\1607.html}
}

@book{putermanMarkovDecisionProcesses2005,
  langid = {english},
  location = {{Hoboken, NJ}},
  title = {Markov {{Decision Processes}}: {{Discrete Stochastic Dynamic Programming}}},
  isbn = {978-0-471-72782-8},
  shorttitle = {Markov Decision Processes},
  pagetotal = {649},
  series = {Wiley Series in Probability and Statistics},
  publisher = {{Wiley-Interscience}},
  date = {2005},
  author = {Puterman, Martin L.},
  file = {C:\\Users\\felix\\Zotero\\storage\\VNB8FB4L\\Puterman - 2005 - Markov decision processes discrete stochastic dyn.pdf},
  note = {OCLC: 254152847}
}


