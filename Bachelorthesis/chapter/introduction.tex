% !TEX root = ../BScWIMEngl.tex

\chapter{Introduction}
    Let us consider a system to be a intelligent, if it collects information from its sensors and transforms this information into an ``appropriate'' response. Then the question where this input comes from (eyes, ears, cameras, etc.), or what kind of input it is (visual, audio, etc.), is as irrelevant for this definition, as the question what a response is (motor activity, sound, etc.). An intelligent system is simply a function mapping inputs to ``appropriate'' outputs.  The question what ``appropriate'' means, is in contrast a lot more important and difficult to answer. But if we assume that there exists a ``correct'' response, then this correct response is itself a function from the input space to the output space. Artificial intelligence is therefore simply an artificial implementation of this correct response function. 

    If we have complete knowledge of this function, we can encode this function as an executable program manually. But more often than not, we do not really understand the rules according to which the inputs are supposed to be transformed into outputs. 

    For this reason we might want a machine to ``learn'' these rules by itself, i.e. we want it to approximate the correct response function. There are two larger subcategories in this category of \emph{machine learning}. 
    
    In \emph{supervised learning} we do not know the correct response function, but we know the correct response for certain inputs intuitively. An example for this case is image classification of animals. There, we do not know how the input (pixels of the picture) map to the output (name of the animal), but we can tell for a given picture what animal is depicted intuitively, i.e. we can provide examples of the correct response function to the learning algorithm. Supervised Learning is therefore concerned with generalization from examples. And while approximating a function with a finite sample of (error prone) function evaluations is a relatively old problem in numerical analysis and statistics, supervised learning is often associated with more recent approaches like artificial neuronal networks. 

    While we are still able to provide examples in supervised learning, \emph{unsupervised learning} has to work with even less. This category includes clustering and principle component analysis, as well as \emph{reinforcement learning}. Reinforcement learning is applied to problems, where we ``know the correct response if we see it'', but can not provide examples ourselves. This is for example the case with walking. We know how walking looks like, but we have a hard time giving an example for the correct output signals sent to the motors in the leg. It is also used in cases where we do not know the correct response and have never seen it, but are able to compare different response functions. Examples range from games like chess, to profit maximization of a company. 

    In general reinforcement learning is used in cases, where humans or the environment in general can rate the solution, and provide rewards for good outcomes. Similar to the conditioning of animals, desirable actions are \emph{reinforced} with rewards. 

    But in order to write algorithms, which maximize their rewards, we first need to formulate the relationship of possibly delayed rewards with actions (outputs) of the learning algorithm in certain states (given certain inputs). The model for this relationship -- which virtually all modern reinforcement learning algorithms are based on -- is the Markov Decision Process. 
    
    We will therefore introduce this model and its properties in the first chapter, continue with a review of common reinforcement algorithms in the second chapter, and explore the relation between the theory of stochastic approximation and reinforcement learning in the third chapter, which yields proofs of convergence to the optimal policy for a number of reinforcement algorithms. \fxnote{check}
\endinput
