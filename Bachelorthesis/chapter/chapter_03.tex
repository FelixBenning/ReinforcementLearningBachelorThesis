% !TEX root = ../BScWIMEngl.tex  

\chapter{Stochastic Approximation -- Convergence}\label{stoch apprx}
\section{Introduction}
We motivated the concept of learning rates, by unwinding the arithmetic mean for iid \(X_i\) (c.f. \ref{unwinding the mean}), which resulted in
\begin{align*}
    \widebar{X}_n
    &=\widebar{X}_{n-1} +\mfrac{1}{n}(X_n-\widebar{X}_{n-1})\\
    &=\widebar{X}_{n-1} +\alpha_n(X_n-\widebar{X}_{n-1}).
\end{align*}
The structure of this iteration has similarities to numeric approximation algorithms like newton's method
\[
    \theta_{n+1} = \theta_n - \mfrac{1}{g'(\theta_n)}g(\theta_n),
\]
where the learning rate would be \(-1/f'(\theta_n)\) in this case. Now if we set
\[
    f(\theta) = \E[X_1]-\theta,
\]
then \(X_n-\theta\) could be interpreted as a blurred function evaluation of \(f(\theta)\), since
\[
    X_n-\theta = \underbracket[1pt]{\E[X_1] - \theta}_{
        =f(\theta)
        } + \underbracket[1pt]{X_n - \E[X_1]}_{\eqqcolon \delta_n}.
\]
This leads us straight to the algorithm proposed by \textcite{robbinsStochasticApproximationMethod1951}
\begin{align}
    \theta_{n+1}= \theta_n +\alpha_n Y_n(\theta_n), \label{robbins-monro algo}
\end{align}
which converges to the zero \(\theta^*\) of a non-increasing function \(g(\theta)=\E[Y_n(\theta)]\) with probability one, if \(Y_n(\theta)\) is bounded, \(g'(\theta^*)\) exists and is negative, and the learning rate satisfies
\begin{align}
    \sum_{n=1}^\infty \alpha_n = \infty \qquad \text{and} \qquad \sum_{n=1}^\infty \alpha_n^2 <\infty.\label{learning rate conditions}
\end{align}

This algorithm was the first algorithm in a group of algorithms, which is now known as stochastic approximation. And while most of the technical requirements for convergence change with the algorithm, the requirements (\ref{learning rate conditions}) seems to be essential to this problem formulation. Although general theorems like \citeauthor{dvoretzkyStochasticApproximation1956}'s avoid this, by replacing the explicit functional form of (\ref{robbins-monro algo}) with a general transformation, which has to fulfil constraints formulated with multiple sequences. And these sequences individually fulfil only parts of (\ref{learning rate conditions}), but both parts of this requirement still shine through.

\section{Learning Rates}
To interpret this requirement, examining the special case \(\alpha_n=n^{-\beta}\) is instructive.
\[
    \sum_{n=1}^\infty n^{-\beta}=\infty \qquad \text{and}\qquad \sum_{n=1}^\infty n^{-2\beta}<\infty
\]
forces \(1\ge \beta >1/2\). Therefore the learning rate needs to behave similar to a sequence between \(1/n\) and \(1/\sqrt{n}\). 

The second part of this requirement is somewhat reminiscent of the central limit theorem, where the variance is kept positive by dividing the sum of estimates only by \(1/\sqrt{n}\). Since we know for any \(\beta>1/2\):
\[
    \text{Var}\left(n^{-\beta}\sum_{k=1}^n X_k \right)=n^{1-2\beta}\text{Var}(X_1)\to 0 \qquad (n\to \infty)
\]

But if there is a deeper relationship between these two cases, it is not as obvious as it might seem from that initial observation. Because our ``unwinding'' strategy does not work for \(\beta \neq 1\). In fact the weights generated by learning rates inherently sum to 1, assuming \(\alpha_1=1\), which is a result of the assumption 
\[
    \theta_1=X_1=\theta_0+1(X_1-\theta_0).
\]
These weights can be calculated inductively as follows:
\begin{align*}
    \theta_n&=\theta_{n-1} + \alpha_n(X_n -\theta_{n-1})\\
    &=(1-\alpha_n)\underbracket[1pt]{\theta_{n-1}}_{
        \lxeq{\text{ind.}}
        \mathrlap{\sum_{k=1}^{n-1} \left(\alpha_k \prod_{i=k+1}^{n-1}(1-\alpha_i)\right)X_k \xeq{n=2} X_1}
    }
    + \alpha_n X_n\\
    & = \sum_{k=1}^n \underbracket[1pt]{
        \left(\alpha_k \prod_{i=k+1}^{n}(1-\alpha_i)\right)
        }_{
            \eqqcolon w_k^{(n)}
        } X_k
\end{align*}
And by induction the sum of these weights is equal to 1:
\begin{align*}
    \sum_{k=1}^n w_k^{(n)} = (1-\alpha_n)\underbracket[1pt]{\sum_{k=1}^{n-1}w_k^{(n-1)}}_{=1} + \alpha_n =1
\end{align*}
Setting the learning rate, corresponds therefore with shifts in the weight distribution of the observations.


A common interpretation of the first part of (\ref{learning rate conditions}) is, that no matter how far away initial estimates are from the true value, the sequence needs to converge to the true value. Therefore the sum of the learning rates needs to sum to infinity. Otherwise the sequence could only cover a finite distance, given a constant delta to the target, which would be decreasing with steps towards the target in reality. And by constructing cases where we get a nudge of size \(1/n\) in the right direction with equal probability to a nudge in the wrong direction of size \(1/(n+1)\), it is quite intuitive, why we need the ability to cover an infinite distance. Since we could slow progress arbitrarily by increasing \(n\). 

Additionally, for the application we want to use stochastic approximation for, the case can be made that newer observations should be weighted more, but certainly not less. And by weighting every observation equally, the learning rate \(1/n\) is already at the border. More formally, if we want to ensure increasing weights
\[
    w_1^{(n)} \le \dots \le w_n^{(n)}  \qquad \forall n\in\N,
\]
we only need to ensure
\[
    w_{n-1}^{(n)} \le w_n^{(n)} \qquad \forall n\in\N,
\]
since
\begin{align*}
    w_1^{(n-1)} \le \dots \le w_{n-1}^{(n-1)}
    \quad \implies\quad
    \underbracket[1pt]{(1-\alpha_n) w_1^{(n-1)}}_{=w_1^{(n)}} 
    \le \dots \le 
    \underbracket[1pt]{(1-\alpha_n) w_{n-1}^{(n-1)}}_{=w_{n-1}^{(n)}}.
\end{align*}
This translates to:
\begin{align*}
    &\alpha_{n-1}(1-\alpha_n) = w_{n-1}^{(n)} \le w_n^{(n)} = \alpha_n,\\
    &\iff \alpha_{n-1}\le \alpha_n(1+\alpha_{n-1})\\
    &\iff \frac{\alpha_{n-1}}{1+\alpha_{n-1}}\le \alpha_n
\end{align*}
In our special case this requires:
\begin{align*}
    &\frac{1}{(n-1)^\beta+1}=\frac{(n-1)^{-\beta}}{1+(n-1)^{-\beta}}= \frac{\alpha_{n-1}}{1+\alpha_{n-1}}\le \alpha_n= n^{-\beta}\\
    &\implies n^\beta \le (n-1)^\beta +1\\
    &\implies f(n)\coloneqq n^\beta -(n-1)^\beta -1\le 0
\end{align*}
For \(n=1\) this is true for any \(\beta>0\), but because of
\begin{align*}
    f'(n)= \beta n^{\beta-1} -\beta(n-1)^{\beta-1}=\beta(n^{\beta-1}-(n-1)^{\beta-1})\le 0 \iff \beta\le 1,
\end{align*}
we require \(\beta\le 1\).

In applications the learning rates usually fulfil the first requirement. But constant learning rates are still quite common. This is justified by the fact, that in reality these algorithms do not run indefinitely. And for small \(n\), constant learning rates behave relatively similar to learning rates close to \(1/\sqrt{n}\). Since the weight \(1/\sqrt{n}\) of the newest observation only becomes smaller than that of a constant learning rate \(\alpha\), once \(n\ge1/\alpha^2 \). The larger problem with constant learning rates is probably the fact, that the weight of the first observation is comparatively high since \(\theta_1=X_1\) effectively sets \(\alpha_1=1\). But this is only a problem in the very beginning for very small \(n\), as you can see in the following plots:

\[
\begin{tikzpicture}[scale=0.92]
    \begin{groupplot}[
        group style={
            group size=2 by 2,
            horizontal sep=1.2cm,
            vertical sep=1.5cm,
            xlabels at=edge bottom,
            ylabels at=edge left
        },
        scale only axis,
        height=5cm,
        width=5cm,
        xlabel=\(k\),
        ylabel=\(w_k^{(n)}\)
    ]
    \nextgroupplot[
        title={\(n=30\)},
        legend entries={{\(\alpha_k=0.15\; (k>1) \)}, \(\alpha_k=1/\sqrt{k}\), \(\alpha_k=1/k\)},
        legend pos=north west,
        legend cell align={left},
    ]
        \addplot table {./Other/expon30.csv};
        \addplot table {./Other/sqrtn30.csv};
        \addplot table {./Other/n30.csv};
    \nextgroupplot[
        title ={\(n=30\) loglog},
        ymode=log,
        xmode=log, 
        ]
        \addplot table {./Other/expon30.csv};
        \addplot table {./Other/sqrtn30.csv};
        \addplot table {./Other/n30.csv};
    \nextgroupplot[
        title ={\(n=100\) loglog},
        ymode=log,
        xmode=log, 
        ]
        \addplot table {./Other/expon100.csv};
        \addplot table {./Other/sqrtn100.csv};
        \addplot table {./Other/n100.csv};
    \nextgroupplot[
        title ={\(n=500\) loglog},
        ymode=log,
        xmode=log, 
        ]
        \addplot table {./Other/expon500.csv};
        \addplot table {./Other/sqrtn500.csv};
        \addplot table {./Other/n500.csv};
    \end{groupplot}
\end{tikzpicture}
\]

\section{Application to Reinforcement Learning}

Most, if not all convergence proofs of reinforcement learning methods rely on stochastic approximation in one way or another. As an example, \citeauthor{watkinsQlearning1992}'s (\citeyear{watkinsQlearning1992}) original proof for the convergence of Q-learning constructs a new MDP for every realization of the sequence \(((X_t, A_t,R_{t+1}), t\in\N_0)\). This new MDP has the state space \(\N\times\cX\), where \(\cX\) is the state space of the old MDP. And this new MDP is designed in such a way, that \(Q_n(x,a)\), generated by the realization of \(((X_t, A_t,R_{t+1}), t\in\N_0)\) with Q-learning, is by definition equal to the optimal value function \(Q^*((n,x),a)\) of the new MDP. Using stochastic approximation results, they then prove, that the transition probabilities and immediate rewards of this new MDP, converge to the old transition probabilities and immediate rewards in the sense:
\[
    r((n,x),a) \to r(x,a) \quad \text{and}\quad \underbracket[1pt]{p(y\mid (n,x), a)}_{
        \begin{aligned}
            &\coloneqq\sum_{m=1}^{n-1}\mathrlap{p((m,y)\mid (n,x),a)}\\
            &=\sum_{m=1}^{\infty}\mathrlap{p((m,y)\mid (n,x),a)}
        \end{aligned}
    }\to p(y\mid x,a) \quad (n\to \infty)
\]
And starting from a state \((n,x)\), the chance of transitioning to a state \((m,y)\) with \(m<n_0\) can be bounded for any \(n_0\). This means the states \((n,x)\) in the new MDP behave similar to the states \(x\) in the old MDP, when \(n\) is high. This results in 
\[
    Q_n(x,a)= Q^*((n,x),a)\approx Q^*(x,a) \qquad \text{for } n>>0.
\]
And carefully lowering the epsilon bounds, yields convergence with probability one.

But it seems much more desirable, to apply stochastic approximation directly to reinforcement learning algorithms. Because, while we do not look for the zero of a function, we do look for the fixed point of \(T^*\) in case of Q-learning, or \(T^\pi\) in case of TD(\(\lambda\)); and it seems like it should be unnecessary to have such an involved proof for convergence. The classic way to transform a fixed point problem into a zero problem would be to define
\[
    g(Q)\coloneqq T^*(Q)-Q,
\]
which would result in the stochastic approximation algorithm
\[
    Q_{n+1}(x,a) = Q_n(x,a) + \alpha_n \underbracket[1pt]{(\hat{Q}_n(x,a) -Q_n(x,a))}_{\begin{aligned}
        &\E[\hat{Q}_n(x,a) -Q_n(x,a)]\\
        &=T^*Q_n(x,a)-Q_n(x,a)\\
        &=g(Q)(x,a)  
    \end{aligned}},
\]
with 
\[
    \hat{Q}_n(X_t,A_t)=R_{t+1} + \gamma \max_{b\in\cA_{X_{t+1}}} Q_n(X_{t+1}, b)
\] in case of Q-learning. So it seems like we only need to find a general enough result in stochastic approximation, to cover all reinforcement learning algorithms. 

One such attempt is made by \textcite{jaakkolaConvergenceStochasticIterative1994a}, which transform the functional form above as follows:
\begin{align*}
    &&Q_{n+1}(x,a)
    =&Q_n(x,a) + \alpha_n(\hat{Q}_n (x,a)-Q_n(x,a))
    \\
    \iff
    && \underbracket[1pt]{Q_{n+1}(x,a) -Q^*(x,a)}_{\eqqcolon\Delta_{n+1}(x,a)} 
    =&\underbracket[1pt]{Q_n(x,a)-Q^*(x,a) }_{\eqqcolon\Delta_n(x,a)}
    + \alpha_n(\hat{Q}_n (x,a)-Q_n(x,a))\\
    \iff&& \Delta_{n+1}(x,a) =&\Delta_n(x,a) +\alpha_n 
    (\underbracket[1pt]{\hat{Q}_n (x,a)-Q^*(x,a)}_{
        \eqqcolon F_n(x,a)
    }-\Delta_n(x,a))
\end{align*}
With \(\alpha_n=\beta_n\), this is essentially the iterative process of their theorem

\begin{thm}[\citeauthor*{jaakkolaConvergenceStochasticIterative1994a}]\label{JAAKKOLA:THM}
    A random iterative process
    \[
        \Delta_{n+1}(x)=(1-\alpha_n(x)) \Delta_n(x) +\beta_n(x) F_n(x)
    \]
    converges to zero w.p.1 under the following assumption:
    \begin{enumerate}
        \item\label{Jaakkola:1} \(x\in S\), where S is a finite set.
        \item\label{Jaakkola:2} \(\sum_n \alpha_n(x)=\infty\), \(\sum_n \alpha_n^2(x) <\infty\), \(\sum_n\beta_n(x)=\infty\), \(\sum_n \beta_n^2 (x)<\infty\) and
        \[
            \E[\beta_n(x)\mid H_n]\le \E[\alpha_n(x)\mid H_n]
        \]
        uniformly over x w.p.1.
        \item\label{Jaakkola:3} \(\|\E[F_n(\cdot)\mid H_n,\beta_n]\|_\infty \le \gamma \|\Delta_n\|_\infty\), where \(\gamma \in (0,1) \).
        \item\label{Jaakkola:4} \(\text{Var}[F_n(x)\mid H_n,\beta_n]\le C(1+\|\Delta_n\|_\infty)^2\), where C is some constant.
    \end{enumerate}
    Here
    \[
        H_n=\{X_n, X_{n-1}, \dots, F_{n-1}, \dots, \alpha_{n-1},\dots, \beta_{n-1},\dots\}
    \]
    stands for the history up to until \(n\). \(F_n(x), \alpha_n(x)\) and \(\beta_n(x)\) are allowed to depend on the past. \(\alpha_n(x)\) and \(\beta_n(x)\) are assumed to be non-negative and mutually independent given \(H_n\).
\end{thm}

``Essentially the iterative process of this theorem'', because an important detail is different: Here \(\alpha_n\) is not just a function of the history, but it is also indexed by the state (state-action). This might seem strange, since \(\alpha_n\) is just used in one state action tuple:
\begin{align*}
    &Q_{n+1}(x,a)\\
    &= \begin{cases}
        Q_n(x,a)+\alpha_n (\smash{
            \overbracket[1pt]{
            R_{n+1}+\max_{b\in\cA_{X_{n+1}}}Q_n(X_{n+1},b)
            }^{
                \hat{Q}(x,a)
            }
        } 
        -Q_n(x,a)) & \substack{X_n=x,\\ A_n=a}\\
        Q_n(x,a) & \text{else}
    \end{cases}
\end{align*}

But looking closer at the process in the theorem, every state is updated in every iteration. In order to bring this definition in line with reinforcement learning algorithms, \(\alpha(x)\) (and \(\beta_n(x)\)) is set to zero, for every state \(x\), which is not supposed to be updated. The other way one might come up with, setting \(\alpha_n(x)=\beta_n(x)\) and \(F_n(x)=\Delta_n(x)\) for all states which are not supposed to be updated, would violate the third assumption. 

In order to adhere to the third assumption, it is in fact assumed that an observation \(\hat{Q}(x,a)\) is made in every state-action, every turn, and all these observations but one are simply discarded by \(\alpha_n(x)=0\).

Another source of confusion might be the fact, that \(\alpha_n(x)=\beta_n(x)\) are supposed to be independent conditional on the history, but if they are simply functions of the history, then they are constants conditional on the history. So most use cases are in fact included. 

\begin{corollary}
    Let \(\cX\times\cA\) be finite, \(\E[R_{(x,a)}^2]<c\in\R\) and \(\alpha_n(x,a)\) a function of the history up to \(n\), with
    \begin{align}\label{learning rate:Q-learning}
        \sum_{n=1}^\infty \alpha_n(x,a)=\infty \qquad \text{and}\qquad \sum_{n=1}^\infty \alpha_n^2(x,a)<\infty
    \end{align}
    uniformly over all histories.\fxnote{double-check}
    Then Q-learning converges with probability one with regard to \(\|\cdot\|_\infty\) to \(Q^*\). 
\end{corollary}
Note: Due to 
    \[
        \alpha_n(x,a)=0 \qquad \text{for } (x,a)\neq (X_n,A_n) 
    \]
    and (\ref{learning rate:Q-learning}), we require that every state and action is visited an infinite amount of times.
\begin{proof}
    \ref{Jaakkola:1}. \(S=\cX\times\cA\) is finite.
    \ref{Jaakkola:2}. \(\alpha_n(x,a)=\beta_n(x,a)\) fulfils the requirement.
    \ref{Jaakkola:3}. Recall that \(\alpha_n(x,a)=0\) for most state-actions allows us to pretend there is an observation \(\hat{Q}_n(x,a)\) in every state-action tuple. Therefore:
    \begin{align*}
        &\|\E[F_n(\cdot)\mid H_n, \beta_n]\|_\infty
        =\max_{(x,a)\in\cX\times\cA} |\E[\hat{Q}_n(x,a)-Q^*(x,a)\mid H_n, \beta_n]|\\
        &\lxeq{\text{Markov}}\max_{(x,a)\in\cX\times\cA}
        \left|r(x,a)+\gamma\sum_{y\in\cX}p(y\mid x,a)\max_{b\in\cA_y}Q_n(y,b) -Q^*(x,a)\right|\\
        &\lxeq{\text{\ref{V*,Q* relation}}}\max_{(x,a)\in\cX\times\cA}
        \left|\gamma\sum_{y\in\cX}p(y\mid x,a)\left(
            \max_{b\in\cA_y}Q_n(y,b) - \max_{c\in\cA_y}Q^*(y,c) 
        \right)\right|\\
        &\lxlq{\text{(\ref{sup and absolute value})}} \max_{(x,a)\in\cX\times\cA} \gamma \underbracket[1pt]{\sum_{y\in\cX}p(y\mid x,a)}_{=1}  
        \underbracket[1pt]{
            \max_{b\in\cA_y}\left|Q_n(y,b)-Q^*(y,b)\right|
        }_{\le \|\Delta_n\|_\infty}\\
        &\le \gamma \|\Delta_n\|_\infty
    \end{align*}
\ref{Jaakkola:4}. To make our lives easier, we first prove:
\begin{align}
    && (a+b)^2 &\le 2a^2+2b^2\\
    \iff&& a^2+2ab +b^2 &\le 2a^2+2b^2\nonumber\\
    \iff&& 2ab &\le a^2 + b^2 \nonumber\\
    \iff&& 0 &\le a^2 -2ab -b^2 = (a-b)^2 \nonumber
\end{align}
Using this, we get the last requirement:
\begin{align*}
    &\text{Var}[F_n(x,a)\mid H_n,\beta_n]\\
    &\xeq{\text{\ref{appx4}}}\min_{f} \E[(\hat{Q}_n(x,a)-Q^*(x,a) -f(H_n,\beta_n))^2\mid H_n, \beta_n ]\\
    &\le \E[(\underbracket[1pt]{\hat{Q}_n(x,a)}_{
        =R_{(x,a)} + \mathrlap{\gamma \max\limits_{b\in\cA_{Y_{\mathrlap{(x,a)}}}}Q_n(Y_{(x,a)}, b)}
    }-\gamma Q^*(y,c))^2\mid H_n, \beta_n ]\\
    &\le 2\E[ R_{(x,a)}^2] 
    + 2\gamma \E\left[
        \underbracket[1pt]{
        \left(
            \max\limits_{b\in\cA_{Y_{\mathrlap{(x,a)}}}}
            Q_n(Y_{(x,a)}, b)-Q^*(y,c)
        \right)^2 
        }_{
        \le \|\Delta_n\|_\infty^2
    }\;\middle|\; H_n,\beta_n \right] \\
    &\le C(1+\|\Delta_n\|_\infty)^2\qedhere
\end{align*}
\end{proof}

\begin{corollary}
    
\end{corollary}

\section{Proof of Theorem \ref{JAAKKOLA:THM}}
The proof of this theorem is built on \citeauthor{dvoretzkyStochasticApproximation1956}'s (\citeyear{dvoretzkyStochasticApproximation1956}) very general theorem about one dimensional sequences:

\begin{thm}[\citeauthor{dvoretzkyStochasticApproximation1956}]
    Let 
    \[
        \alpha_n, \beta_n,\gamma_n\colon \R^n \to \R_{\ge 0}, \qquad n\in\N,
    \] 
    be measurable functions satisfying:
    \begin{enumerate}
        \item The functions \(\alpha_n(r_1,\dots,r_n) \) are uniformly bounded, and 
        \begin{align}\label{alpha_n property}
            \lim_{n\to\infty}\alpha_n(r_1,\dots,r_n)=0
        \end{align}
        uniformly for all sequences \(r_1,r_2,\dots\), i.e. \(\lim_n \|\alpha_n\|_\infty \to 0\).
        \item The series
        \begin{align}\label{beta_n property}
            \sum_{n=1}^\infty \beta_n(r_1,\dots, r_n)
        \end{align}
        is uniformly bounded (i.e. \(\|\sum_n^\infty \beta_n\|_\infty<C\)) and uniformly convergent (i.e. \(\lim_k\| \sum_n^k\beta_n -\sum^\infty_n\beta_n\|_\infty=0  \))
        \item For all sequences \(r_1,r_2,\dots \) which satisfy
        \begin{align}\label{sup r_n < L}
            \sup_n |r_n| < L
        \end{align}
        for an arbitrary \(L\in\R\),
        \begin{align}\label{gamma_n property}
            \sum_{n+1}^\infty \gamma_n(r_1,\dots,r_n)=\infty
        \end{align}
        holds uniformly. (i.e. \(\lim_k \inf_{\|r\|_\infty < L}  \sum^k_n \gamma_n \to \infty \))
    \end{enumerate}
    Let \(\theta\) be a real number and \(T_n\), \(n\in\N\) be a measurable transformation satisfying
    \begin{align}
        |T_n(r_1,\dots,r_n)-\theta| \le \max\{\alpha_n, (1+\beta_n)|r_n -\theta| -\gamma_n\} \label{dvoretzky: main condition}
    \end{align}
    for all real \(r_1,\dots,r_n\) with \(\alpha_n=\alpha_n(r_1,\dots,r_n)\), etc. Let \(X_1\) and \(Y_n\), \(n\in\N\) be random variables. Define the recursion
    \begin{align}
        X_{n+1}=T_n[X_1,\dots,X_n] +Y_n \qquad \text{for }n\ge 1
    \end{align}
    Then the conditions
    \begin{enumerate}
        \item \(E[X_1^2]<\infty\),
        \item \(\sum\limits_{n=1}^\infty \E[Y_n^2]<\infty\),
        \item \(\E[Y_n\mid X_1,\dots,X_n]=0\) with probability 1 for all \(n\in\N\),
    \end{enumerate}
    imply convergence in \(L^2\)
    \begin{align}
        \lim_{n\to\infty} \E[(X_n-\theta)^2]=0,
    \end{align}
    and almost sure convergence
    \begin{align}
        \Pr(\lim_{n\to\infty} X_n=\theta)=1.
    \end{align}
\end{thm}
\begin{proof}
    see \textcite{dvoretzkyStochasticApproximation1956} \fxnote{appendix}
\end{proof}
\begin{remark}
    The condition (\ref{dvoretzky: main condition}) is weaker than (can be replaced by)
    \begin{align}\label{dvoretzky: alt main cond}
        |T_n(r_1,\dots,r_n)-\theta|\le \max \{\alpha_n, (1+\beta_n -\gamma_n)|r_n-\theta|\}
    \end{align}
\end{remark}
\begin{proof}
    Essential to this proof is the construction of a sequence \(\rho_n\) such that \(\rho_n\to 0\) but retains
    \begin{align}\label{gamma tilde retains}
        \sum_n^\infty \gamma_n \rho_n =\infty
    \end{align}
    in the same sense as \(\gamma_n\).

    We will first discuss the simpler case where \(\alpha_n, \beta_n,\gamma_n\) are simply real sequences, and later argue that the same proof can be applied to the defined real functions. Let \(K_1=0\), due to
    \begin{align}
        \lim_{K\to\infty} \sum_{j=K_n+1}^K \gamma_j \to \infty
    \end{align}
    we can select \(K_{n+1}\) inductively, such that
    \begin{align}\label{K_n+1 selection}
        \sum_{j=K_n+1}^{K_{n+1}}\gamma_j > n.
    \end{align}
    And we define
    \begin{align}
        \rho_{K_n+1}\coloneqq\dots\coloneqq \rho_{K_{n+1}}\coloneqq\mfrac{1}{n}.
    \end{align}
    Then we get
    \[
        \sum_{j=1}^{K_{n+1}} \gamma_j\rho_j
        =\sum_{k=1}^n \sum_{j=K_k+1}^{K_{k+1}}\gamma_j \rho_j 
        = \sum_{k=1}^n \mfrac{1}{k}\sum_{j=K_k+1}^{K_{k+1}}\gamma_j > \sum_{k=1}^n \mfrac{k}{k}
        = n \to \infty \quad (n\to \infty)
    \]
    Due to \(\gamma_n\) diverging uniformly, we can select \(K_{n+1}\) such that
    \[
        \inf_{\|r\|_\infty<L} \sum_{j=K_n+1}^{K_{n+1}} \gamma_j(r_1,\dots,r_j) >n.
    \]
    Then we can prove the properties of the sequence \(\rho_n\) the same way, due to:
    \[
        \inf_{\|r\|_\infty<L}\sum_{j=1}^{K_{n+1}} \gamma_j(r_1,\dots,r_j)\rho_j \ge \sum_{k=1}^n \inf_{\|r\|_\infty<L}\sum_{j=K_k+1}^{K_{k+1}}\gamma_j(r_1,\dots,r_j) \rho_j 
    \]
    We can now use this sequence \(\rho_n\) to make a case by case analysis. 
    
    \noindent If \(|r_n-\theta|\ge\rho_n\), then
    \begin{align}
        (1+\beta_n -\gamma_n)|r_n-\theta| \le  (1+\beta_n)|r_n-\theta| - 
        \underbracket[1pt]{\gamma_n \rho_n}_{\eqqcolon \tilde{\gamma}_n}
    \end{align}
    If \(|r_n-\theta|\le\rho_n\), then
    \begin{align}
        (1+\beta_n -\gamma_n)|r_n-\theta|\le (1+\beta_n)\rho_n        
    \end{align}
    Therefore we get the required inequality
    \begin{align*}
        |T_n(r_1,\dots,r_n)-\theta|\le 
        \max\left\{ \underbracket[1pt]{\alpha_n, (1+\beta_n)\rho_n,}_{
            \tilde{\alpha}_n \coloneqq \max\{\alpha_n, (1+\beta_n)\rho_n\}
        } 
        (1+\beta_n)|r_n-\theta| - \tilde{\gamma}_n\right\}
    \end{align*}
    Where \(\tilde{\gamma}_n\) has the required property (\ref{gamma_n property}) due to (\ref{gamma tilde retains}) and \(\tilde{\alpha}\) has the required property (\ref{alpha_n property}), since \(\beta_n\) converges uniformly due to (\ref{beta_n property}).
\end{proof}

\begin{lemma}
    A random process
    \[
        w_{n+1}(x)=(1-\alpha_n(x))w_n(x) +\beta_n(x)\eta_n(x) \qquad x\in S
    \]
    converges to zero with probability one, if 
    \begin{enumerate}
        \item \(\sum_n \alpha_n(x)=\infty\), \(\sum_n \alpha_n^2(x) <\infty\), \(\sum_n\beta_n(x)=\infty\) and \(\sum_n \beta_n^2 (x)<\infty\) uniformly over every possible history with probability one. \(\alpha_n\) and \(\beta_n\) are assumed to be non-negative and mutually independent given history \(H_n\).
        \item \(\E[\eta_n(x)\mid H_n, \beta_n]  \) and \( \E[\eta_n^2(x)\mid H_n,\beta_n]\le C \) w.p.1, where 
        \[
            H_n=\{w_n, w_{n-1}, \dots, \eta_{n-1}, \dots, \alpha_{n-1},\dots, \beta_{n-1},\dots\}
        \]
    \end{enumerate}
\end{lemma}
\begin{proof}
    Since there are no interactions between the processes indexed by \(x\in S\), we can essentially treat it like a one dimensional process for a given \(x\) and apply \citeauthor{dvoretzkyStochasticApproximation1956}'s theorem. 
    We set
    \begin{align}
        T_n(r_1,\dots, r_n)
        &\coloneqq (1-\alpha_n(x)(r_1,\dots,r_n)) w_n(x) \\
        &\le \max\{\cdot, (1+ 0 -\alpha_n(x))|w_n(x)|\}
    \end{align}
    which fulfils (\ref{dvoretzky: alt main cond}) with \(\alpha(x)=\gamma_n\). And
    \[
       Y_n\coloneqq \beta_n(x) \eta_n(x)
    \]
    which fulfils the required conditions
    \[
        \sum_{n=1}^\infty \E[Y_n^2] 
        = \sum_{n=1}^\infty \E[\beta_n(x)^2\underbracket[1pt]{\E[\eta_n^2(x)\mid H_n,\beta_n,]}_{\le C}]
        \xeq{\text{Fub.}} C \E\left[\vphantom{\sum_{n=1}^\infty}
            \smash{\underbracket[1pt]{
                \sum_{n=1}^\infty \beta_n(x)^2}_{
                <\infty \mathrlap{\text{ (uniformly)}}
                }}
        \right] <\infty
    \]
    and since \(w_n,\dots, w_1\) is part of \(H_n\) \fxnote{proof necessary?}
    \[
        \E[Y_n\mid w_n,\dots, w_1]=\E\left[\beta_n(x) \underbracket[1pt]{\E[r_n(x)\mid H_n, \beta_n]}_{\smash{=0}}   \middle| w_n,\dots, w_1\right]=0\qedhere
    \]
\end{proof}








%%%%%%%%%%%%%%%%%%%%%%%%%
\endinput