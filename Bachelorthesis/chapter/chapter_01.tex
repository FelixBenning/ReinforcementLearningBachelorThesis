\chapter{Markov Decision Processes}
\begin{definition}(Kernel)
	\((Y,\cA_Y), (X,\cA_X)\) measure spaces\\
	 \(\lambda\colon X\times\cA_Y\to \R\) is a \emph{(probability) kernel} \(
	 :\iff \begin{aligned}[t]
	 &\lambda(\cdot,A)\colon x\mapsto \lambda(x,A) \text{ measurable}\\
	 &\lambda(x,\cdot)\colon A\mapsto \lambda(x,A) \text{ a (prob.) measure}
	 \end{aligned}
	  \)\\
	  Since we will interpret probability kernels as distributions over \(Y\) given a  certain condition \(X\), the notation \(\lambda(\cdot\mid x) \coloneqq \lambda(x,\cdot)\) helps this intuition. 
\end{definition}
\begin{definition}(Markov Decision Process - MDP)\\
\(\cM=(\cX,\cA,\cP_0) \), with:
	
\begin{tabular}{l l l}
	\(\cX\) & countable (finite) set of states&\\
	\(\cA\) & countable (finite) set of actions &\\
	\multicolumn{2}{l}{
		\(
		\begin{cases}
		\cX\times \cA \to \pmeas(\cX\times\R)\\
		(x,a)\mapsto \cP_0(\cdot \mid x,a)
		\end{cases}
		\)
	}  &
	\parbox[h]{17em}{
		\emph{transition probability kernel} \\
		\(\pmeas(\cX\times\R) \) the set of probability measures on \(\cX\times\R \), \\
		\(\cX\) represents the next states,\\
		\(\R\) the payoffs
	}
	\end{tabular}\\
is a \emph{(finite) Markov Decision Process}.\\
Together with a discount factor \(\gamma\in(0,1]\) it is a:\\
\begin{tabular}{l l}
	\emph{discounted reward} MDP & \(\gamma <1 \)\\
	\emph{undiscounted reward} MDP & \(\gamma=1 \)
\end{tabular}\\
For \((Y_{(x,a)}, R_{(x,a)})\sim \cP_0(\cdot\mid x,a) \) a random variable, is
\[r(x,a)\coloneqq \E[R_{(x,a)}] \quad\text{ the \emph{immediate reward function}}\]
An MDP is \emph{evaluated} as follows:\\
1. Select the initial state \(X_0\) an \(\cX\)-valued random variable.\\ 
2. \((A_t, t\in \N)\) action selection rules (behaviors) will be discussed later, for now simply assume \(\cA\)-valued random variables.\\
3. Select inductively: \((X_{t+1}, R_{t+1})\sim\cP_0(\cdot\mid X_t, A_t)\) with the markov property, i.e.:
\begin{align*} 
&\Pr[(X_{t+1},R_{t+1})=(x,r) \mid (X_t,A_t)=(x_t,a_t),\dots, (X_0,A_0)=(x_0,a_0)]\\
&=\Pr[(X_{t+1}, R_{t+1})=(x,r)\mid (X_t,A_t)=(x_t,a_t)]
\end{align*}
resulting in the stochastic process \(((X_t,A_t,R_{t+1}), t\geq 0)\), which allows to define the \emph{return}:
\[\cR\coloneqq\sum_{t=0}^{\infty}\gamma^t R_{t+1}\]
\end{definition}
%(

\begin{remark}
\((X_{t+1}, R_{t+1})\sim\cP_0(\cdot\mid X_t, A_t)\) with the markov property is well defined, i.e.:
\begin{align*}
	&\exists (X_{t+1}, R_{t+1})\ \cX\times\R\text{-valued random variable}: \\ 
	&(X_{t+1}, R_{t+1})\sim\cP_0(\cdot\mid X_t, A_t) \text{ and satisfies the markov property}
\end{align*}
\end{remark}
\begin{proof}
	%TODO
\end{proof}
\begin{remark}\leavevmode
\begin{enumerate}
	\item From now on we assume that \(\forall (x,a)\in\cX\times\cA:|R_{(x,a)}|\le R\in\R\) almost surely. This also implies: 			\(
	\begin{aligned}[t]
		&\|r\|_{\infty}=\sup_{(x,a)\in\cX\times\cA}|\E[R_{(x,a)}]|\le R\\
		&|\cR|\le\sum_{t=0}^\infty \gamma^t |R_{t+1}|\le \frac{R}{1-\gamma} \text{ a.s.}
	\end{aligned}
	\)
	\item Sometimes not all actions make sense in all states. A simple fix would be to set the immediate reward functions for those actions very low, or (if possible) redirect them to the closest possible action. \\
	A more formal approach would be to introduce an additional mapping, which assigns the set of admissible actions to each state \(\cX\to\cP(\cA)\), or alternatively define a (binary) relation on \(\cX\times\cA\).
	\item If there is just one admissible action in every state, the MDP is equivalent to a normal Markov Process.
	\item Instead of a transition probability kernel \(\cP_0\), sometimes a \emph{transition function} f with a and an exogenous random element \(D_t\) (e.g. Demand) is used to define the next state and reward: \((X_{t+1},R_{t+1})=f(X_t,A_t,D_t)\) 
\end{enumerate}
\end{remark}
\begin{definition}\(\cM=(\cX,\cA,\cP_0)\) a MDP\\
	\(x\in\cX\) is a \emph{terminal (absorbing)} state \(:\iff \forall s\in\N: \Pr(X_{t+s}=x\mid X_t=x)=1\)\\
	An MDP with such states is called \emph{episodic}.\\
	An \emph{episode} is the random time period \((1,\dots,T)\) until a terminal state is reached.
\end{definition}
\begin{remark}\leavevmode
	\begin{itemize}
		\item The reward in a terminal state is by convention zero, i.e. \(x\) terminal state implies \(\forall a\in\cA: R_{(x,a)}=0\).
		\item Episodic MDPs are often undiscounted
	\end{itemize}	
\end{remark}
\begin{definition}\(\cM=(\cX,\cA,\cP_0)\) a MDP\\
	 An \(A_t\) selection-rule \(\pi=(\pi_t,t\in\N_0)\) is called \emph{behavior}, where
	 \[ 
	 	\pi_t\colon
	 	\begin{cases}
	 		((\cX\times\cA\times\R)^t\times\cX)\times\cP(\cA) \to \R \\
	 		(y,A)\mapsto \pi_t(A\mid y)
	 	\end{cases} \text{ is a probability kernel}
	 \]
	 and \(A_t\sim \pi_t(\cdot\mid (X_0,A_0,R_1), \dots,(X_{t-1},A_{t-1},R_t),X_t))\)\\
	 Special cases:
	 \begin{enumerate}
	 	\item \emph{Determinisitic stationary policies} specified with some abuse of notation:
	 	\[\leavevmode \pi\colon\cX\to\cA \text{ with }A_t=\pi(X_t)\]
	 	\item \emph{Stochastic stationary policies} specified by:
	 	\[\pi\colon \begin{cases}
	 	\cX\times\cP(\cA)\to\R\\
	 	(x,A)\mapsto \pi(A\mid x)
	 	\end{cases} \text{ with } A_t\sim\pi(\cdot\mid x)
	 	\]
	 \end{enumerate}
	 \(\Pi_{\text{stat}}\) denotes the \emph{set of (stoch.) stationary policies} (note that the deterministic policies are a subset of the stochastic policies)
\end{definition}
\begin{remark}
A stationary policy induces a \emph{time-homogenous} markov chain.
\end{remark}
\begin{definition}(Markov Reward Process - MRP)
%TODO
\end{definition}
\begin{definition}\(\cM=(\cX,\cA,\cP_0)\) MDP, \(\pi\) Behavior\\
Select \(X_0\) such that \(\forall x\in\cX:\Pr(X_0=x)>0\) and evaluate the MDP with
\(((X_t,A_t,R_{t+1}), t\in \N_0)\) the resulting stoch. process.\\
\def\arraystretch{3}
\[
\begin{array}{l l}
	V^\pi\colon
	\begin{cases}
		\cX\to\R\\
		x\mapsto \E[\cR\mid X_0=x]
	\end{cases} 
	& \text{is the \emph{value function} for } \pi \footnotemark\\
	Q^\pi\colon
	\begin{cases}
		\cX\times\cA\to\R\\
		(x,a)\mapsto \E[\cR\mid X_0=x, A_0=a]
	\end{cases}
	& \text{is the \emph{action value function} for } \pi \footnotemark\\
	V^*\colon
	\begin{cases}
		\cX\to\R\\
		x\mapsto \sup\limits_{\pi\text{ Behav.}} V^\pi(x)
	\end{cases} 
	& \text{is the \emph{optimal value function}}\\
	Q^*\colon
	\begin{cases}
		\cX\times\cA\to\R\\
		(x,a)\mapsto \sup\limits_{\pi\text{ Behav.}} Q^\pi(x,a)
	\end{cases}
	& \text{is the \emph{optimal action value function}}
\end{array}
\]
\(\pi\) is \emph{optimal} \(:\iff V^*=V^\pi\)
\end{definition}
\footnotetext[1]{Well defined because \(\Pr(X_0=x)>0\)}
\footnotetext{Well defined because \(A_1\sim \pi_1(\cdot\mid (x,a,r_0), x_1)\) is defined for all \(a\) regardless of \(\pi_0\)}
\begin{definition}\(\cM=(\cX,\cA,\cP_0)\) MDP\\
Sometimes we don't care about the probability distribution of the reward, so we define:
	\[
	p\colon 
	\begin{cases}
		\cX\times\cA\times\cP(\cX)\to\R\\
		(x,a,Y)\mapsto \cP_0(Y\times\R\mid x,a)
	\end{cases}\text{ the \emph{\underline{state} transition kernel}.}
	\] 
And use the notation \(p(y\mid x,a)\coloneqq p(\{y\}\mid x,a)\) with \((x,a,y)\in\cX\times\cA\times\cX\)
	
\end{definition}
\begin{lemma}\label{V*,Q* relation}\(\cM=(\cX,\cA,\cP_0)\) MDP
\begin{enumerate}[label=\textbf{(\roman*)},font=\normalfont]
\item \(V^*(x)=\sup\limits_{a\in\cA}Q^*(x,a)\)
\item \(Q^*(x,a)=r(x,a)+\gamma \sum\limits_{y\in\cX}p(y\mid x,a) V^*(y)\)
\end{enumerate}
\end{lemma}
\begin{proof}
	%TODO
\end{proof}
\begin{definition}\(\cM=(\cX,\cA,\cP_0)\) MDP\\
The mapping \(T^*\colon\R^\cX\to\R^\cX\) with:
	\[
	 T^* V(x)\coloneqq \sup_{a\in\cA}\left\{r(x,a)+\sum_{y\in\cX}p(y\mid x,a) V(y)\right\} \qquad x\in\cX
	\]
is the \emph{Bellman optimality operator}
\end{definition}
\begin{remark}
Because of \ref{V*,Q* relation} \(V^*\) is a fixpoint of \(T^*\):
\[
	V^*(x)=\sup_{a\in\cA}Q^*(x,a)=\sup_{a\in\cA}\left\{r(x,a)+\sum_{y\in\cX}p(y\mid x,a) V^*(y)\right\} =T^*V^*(x)
\]
\end{remark}
\begin{prop}\(\cM=(\cX,\cA,\cP_0)\) MDP
	\[V^*(x)=\sup_{\pi\in\Pi_{\textnormal{stat}}}V^\pi(x)=\sup_{\substack{
	\pi\in\Pi_{\textnormal{stat}}\\ 
	\pi\textnormal{ determ.} 
	}}V^\pi(x) \]
\end{prop}
\endinput
