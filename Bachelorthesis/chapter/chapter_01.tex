\chapter{Markov Decision Processes}
\section{Introduction}
A Markov Process is a random process in a state space with no memory of where it was, that is, only the current state influences where it goes next.
While Markov Processes allow to model random phenomena evolving over time and make predictions about certain events (e.g. terminal states), they are unable to model the interaction of an actor with such a processes. \emph{Markov Decision Processes} (MDPs) introduce \emph{actions} and \emph{rewards} to the state space and transition probabilities of Markov Processces, and shift the focus from \emph{describing} terminal distributions, absorption times, etc. towards \emph{finding} the optimal action(s) to take in each state (If such an action exists).

The MDP model inherits the restriction of Markov Chains to have no memory of past states. We will also not consider changing transition probabilities over time. Rather the transition probabilities will only be influenced by the state and the action. \\
Both of these limitations could in principle be circumvented by including the time in the state space at the expense of a larger state space. Although it is questionable whether such a construct would yield any interesting results, as then no state is visited twice. So it is of no use to an actor to learn the value of an action in a certain state without further assumptions.

To illustrate the uses of such a framework, I have selected a few examples from \textcite{whiteRealApplicationsMarkov1985}:
\begin{enumerate}
	\item Resource Management: The state is the resource level
	\begin{itemize}
		\item Inventory Management: The resource is the inventory, the possible action is to order resupply, influencing the inventory (state) together with the stochastic demand, and the reward is the profit. The essential trade-off is the cost of storage versus lost sales from a stock-out.
		\item Fishing: The resource is the amount of fish, the action is the amount fished, the reward is directly proportional to the amount fished, and the repopulation is the random element.
		\item Pumped storage Hydro-power: The state is the amount of water in the higher reservoir and the electricity price, the action is to use water to generate electricity or wait for higher prices.
		\item Beds in a hospital: How many empty beds are needed for emergencies?
	\end{itemize}
	\item Stock trading: The state is the price level and stock and liquidity owned.
	\item Maintenance: When does a car/road become too expensive to repair?
	\item Evacuation in response to flood forecasts
\end{enumerate}

\section{Model Formulation}
%countable state space
Most of the definitions in this chapter are adaptions from \textcite{szepesvariAlgorithmsReinforcementLearning2010}.
But to properly define the transition probabilities given an action in a certain state, let us define a probability kernel first.

\begin{definition}(Kernel)
	Let \((Y,\sigma_Y), (X,\sigma_X)\) be measure spaces.
	 \begin{align*}
	 &\lambda\colon X\times\sigma_Y\to \R \text{ is a \emph{(probability) kernel}}\\
	 &:\iff \begin{aligned}[t]
	 &\lambda(\cdot,A)\colon x\mapsto \lambda(x,A) \text{ measurable}\\
	 &\lambda(x,\cdot)\colon A\mapsto \lambda(x,A) \text{ a (probability) measure}
	 \end{aligned}
	  \end{align*}
	  Since we will interpret probability kernels as distributions over \(Y\) given a  certain condition \(x\in X\), the notation \(\lambda(\cdot\mid x) \coloneqq \lambda(x,\cdot)\) helps this intuition. 
\end{definition}

Now we can define a Markov Decision Process


\begin{definition} \leavevmode \\ 
	\(\cM=(\cX,\cA,\cP_0) \) is called a \emph{(finite) Markov Decision Process} (MDP). Where:
	\par\begin{tabular}{l l}
		\(\cX\) & is a countable (finite) set of states.\\
		\(\cA\) & is a countable (finite) set of actions.
	\end{tabular}\\
	And \(\cP_0\colon (\cX\times\cA) \times \sigma_{\cX\times\R} \to \R\) is a probability kernel.\\ \\
	\(\cX\times\R\) represents the next state and the reward. So \(\cP_0(\cdot\mid x,a) \) represents the probability distribution over the next states and rewards given an action \(a\) in the state \(x\).
	\\ \\
	\(\quad\) \(\cP_0\) is called the \emph{transition probability kernel}. 
\end{definition}
\begin{remark}
	Some authors include a Time set \(T\) in the tuple \parencite[e.g.][]{putermanMarkovDecisionProcesses2014}. Most authors split the transition kernel into a state transition kernel and a reward kernel \parencite[e.g.][]{putermanMarkovDecisionProcesses2014}. But since it is easier to define a marginal distribution from a joint distribution than vice versa, and since this notation is more compact I will stick to the definition from \textcite{szepesvariAlgorithmsReinforcementLearning2010}.
\end{remark}

According to \textcite{putermanMarkovDecisionProcesses2014} some authors call this tuple a Markov Decision Problem instead of Markov Decision Process, presumably to reserve the term Markov Decision Process for the resulting sequence of states, actions and rewards \((X_t,A_t,R_{t+1},t\in\N_0)\), aligning the Definition with the definition of a Markov process. Although this does not appear to be common practice.

I can find no explanation for this deviation from the notation of Markov processes. So I offer my own interpretation:\\
The objective of the theory of MDPs is to find an optimal action selection rule (behavior). And without a fixed behavior the sequence \((X_t, A_t, R_{t+1}, t\in\N_0 )\) is undefined, since the \((A_t,t\in\N_0)\) are not defined. But fixing the behavior defeats the purpose of modeling decisions. As it would not make sense to talk about optimal behaviors in an MDP if every behavior creates its own MDP.\\ 

Nevertheless we still need to construct a stochastic process from the MDP when we have an action selection rule. 

First we need to select the random variable \(X_0\) of the initial state. The initial state is not included in the definition of an MDP because later objects will be defined conditional on the current state. They are thus invariant to different starting distributions, as long as \(\Pr(X_0=x)>0\) holds for all \(x\in\cX\) ensuring that conditioning on every state is possible.

To inductively define a stochastic process we need an action selection rule, more formally:

\begin{definition} An \(A_t\) selection-rule \(\pi=(\pi_t,t\in\N_0)\) is called \emph{behavior}, where
	\[ 
		\pi_t\colon
		\begin{cases}
			((\cX\times\cA\times\R)^t\times\cX)\times\sigma_\cA \to \R \\
			(y,A)\mapsto \pi_t(A\mid y)
		\end{cases} \text{ is a probability kernel,}
	\]
	and \(A_t\sim \pi_t(\cdot\mid (X_0,A_0,R_1), \dots,(X_{t-1},A_{t-1},R_t),X_t))\).\\
	Special cases:
	\begin{enumerate}
		\item \emph{Determinisitic stationary policies} specified with some abuse of notation:
		\[\leavevmode \pi\colon\cX\to\cA \text{ with }A_t=\pi(X_t)\]
		\item \emph{(Stochastic) stationary policies} specified by:
		\[\pi\colon \begin{cases}
		\cX\times\sigma_\cA\to\R\\
		(x,A)\mapsto \pi(A\mid x)
		\end{cases} \text{ with } A_t\sim\pi(\cdot\mid x)
		\]
	\end{enumerate}
	\(\Pi\) is the set of behaviors,\\
	\(\statPolicy\) is the set of (stochastic) stationary policies, \\
	\(\detPolicy\) is the set of deterministic stationary policies (note \(\detPolicy\subseteq\statPolicy\subseteq \Pi \))
\end{definition}

Now we define inductively: \((X_{t+1}, R_{t+1})\sim\cP_0(\cdot\mid X_t, A_t)\) with the Markov property, i.e.:
\begin{align*} 
&\Pr[(X_{t+1},R_{t+1})=(x,r) \mid (X_t,A_t)=(x_t,a_t),\dots, (X_0,A_0)=(x_0,a_0)]\\
&=\Pr[(X_{t+1}, R_{t+1})=(x,r)\mid (X_t,A_t)=(x_t,a_t)]
\end{align*}
resulting in the stochastic process \(((X_t,A_t,R_{t+1}), t\in \N_0)\)
\begin{remark}
	\((X_{t+1}, R_{t+1})\sim\cP_0(\cdot\mid X_t, A_t)\) with the Markov property is well defined, i.e.:
	\begin{align*}
		&\exists (X_{t+1}, R_{t+1})\ \cX\times\R\text{-valued random variable}: \\ 
		&(X_{t+1}, R_{t+1})\sim\cP_0(\cdot\mid X_t, A_t) \text{ and satisfies the Markov property}
	\end{align*}
\end{remark}
\begin{proof}
	%TODO
\end{proof}

\begin{remark}
	A stationary policy induces a \emph{time-homogenous} Markov chain.
\end{remark}

\begin{definition}
An MDP together with a discount factor \(\gamma\in[0,1]\) is a
\par\begin{tabular}{l l}
	\emph{discounted reward} MDP & for \(\gamma <1 \)\\
	\emph{undiscounted reward} MDP & for \(\gamma=1 \)
\end{tabular}\\
This allows us to define the \emph{return}:
\[\cR\coloneqq\sum_{t=0}^{\infty}\gamma^t R_{t+1}\]
\end{definition}

\begin{definition}
Let \((Y_{(x,a)}, R_{(x,a)})\sim \cP_0(\cdot\mid x,a) \) be a random variable.
\[r(x,a)\coloneqq \E[R_{(x,a)}] \quad\text{ is the \emph{immediate reward function}.}\]
\end{definition}

\begin{remark}\leavevmode
\begin{enumerate}
	\item From now on we assume that \(\forall (x,a)\in\cX\times\cA:|R_{(x,a)}|\le R\in\R\) almost surely. This also implies: 			\(
	\begin{aligned}[t]
		&\|r\|_{\infty}=\sup_{(x,a)\in\cX\times\cA}|\E[R_{(x,a)}]|\le R\\
		&|\cR|\le\sum_{t=0}^\infty \gamma^t |R_{t+1}|\le \frac{R}{1-\gamma} \text{ a.s.}
	\end{aligned}
	\)
	\item Sometimes not all actions make sense in all states. A simple fix would be to set the immediate reward functions for those actions very low, or (if possible) redirect them to the closest possible action. \\
	A more formal approach would be to introduce an additional mapping, which assigns the set of admissible actions to each state \(\cX\to\cP(\cA)\), or alternatively define a (binary) relation on \(\cX\times\cA\).
	\item If there is just one admissible action in every state, the MDP is equivalent to a normal Markov Process.
	\item Instead of a transition probability kernel \(\cP_0\), sometimes a \emph{transition function} f with a and an exogenous random element \(D_t\) (e.g. Demand) is used to define the next state and reward: \((X_{t+1},R_{t+1})=f(X_t,A_t,D_t)\) 
\end{enumerate}
\end{remark}
\begin{definition}\(\cM=(\cX,\cA,\cP_0)\) a MDP\\
	\(x\in\cX\) is a \emph{terminal (absorbing)} state \(:\iff \forall s\in\N: \Pr(X_{t+s}=x\mid X_t=x)=1\)\\
	An MDP with such states is called \emph{episodic}.\\
	An \emph{episode} is the random time period \((1,\dots,T)\) until a terminal state is reached.
\end{definition}
\begin{remark}\leavevmode
	\begin{itemize}
		\item The reward in a terminal state is by convention zero, i.e. \(x\) terminal state implies \(\forall a\in\cA: R_{(x,a)}=0\).
		\item Episodic MDPs are often undiscounted
	\end{itemize}	
\end{remark}

\begin{definition}(Markov Reward Process - MRP)
%TODO
\end{definition}
\section{Value functions}
The goal in this section is to
\begin{itemize}[itemsep=0pt, topsep=1pt]
\item define Value functions which assign states (and actions) a value, which allow the agent to make a more nuanced decisions than comparing immediate rewards of different actions
\item explore the relation of different value functions
\item show uniqueness of optimal value functions with the Banach fixpoint theorem, yielding a simple approximation methode along the way
\item demonstrate that in MDPs deterministic stationary policies are generally a large enough set of policies to choose from
 \end{itemize}
\begin{definition}\(\cM=(\cX,\cA,\cP_0)\) MDP, \(\pi\) Behavior\\
Select \(X_0\) such that \(\forall x\in\cX:\Pr(X_0=x)>0\) and evaluate the MDP with
\(((X_t,A_t,R_{t+1}), t\in \N_0)\) the resulting stoch. process.\\
\def\arraystretch{3}
\[
\begin{array}{l l}
	V^\pi\colon
	\begin{cases}
		\cX\to\R\\
		x\mapsto \E[\cR\mid X_0=x]
	\end{cases} 
	& \text{is the \emph{value function} for } \pi \footnotemark\\
	Q^\pi\colon
	\begin{cases}
		\cX\times\cA\to\R\\
		(x,a)\mapsto \E[\cR\mid X_0=x, A_0=a]
	\end{cases}
	& \text{is the \emph{action value function} for } \pi \footnotemark\\
	V^*\colon
	\begin{cases}
		\cX\to\R\\
		x\mapsto \sup\limits_{\pi\text{ Behav.}} V^\pi(x)
	\end{cases} 
	& \text{is the \emph{optimal value function}}\\
	Q^*\colon
	\begin{cases}
		\cX\times\cA\to\R\\
		(x,a)\mapsto \sup\limits_{\pi\text{ Behav.}} Q^\pi(x,a)
	\end{cases}
	& \text{is the \emph{optimal action value function}}
\end{array}
\]
\(\pi\) is \emph{optimal} \(:\iff V^*=V^\pi\)
\end{definition}
\footnotetext[1]{Well defined because \(\Pr(X_0=x)>0\)}
\footnotetext{Well defined because \(A_1\sim \pi_1(\cdot\mid (x,a,r_0), x_1)\) is defined for all \(a\) regardless of \(\pi_0\)}

\begin{remark}
With the distribution of \(X_0\) set (or \(X_0\) being realized with a fixed value \(x\)), the distribution of \(X_t, A_t,R_{t+1}\) is determined for all \(t\in\N_0\). The conditional expectation is thus unique for a given \(X_0=x\), for all possible realizations of the MDP with a given behavior. \\
This means \(V^\pi, Q^\pi\) are well defined.
\end{remark}


\begin{definition}\(\cM=(\cX,\cA,\cP_0)\) MDP\\
Sometimes we don't care about the probability distribution of the reward, so we define:
	\[
	p\colon 
	\begin{cases}
		\cX\times\cA\times\cP(\cX)\to\R\\
		(x,a,Y)\mapsto \cP_0(Y\times\R\mid x,a)
	\end{cases}\text{ the \emph{\underline{state} transition kernel}.}
	\] 
And use the notation \(p(y\mid x,a)\coloneqq p(\{y\}\mid x,a)\) with \((x,a,y)\in\cX\times\cA\times\cX\)
\end{definition}


\begin{prop}\label{expand Q^pi}\(\cM=(\cX,\cA,\cP_0)\) MDP, \(\pi\in\detPolicy\) 
	\[Q^\pi(x,a)=r(x,a)+\gamma\sum_{y\in\cX}p(y\mid x,a)V^\pi(y)	\]
\end{prop}

\begin{proof}
\begin{align*}
Q^\pi &= \E[\cR(\pi)\mid X_0=x, A_0=a]\\
&=\E[R_1(\pi)\mid X_0=x,A_0=a]+\gamma\E\left[\sum_{t=0}^\infty\gamma^t R_{t+2}(\pi)\middle| X_0=x,A_0=a\right]\\
&=\E[R_{(x,a)}] 
 + \gamma \sum_{y\in\cX}\E\left[\sum_{t=0}^\infty\gamma^t R_{t+2}(\pi) \middle| X_0=x,A_0=a, X_1=y\right]p(y\mid x, a)\\
&\lxeq{\text{Markov}} r(x,a)
 + \gamma\sum_{y\in\cX}\underbracket{\E\left[\sum_{t=0}^\infty\gamma^t R_{t+2}(\pi)\middle| X_1=y, A_1=\pi(y)\right]}_{
 \begin{aligned}
 	&= \E\left[\sum_{t=0}^\infty\gamma^t R_{t+2}(\pi)\middle| X_1=y \right] \\
 	&\lxeq{(*)} \E\left[\sum_{t=0}^\infty\gamma^t \tilde{R}_{t+1}(\pi)\middle| \tilde{X}_0=y \right]=V^\pi(y)
 \end{aligned}
 }
 p(y\mid x, a)
\end{align*}
\((*)\) Rename: \(\tilde{X}_{t}\coloneqq X_{t+1}, \tilde{A}_t\coloneqq A_{t+1},\tilde{R}_{t}\coloneqq R_{t+1}\), then \((\tilde{X}_t,\tilde{A}_t,\tilde{R}_{t+1}, t\in\N_0)\) is an evaluation of the MDP with the (stationary) policy \(\pi\)
\end{proof}


\begin{corollary}\label{V^pi,Q^pi relation}  \(\cM=(\cX,\cA,\cP_0)\) MDP, \(\pi\in\detPolicy\) 
\begin{align*}
	V^\pi(x)&=Q^\pi(x,\pi(x))\\
	 &=r(x,\pi(x))+\gamma\sum\limits_{y\in\cX}p(y\mid x,\pi(x))V^\pi(y) 
\end{align*}
\end{corollary}

\begin{proof}
Since \(\pi\) is a deterministic stationary policy:
\[V^\pi(x)=\E[\cR(\pi)\mid X_0=x]=\E[\cR(\pi)\mid X_0=x, A_0=\pi(x)]=Q^\pi(x,\pi(x))\]
The rest follows from \ref{expand Q^pi}
\end{proof}

\begin{definition}\(\cM=(\cX,\cA,\cP_0)\) MDP, \(\pi\in\detPolicy\) \\
The mapping \(T^\pi\colon\R^\cX\to\R^\cX\) with:
	\[
	T^\pi V(x)\coloneqq r(x,\pi(x))+\gamma\sum_{y\in\cX}p(y\mid x,\pi(x)) V(y)\qquad V\in\R^\cX, x\in\cX
	\]
is called the \emph{Bellman Operator}
\end{definition}


\begin{remark}\label{properties T^pi}\leavevmode
	\begin{enumerate}[label=\arabic*.]
	\item \(\forall\pi\in\detPolicy : T^\pi V^\pi=V^\pi\) (c.f. \ref{V^pi,Q^pi relation})
	\item\label{num:2} \(T^\pi\) meets the requirements of the Banach fixed-point theorem for \({\gamma<1}\), this implies that \(V^\pi\) for \(\pi\in\detPolicy\)
	is a \emph{unique} fixpoint and can be approximated with the canonical iteration
	\item \(T^\pi\) is an affine operator
	\item\label{num:4} \(W_1,W_2\in\R^\cX\), write \(W_1 \le W_2\) for \(\forall x\in\cX:W_1(x)\le W_2(x)\), then:
	\[W_1\le W_2 \implies T^\pi W_1\le T^\pi W_2\]
	\end{enumerate}
\end{remark}

\begin{proof}
\ref{num:2} \((\R^\cX, \|\cdot\|_\infty)\) is a non-empty, complete metric space and the mapping maps onto itself. It is left to show, that \(T^\pi\) is a contraction. Be \(V,W\in\R^\cX\):
\begin{align*}
	\|T^\pi V -T^\pi W\|_\infty &=\|\gamma\sum_{y\in\cX} p(y\mid \cdot,\pi(\cdot)) (V(y)-W(y))\|_\infty\\
	&\le\gamma \sup_{x\in\cX}\left\{ \sum_{y\in\cX} p(y\mid x,\pi(x)) \|V-W\|_\infty \right\}\\
	&=\gamma \|V-W\|_\infty  \sup_{x\in\cX}\underbracket{\left\{ \sum_{y\in\cX} p(y\mid x,\pi(x)) \right\}}_{=1}\\
	&=\gamma \|V-W\|_\infty 
\end{align*}
\ref{num:4} Be \(W_1,W_2\in\R^\cX\), \(W_1\le W_2\) and \(x\in\cX\):
\begin{align*}
	T^\pi W_2 (x) - T^\pi W_1(x) 
	= \gamma \sum_{y\in\cX} p(y\mid x,\pi(x)) \underbracket{(W_2(y)-W_1(y))}_{\ge 0} 
	\ge 0
\end{align*}
\end{proof}


\begin{definition}\(\cM=(\cX,\cA,\cP_0)\) MDP
\begin{align*}
	\tilde{V}(x)&\coloneqq \sup_{\pi\in\detPolicy} V^\pi(x)
\end{align*}
\end{definition}


\begin{definition}\(\cM=(\cX,\cA,\cP_0)\) MDP\\
The mapping \(T^*\colon\R^\cX\to\R^\cX\) with:
	\[
	 T^* V(x)\coloneqq \sup_{a\in\cA}\left\{r(x,a)+\sum_{y\in\cX}p(y\mid x,a) V(y)\right\} \qquad V\in\R^\cX, x\in\cX
	\]
is called the \emph{Bellman Optimality Operator}
\end{definition}


\begin{lemma}\label{V*,Q* relation}\(\cM=(\cX,\cA,\cP_0)\) MDP
\begin{enumerate}[label=\textbf{(\roman*)},font=\normalfont]
\item\label{i:1} \(\tilde{V}(x)=\sup\limits_{a\in\cA} r(x,a)+\gamma\sum\limits_{y\in\cX}p(y\mid x,a)\tilde{V}(y) \)
\item\label{i:3} \(V^*(x)=\sup\limits_{a\in\cA}Q^*(x,a)\)
\item\label{i:4} \(Q^*(x,a)=r(x,a)+\gamma \sum\limits_{y\in\cX}p(y\mid x,a) V^*(y)\)
\end{enumerate}
\end{lemma}

\begin{proof} \ref{i:1}
By \ref{V^pi,Q^pi relation} we know \(V^\pi(x)=Q^\pi(x,\pi(x))\) thus:
\begin{align*}
	\tilde{V}(x)& =\sup_{\pi \in \detPolicy} V^\pi(x)\\
	&= \sup_{\pi\in\detPolicy} \left\{ r(x,\pi(x))+\gamma\sum\limits_{y\in\cX}p(y\mid x,\pi(x)) V^\pi(y) \right\}\\
	&\lxlq{(*)}\sup_{a\in \cA}\left\{ r(x,a)+\gamma\sum\limits_{y\in\cX}p(y\mid x,a) 
	\sup_{\pi\in\detPolicy} V^\pi(y) \right\}\\
	&=\sup_{a\in \cA}\left\{ r(x,a)+\gamma\sum\limits_{y\in\cX}p(y\mid x,a) 
	 \tilde{V}(y) \right\}
\end{align*}
Assume \((*)\) is a true inequality for some \(x\in\cX\), since the supremum can be arbitrarily closely approximated:
\[ \exists \pi, \exists a : \tilde{V}(x) < r(x,a) + \gamma\sum_{y\in\cX} p(y\mid x,a) V^\pi(y)\]
Define a slightly changed deterministic policy with this \(\pi,a\):
\begin{align*}
	\hat{\pi}\colon
	\begin{cases}
		\cX\to\cA\\
		y\mapsto
		\begin{cases}
			\pi(y) & y\neq x\\
			a & y=x
		\end{cases}
	\end{cases}
\end{align*}
Define \(W_n\coloneqq (T^{\hat{\pi}})^n V^\pi\), then:
\begin{align*}
	W_1(y)&=r(y, \hat{\pi}(y)) + \gamma \sum_{z\in\cX} p(z \mid y, \hat{\pi}(y)) V^\pi(z)  \\
	&=\begin{cases}
			r(y, \pi(y)) + \gamma \sum_{z\in\cX} p(z \mid y, \pi(y)) V^\pi(z) = V^\pi(x) & y\neq x  \\
			r(x, a) + \gamma \sum_{z\in\cX} p(z \mid x, a) V^\pi(z) > \tilde{V}(x)& y=x
	\end{cases}\\
	&\ge V^\pi(y) = W_0(y)
\end{align*}
By induction with \ref{properties T^pi} (\ref{num:4}): \(W_{n+1}=T^{\hat{\pi}}W_n \ge T^{\hat{\pi}}W_{n-1}=W_n\), thus:
\begin{align*}
	V^{\hat{\pi}}(x)&=\lim_{n\to\infty}(T^{\hat{\pi}})^n V^\pi(x)=\lim_{n\to\infty} W_n(x)\ge W_1(x) \\
	&=r(x, a) + \gamma \sum_{z\in\cX} p(z \mid x, a) V^\pi(z) \\
	&> \tilde{V}(x) \contradiction
\end{align*}
%TODO
\end{proof}


\begin{corollary}
	\begin{align*}
	&T^*\tilde{V}=\tilde{V}\\
	&T^*V^*=V^*
	\end{align*}
\end{corollary}

\begin{proof}
\begin{align*}
	V^*(x)\xeq{\ref{i:3}}\sup_{a\in\cA}Q^*(x,a)\xeq{\ref{i:4}}\sup_{a\in\cA}\left\{r(x,a)+\sum_{y\in\cX}p(y\mid x,a) V^*(y)\right\} =T^*V^*(x)
\end{align*}
\(\tilde{V}\) analogous
\end{proof}


\begin{thm}\(\cM=(\cX,\cA,\cP_0)\) MDP\\
\(T^*\) satisfies the requirements of the Banach fixpoint theorem, in particular:
	\[V^*(x)=\sup_{\pi\in\statPolicy}V^\pi(x)=\tilde{V}(x) \]
is the unique fixpoint of \(T^*\)
\end{thm}

\begin{lemma}(Blackwell's condition for contraction)
\end{lemma}

\begin{proof}
https://math.stackexchange.com/questions/1087885/blackwells-condition-for-a-contraction-why-is-boundedness-neccessary?rq=1
\end{proof}

\begin{proof}[Proof (Theorem)]
\end{proof}


\begin{prop}\label{sup is attained}\(\cM=(\cX,\cA,\cP_0)\) MDP\\
The following statements are equivalent:
\begin{enumerate}[label={(\roman*)},font=\normalfont]
\item \(\pi \in\statPolicy\) is optimal (\(V^*=V^\pi\))
\item \(\forall x\in\cX: V^*(x)=\sum\limits_{a\in\cA}\pi(a\mid x)Q^*(x,a)\)
\item\label{ii:3} \(\forall x\in\cX: \pi=\arg\max\limits_{\pi\in\statPolicy}\sum\limits_{a\in\cA}\pi(a\mid x)Q^*(x,a) \)
\item \(\pi(a\mid x)>0 \iff Q^*(x,a)=V^*(x) =\sup\limits_{b\in\cA}Q*(x,b)\) \\
	``actions are concentrated on the set of actions that maximize \(Q^*(x,\cdot)\)''\\
	(this also implies: \(Q^*(x,a)<V^*(x) \implies \pi(a\mid x)=0\))
\end{enumerate}
\end{prop}

\begin{proof}
%TODO
\end{proof}


\begin{definition}
	\(Q\colon\cX\times\cA\to\R\) an action value function, \(\tilde{\pi}\colon\cX\to\cA\) with:
	\[
	\tilde{\pi}(x)\coloneqq\arg\max_{\pi\in\statPolicy}\sum_{a\in\cA}\pi(a\mid x) Q(x,a)\qquad x\in\cX
	\]
	\(\tilde{\pi}(x)\) is called \emph{greedy} with respect to Q in \(x\in\cX\)\\
	\(\tilde{\pi}\) is called \emph{greedy} w.r.t. Q
\end{definition}


\begin{remark}\leavevmode
	\begin{itemize}
	\item \ref{sup is attained}\ref{ii:3} implies that greedy w.r.t. \(Q^*\) is optimal. 
	This means that knowledge of \(Q^*\) is sufficient to select the best action.
	\item \ref{V*,Q* relation} implies that knowledge of \(V^*,r,p\) is sufficient as well.
	\end{itemize}
\end{remark}
\endinput
