\chapter{Markov Decision Processes}
While Markov Processes allow to model random phenomena evolving over time and make predictions about certain events (e.g. terminal states), they are unable to model the interaction of an actor with such a processes. \emph{Markov Decicion Processes} (MDPs) introduce actions and rewards to the model of Markov Processces, and shift the focus from \emph{describing} terminal distributions, absorption types, etc. towards \emph{finding} the optimal action(s) to take in each state.\\
To illustrate the uses of such a framework, I have selected a few examples from \textcite{whiteRealApplicationsMarkov1985}:
\begin{enumerate}
	\item Resource Management: The state is the resource level
	\begin{itemize}
		\item Inventory Management: The resource is the inventory, the possible action is to order resupply, influencing the inventory (state) together with the stochastic demand, and the reward is the profit. The essential trade-off is the cost of storage versus lost sales from a stock-out.
		\item Fishing: The resource is the amount of fish, the action is the amount fished, the reward is directly proportional to the amount fished, and the repopulation is the random element.
		\item Pumped storage Hydro-power: The state is the amount of water in the higher reservoir and the electricity price, the action is to use water to generate electricity or wait for higher prices.
		\item Beds in a hospital: How many empty beds are needed for emergencies?
	\end{itemize}
	\item Stock trading: The state is the price level and stock and liquidity owned.
	\item Maintenance: When does a car/road become too expensive to repair?
	\item Evacuation in response to flood forecasts
\end{enumerate}
The MDP model inherits the restriction of Markov Chains to have no memory of past states. We will also not consider changing transition probabilities over time. Rather the transition probabilities will only be influenced by the state and the action. \\
Both of these limitations could in principle be circumvented by including the time in the state space at the expense of a larger state space. Although it is questionable whether such a construct would yield any interesting results, as then no state is visited twice. So it is of no use to an actor to learn the value of an action in a certain state without further assumptions.
%countable state space


One of the obstactles to this goal is the question whether an optimal action even exists.
\begin{definition}(Kernel)
	\((Y,\cA_Y), (X,\cA_X)\) measure spaces\\
	 \(\lambda\colon X\times\cA_Y\to \R\) is a \emph{(probability) kernel} \(
	 :\iff \begin{aligned}[t]
	 &\lambda(\cdot,A)\colon x\mapsto \lambda(x,A) \text{ measurable}\\
	 &\lambda(x,\cdot)\colon A\mapsto \lambda(x,A) \text{ a (prob.) measure}
	 \end{aligned}
	  \)\\
	  Since we will interpret probability kernels as distributions over \(Y\) given a  certain condition \(X\), the notation \(\lambda(\cdot\mid x) \coloneqq \lambda(x,\cdot)\) helps this intuition. 
\end{definition}
\begin{definition}(Markov Decision Process - MDP)\\
\(\cM=(\cX,\cA,\cP_0) \), with:
	
\begin{tabular}{l l l}
	\(\cX\) & countable (finite) set of states&\\
	\(\cA\) & countable (finite) set of actions &\\
	\multicolumn{2}{l}{
		\(
		\begin{cases}
		\cX\times \cA \to \pmeas(\cX\times\R)\\
		(x,a)\mapsto \cP_0(\cdot \mid x,a)
		\end{cases}
		\)
	}  &
	\parbox[h]{17em}{
		\emph{transition probability kernel} \\
		\(\pmeas(\cX\times\R) \) the set of probability measures on \(\cX\times\R \), \\
		\(\cX\) represents the next states,\\
		\(\R\) the payoffs
	}
	\end{tabular}\\
is a \emph{(finite) Markov Decision Process}.\\
Together with a discount factor \(\gamma\in[0,1]\) it is a:\\
\begin{tabular}{l l}
	\emph{discounted reward} MDP & \(\gamma <1 \)\\
	\emph{undiscounted reward} MDP & \(\gamma=1 \)
\end{tabular}\\
For \((Y_{(x,a)}, R_{(x,a)})\sim \cP_0(\cdot\mid x,a) \) a random variable, is
\[r(x,a)\coloneqq \E[R_{(x,a)}] \quad\text{ the \emph{immediate reward function}}\]
An MDP is \emph{evaluated} as follows:\\
1. Select the initial state \(X_0\) an \(\cX\)-valued random variable.\\ 
2. \((A_t, t\in \N)\) action selection rules (behaviors) will be discussed later, for now simply assume \(\cA\)-valued random variables.\\
3. Select inductively: \((X_{t+1}, R_{t+1})\sim\cP_0(\cdot\mid X_t, A_t)\) with the markov property, i.e.:
\begin{align*} 
&\Pr[(X_{t+1},R_{t+1})=(x,r) \mid (X_t,A_t)=(x_t,a_t),\dots, (X_0,A_0)=(x_0,a_0)]\\
&=\Pr[(X_{t+1}, R_{t+1})=(x,r)\mid (X_t,A_t)=(x_t,a_t)]
\end{align*}
resulting in the stochastic process \(((X_t,A_t,R_{t+1}), t\geq 0)\), which allows to define the \emph{return}:
\[\cR\coloneqq\sum_{t=0}^{\infty}\gamma^t R_{t+1}\]
\end{definition}
%(

\begin{remark}
\((X_{t+1}, R_{t+1})\sim\cP_0(\cdot\mid X_t, A_t)\) with the markov property is well defined, i.e.:
\begin{align*}
	&\exists (X_{t+1}, R_{t+1})\ \cX\times\R\text{-valued random variable}: \\ 
	&(X_{t+1}, R_{t+1})\sim\cP_0(\cdot\mid X_t, A_t) \text{ and satisfies the markov property}
\end{align*}
\end{remark}
\begin{proof}
	%TODO
\end{proof}
\begin{remark}\leavevmode
\begin{enumerate}
	\item From now on we assume that \(\forall (x,a)\in\cX\times\cA:|R_{(x,a)}|\le R\in\R\) almost surely. This also implies: 			\(
	\begin{aligned}[t]
		&\|r\|_{\infty}=\sup_{(x,a)\in\cX\times\cA}|\E[R_{(x,a)}]|\le R\\
		&|\cR|\le\sum_{t=0}^\infty \gamma^t |R_{t+1}|\le \frac{R}{1-\gamma} \text{ a.s.}
	\end{aligned}
	\)
	\item Sometimes not all actions make sense in all states. A simple fix would be to set the immediate reward functions for those actions very low, or (if possible) redirect them to the closest possible action. \\
	A more formal approach would be to introduce an additional mapping, which assigns the set of admissible actions to each state \(\cX\to\cP(\cA)\), or alternatively define a (binary) relation on \(\cX\times\cA\).
	\item If there is just one admissible action in every state, the MDP is equivalent to a normal Markov Process.
	\item Instead of a transition probability kernel \(\cP_0\), sometimes a \emph{transition function} f with a and an exogenous random element \(D_t\) (e.g. Demand) is used to define the next state and reward: \((X_{t+1},R_{t+1})=f(X_t,A_t,D_t)\) 
\end{enumerate}
\end{remark}
\begin{definition}\(\cM=(\cX,\cA,\cP_0)\) a MDP\\
	\(x\in\cX\) is a \emph{terminal (absorbing)} state \(:\iff \forall s\in\N: \Pr(X_{t+s}=x\mid X_t=x)=1\)\\
	An MDP with such states is called \emph{episodic}.\\
	An \emph{episode} is the random time period \((1,\dots,T)\) until a terminal state is reached.
\end{definition}
\begin{remark}\leavevmode
	\begin{itemize}
		\item The reward in a terminal state is by convention zero, i.e. \(x\) terminal state implies \(\forall a\in\cA: R_{(x,a)}=0\).
		\item Episodic MDPs are often undiscounted
	\end{itemize}	
\end{remark}
\begin{definition}\(\cM=(\cX,\cA,\cP_0)\) a MDP\\
	 An \(A_t\) selection-rule \(\pi=(\pi_t,t\in\N_0)\) is called \emph{behavior}, where
	 \[ 
	 	\pi_t\colon
	 	\begin{cases}
	 		((\cX\times\cA\times\R)^t\times\cX)\times\cP(\cA) \to \R \\
	 		(y,A)\mapsto \pi_t(A\mid y)
	 	\end{cases} \text{ is a probability kernel}
	 \]
	 and \(A_t\sim \pi_t(\cdot\mid (X_0,A_0,R_1), \dots,(X_{t-1},A_{t-1},R_t),X_t))\)\\
	 Special cases:
	 \begin{enumerate}
	 	\item \emph{Determinisitic stationary policies} specified with some abuse of notation:
	 	\[\leavevmode \pi\colon\cX\to\cA \text{ with }A_t=\pi(X_t)\]
	 	\item \emph{(Stochastic) stationary policies} specified by:
	 	\[\pi\colon \begin{cases}
	 	\cX\times\cP(\cA)\to\R\\
	 	(x,A)\mapsto \pi(A\mid x)
	 	\end{cases} \text{ with } A_t\sim\pi(\cdot\mid x)
	 	\]
	 \end{enumerate}
	 \(\statPolicy\) is the \emph{set of (stoch.) stationary policies}, \\ 
	 \(\detPolicy\) is the \emph{set of deterministic stationary policies} (note \(\detPolicy\subseteq\statPolicy \))
\end{definition}
\begin{remark}
A stationary policy induces a \emph{time-homogenous} markov chain.
\end{remark}
\begin{definition}(Markov Reward Process - MRP)
%TODO
\end{definition}
\section{Value functions}
The goal in this section is to
\begin{itemize}[itemsep=0pt, topsep=1pt]
\item define Value functions which assign states (and actions) a value, which allow the agent to make a more nuanced decisions than comparing immediate rewards of different actions
\item explore the relation of different value functions
\item show uniqueness of optimal value functions with the Banach fixpoint theorem, yielding a simple approximation methode along the way
\item demonstrate that in MDPs deterministic stationary policies are generally a large enough set of policies to choose from
 \end{itemize}
\begin{definition}\(\cM=(\cX,\cA,\cP_0)\) MDP, \(\pi\) Behavior\\
Select \(X_0\) such that \(\forall x\in\cX:\Pr(X_0=x)>0\) and evaluate the MDP with
\(((X_t,A_t,R_{t+1}), t\in \N_0)\) the resulting stoch. process.\\
\def\arraystretch{3}
\[
\begin{array}{l l}
	V^\pi\colon
	\begin{cases}
		\cX\to\R\\
		x\mapsto \E[\cR\mid X_0=x]
	\end{cases} 
	& \text{is the \emph{value function} for } \pi \footnotemark\\
	Q^\pi\colon
	\begin{cases}
		\cX\times\cA\to\R\\
		(x,a)\mapsto \E[\cR\mid X_0=x, A_0=a]
	\end{cases}
	& \text{is the \emph{action value function} for } \pi \footnotemark\\
	V^*\colon
	\begin{cases}
		\cX\to\R\\
		x\mapsto \sup\limits_{\pi\text{ Behav.}} V^\pi(x)
	\end{cases} 
	& \text{is the \emph{optimal value function}}\\
	Q^*\colon
	\begin{cases}
		\cX\times\cA\to\R\\
		(x,a)\mapsto \sup\limits_{\pi\text{ Behav.}} Q^\pi(x,a)
	\end{cases}
	& \text{is the \emph{optimal action value function}}
\end{array}
\]
\(\pi\) is \emph{optimal} \(:\iff V^*=V^\pi\)
\end{definition}
\footnotetext[1]{Well defined because \(\Pr(X_0=x)>0\)}
\footnotetext{Well defined because \(A_1\sim \pi_1(\cdot\mid (x,a,r_0), x_1)\) is defined for all \(a\) regardless of \(\pi_0\)}

\begin{remark}
With the distribution of \(X_0\) set (or \(X_0\) being realized with a fixed value \(x\)), the distribution of \(X_t, A_t,R_{t+1}\) is determined for all \(t\in\N_0\). The conditional expectation is thus unique for a given \(X_0=x\), for all possible realizations of the MDP with a given behavior. \\
This means \(V^\pi, Q^\pi\) are well defined.
\end{remark}


\begin{definition}\(\cM=(\cX,\cA,\cP_0)\) MDP\\
Sometimes we don't care about the probability distribution of the reward, so we define:
	\[
	p\colon 
	\begin{cases}
		\cX\times\cA\times\cP(\cX)\to\R\\
		(x,a,Y)\mapsto \cP_0(Y\times\R\mid x,a)
	\end{cases}\text{ the \emph{\underline{state} transition kernel}.}
	\] 
And use the notation \(p(y\mid x,a)\coloneqq p(\{y\}\mid x,a)\) with \((x,a,y)\in\cX\times\cA\times\cX\)
\end{definition}


\begin{prop}\label{expand Q^pi}\(\cM=(\cX,\cA,\cP_0)\) MDP, \(\pi\in\detPolicy\) 
	\[Q^\pi(x,a)=r(x,a)+\gamma\sum_{y\in\cX}p(y\mid x,a)V^\pi(y)	\]
\end{prop}

\begin{proof}
\begin{align*}
Q^\pi &= \E[\cR(\pi)\mid X_0=x, A_0=a]\\
&=\E[R_1(\pi)\mid X_0=x,A_0=a]+\gamma\E\left[\sum_{t=0}^\infty\gamma^t R_{t+2}(\pi)\middle| X_0=x,A_0=a\right]\\
&=\E[R_{(x,a)}] 
 + \gamma \sum_{y\in\cX}\E\left[\sum_{t=0}^\infty\gamma^t R_{t+2}(\pi) \middle| X_0=x,A_0=a, X_1=y\right]p(y\mid x, a)\\
&\lxeq{\text{Markov}} r(x,a)
 + \gamma\sum_{y\in\cX}\underbracket{\E\left[\sum_{t=0}^\infty\gamma^t R_{t+2}(\pi)\middle| X_1=y, A_1=\pi(y)\right]}_{
 \begin{aligned}
 	&= \E\left[\sum_{t=0}^\infty\gamma^t R_{t+2}(\pi)\middle| X_1=y \right] \\
 	&\lxeq{(*)} \E\left[\sum_{t=0}^\infty\gamma^t \tilde{R}_{t+1}(\pi)\middle| \tilde{X}_0=y \right]=V^\pi(y)
 \end{aligned}
 }
 p(y\mid x, a)
\end{align*}
\((*)\) Rename: \(\tilde{X}_{t}\coloneqq X_{t+1}, \tilde{A}_t\coloneqq A_{t+1},\tilde{R}_{t}\coloneqq R_{t+1}\), then \((\tilde{X}_t,\tilde{A}_t,\tilde{R}_{t+1}, t\in\N_0)\) is an evaluation of the MDP with the (stationary) policy \(\pi\)
\end{proof}


\begin{corollary}\label{V^pi,Q^pi relation}  \(\cM=(\cX,\cA,\cP_0)\) MDP, \(\pi\in\detPolicy\) 
\begin{align*}
	V^\pi(x)&=Q^\pi(x,\pi(x))\\
	 &=r(x,\pi(x))+\gamma\sum\limits_{y\in\cX}p(y\mid x,\pi(x))V^\pi(y) 
\end{align*}
\end{corollary}

\begin{proof}
Since \(\pi\) is a deterministic stationary policy:
\[V^\pi(x)=\E[\cR(\pi)\mid X_0=x]=\E[\cR(\pi)\mid X_0=x, A_0=\pi(x)]=Q^\pi(x,\pi(x))\]
The rest follows from \ref{expand Q^pi}
\end{proof}

\begin{definition}\(\cM=(\cX,\cA,\cP_0)\) MDP, \(\pi\in\detPolicy\) \\
The mapping \(T^\pi\colon\R^\cX\to\R^\cX\) with:
	\[
	T^\pi V(x)\coloneqq r(x,\pi(x))+\gamma\sum_{y\in\cX}p(y\mid x,\pi(x)) V(y)\qquad V\in\R^\cX, x\in\cX
	\]
is called the \emph{Bellman Operator}
\end{definition}


\begin{remark}\label{properties T^pi}\leavevmode
	\begin{enumerate}[label=\arabic*.]
	\item \(\forall\pi\in\detPolicy : T^\pi V^\pi=V^\pi\) (c.f. \ref{V^pi,Q^pi relation})
	\item\label{num:2} \(T^\pi\) meets the requirements of the Banach fixed-point theorem for \({\gamma<1}\), this implies that \(V^\pi\) for \(\pi\in\detPolicy\)
	is a \emph{unique} fixpoint and can be approximated with the canonical iteration
	\item \(T^\pi\) is an affine operator
	\item\label{num:4} \(W_1,W_2\in\R^\cX\), write \(W_1 \le W_2\) for \(\forall x\in\cX:W_1(x)\le W_2(x)\), then:
	\[W_1\le W_2 \implies T^\pi W_1\le T^\pi W_2\]
	\end{enumerate}
\end{remark}

\begin{proof}
\ref{num:2} \((\R^\cX, \|\cdot\|_\infty)\) is a non-empty, complete metric space and the mapping maps onto itself. It is left to show, that \(T^\pi\) is a contraction. Be \(V,W\in\R^\cX\):
\begin{align*}
	\|T^\pi V -T^\pi W\|_\infty &=\|\gamma\sum_{y\in\cX} p(y\mid \cdot,\pi(\cdot)) (V(y)-W(y))\|_\infty\\
	&\le\gamma \sup_{x\in\cX}\left\{ \sum_{y\in\cX} p(y\mid x,\pi(x)) \|V-W\|_\infty \right\}\\
	&=\gamma \|V-W\|_\infty  \sup_{x\in\cX}\underbracket{\left\{ \sum_{y\in\cX} p(y\mid x,\pi(x)) \right\}}_{=1}\\
	&=\gamma \|V-W\|_\infty 
\end{align*}
\ref{num:4} Be \(W_1,W_2\in\R^\cX\), \(W_1\le W_2\) and \(x\in\cX\):
\begin{align*}
	T^\pi W_2 (x) - T^\pi W_1(x) 
	= \gamma \sum_{y\in\cX} p(y\mid x,\pi(x)) \underbracket{(W_2(y)-W_1(y))}_{\ge 0} 
	\ge 0
\end{align*}
\end{proof}


\begin{definition}\(\cM=(\cX,\cA,\cP_0)\) MDP
\begin{align*}
	\tilde{V}(x)&\coloneqq \sup_{\pi\in\detPolicy} V^\pi(x)
\end{align*}
\end{definition}


\begin{definition}\(\cM=(\cX,\cA,\cP_0)\) MDP\\
The mapping \(T^*\colon\R^\cX\to\R^\cX\) with:
	\[
	 T^* V(x)\coloneqq \sup_{a\in\cA}\left\{r(x,a)+\sum_{y\in\cX}p(y\mid x,a) V(y)\right\} \qquad V\in\R^\cX, x\in\cX
	\]
is called the \emph{Bellman Optimality Operator}
\end{definition}


\begin{lemma}\label{V*,Q* relation}\(\cM=(\cX,\cA,\cP_0)\) MDP
\begin{enumerate}[label=\textbf{(\roman*)},font=\normalfont]
\item\label{i:1} \(\tilde{V}(x)=\sup\limits_{a\in\cA} r(x,a)+\gamma\sum\limits_{y\in\cX}p(y\mid x,a)\tilde{V}(y) \)
\item\label{i:3} \(V^*(x)=\sup\limits_{a\in\cA}Q^*(x,a)\)
\item\label{i:4} \(Q^*(x,a)=r(x,a)+\gamma \sum\limits_{y\in\cX}p(y\mid x,a) V^*(y)\)
\end{enumerate}
\end{lemma}

\begin{proof} \ref{i:1}
By \ref{V^pi,Q^pi relation} we know \(V^\pi(x)=Q^\pi(x,\pi(x))\) thus:
\begin{align*}
	\tilde{V}(x)& =\sup_{\pi \in \detPolicy} V^\pi(x)\\
	&= \sup_{\pi\in\detPolicy} \left\{ r(x,\pi(x))+\gamma\sum\limits_{y\in\cX}p(y\mid x,\pi(x)) V^\pi(y) \right\}\\
	&\lxlq{(*)}\sup_{a\in \cA}\left\{ r(x,a)+\gamma\sum\limits_{y\in\cX}p(y\mid x,a) 
	\sup_{\pi\in\detPolicy} V^\pi(y) \right\}\\
	&=\sup_{a\in \cA}\left\{ r(x,a)+\gamma\sum\limits_{y\in\cX}p(y\mid x,a) 
	 \tilde{V}(y) \right\}
\end{align*}
Assume \((*)\) is a true inequality for some \(x\in\cX\), since the supremum can be arbitrarily closely approximated:
\[ \exists \pi, \exists a : \tilde{V}(x) < r(x,a) + \gamma\sum_{y\in\cX} p(y\mid x,a) V^\pi(y)\]
Define a slightly changed deterministic policy with this \(\pi,a\):
\begin{align*}
	\hat{\pi}\colon
	\begin{cases}
		\cX\to\cA\\
		y\mapsto
		\begin{cases}
			\pi(y) & y\neq x\\
			a & y=x
		\end{cases}
	\end{cases}
\end{align*}
Define \(W_n\coloneqq (T^{\hat{\pi}})^n V^\pi\), then:
\begin{align*}
	W_1(y)&=r(y, \hat{\pi}(y)) + \gamma \sum_{z\in\cX} p(z \mid y, \hat{\pi}(y)) V^\pi(z)  \\
	&=\begin{cases}
			r(y, \pi(y)) + \gamma \sum_{z\in\cX} p(z \mid y, \pi(y)) V^\pi(z) = V^\pi(x) & y\neq x  \\
			r(x, a) + \gamma \sum_{z\in\cX} p(z \mid x, a) V^\pi(z) > \tilde{V}(x)& y=x
	\end{cases}\\
	&\ge V^\pi(y) = W_0(y)
\end{align*}
By induction with \ref{properties T^pi} (\ref{num:4}): \(W_{n+1}=T^{\hat{\pi}}W_n \ge T^{\hat{\pi}}W_{n-1}=W_n\), thus:
\begin{align*}
	V^{\hat{\pi}}(x)&=\lim_{n\to\infty}(T^{\hat{\pi}})^n V^\pi(x)=\lim_{n\to\infty} W_n(x)\ge W_1(x) \\
	&=r(x, a) + \gamma \sum_{z\in\cX} p(z \mid x, a) V^\pi(z) \\
	&> \tilde{V}(x) \contradiction
\end{align*}
%TODO
\end{proof}


\begin{corollary}
	\begin{align*}
	&T^*\tilde{V}=\tilde{V}\\
	&T^*V^*=V^*
	\end{align*}
\end{corollary}

\begin{proof}
\begin{align*}
	V^*(x)\xeq{\ref{i:3}}\sup_{a\in\cA}Q^*(x,a)\xeq{\ref{i:4}}\sup_{a\in\cA}\left\{r(x,a)+\sum_{y\in\cX}p(y\mid x,a) V^*(y)\right\} =T^*V^*(x)
\end{align*}
\(\tilde{V}\) analogous
\end{proof}


\begin{thm}\(\cM=(\cX,\cA,\cP_0)\) MDP\\
\(T^*\) satisfies the requirements of the Banach fixpoint theorem, in particular:
	\[V^*(x)=\sup_{\pi\in\statPolicy}V^\pi(x)=\tilde{V}(x) \]
is the unique fixpoint of \(T^*\)
\end{thm}

\begin{lemma}(Blackwell's condition for contraction)
\end{lemma}

\begin{proof}
https://math.stackexchange.com/questions/1087885/blackwells-condition-for-a-contraction-why-is-boundedness-neccessary?rq=1
\end{proof}

\begin{proof}[Proof (Theorem)]
\end{proof}


\begin{prop}\label{sup is attained}\(\cM=(\cX,\cA,\cP_0)\) MDP\\
The following statements are equivalent:
\begin{enumerate}[label={(\roman*)},font=\normalfont]
\item \(\pi \in\statPolicy\) is optimal (\(V^*=V^\pi\))
\item \(\forall x\in\cX: V^*(x)=\sum\limits_{a\in\cA}\pi(a\mid x)Q^*(x,a)\)
\item\label{ii:3} \(\forall x\in\cX: \pi=\arg\max\limits_{\pi\in\statPolicy}\sum\limits_{a\in\cA}\pi(a\mid x)Q^*(x,a) \)
\item \(\pi(a\mid x)>0 \iff Q^*(x,a)=V^*(x) =\sup\limits_{b\in\cA}Q*(x,b)\) \\
	``actions are concentrated on the set of actions that maximize \(Q^*(x,\cdot)\)''\\
	(this also implies: \(Q^*(x,a)<V^*(x) \implies \pi(a\mid x)=0\))
\end{enumerate}
\end{prop}

\begin{proof}
%TODO
\end{proof}


\begin{definition}
	\(Q\colon\cX\times\cA\to\R\) an action value function, \(\tilde{\pi}\colon\cX\to\cA\) with:
	\[
	\tilde{\pi}(x)\coloneqq\arg\max_{\pi\in\statPolicy}\sum_{a\in\cA}\pi(a\mid x) Q(x,a)\qquad x\in\cX
	\]
	\(\tilde{\pi}(x)\) is called \emph{greedy} with respect to Q in \(x\in\cX\)\\
	\(\tilde{\pi}\) is called \emph{greedy} w.r.t. Q
\end{definition}


\begin{remark}\leavevmode
	\begin{itemize}
	\item \ref{sup is attained}\ref{ii:3} implies that greedy w.r.t. \(Q^*\) is optimal. 
	This means that knowledge of \(Q^*\) is sufficient to select the best action.
	\item \ref{V*,Q* relation} implies that knowledge of \(V^*,r,p\) is sufficient as well.
	\end{itemize}
\end{remark}
\endinput
