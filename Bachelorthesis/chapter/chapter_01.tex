% !TEX root = ../BScWIMEngl.tex  



\chapter{Markov Decision Processes}
\section{Introduction}
A Markov Process is a random process in a state space with no memory of where it was, that is, only the current state influences where it goes next.
While Markov Processes allow to model random phenomena evolving over time and make predictions about certain events (e.g. terminal states), they are unable to model the interaction of an actor with such a processes. \emph{Markov Decision Processes} (MDPs) introduce \emph{actions} and \emph{rewards} to the state space and transition probabilities of Markov Processces, and shift the focus from \emph{describing} terminal distributions, absorption times, etc. towards \emph{finding} the optimal action(s) to take in each state (if such an action exists).

The MDP model inherits the restriction of Markov Chains to have no memory of past states. We will also not consider changing transition probabilities over time. Rather the transition probabilities will only be influenced by the state and the action. \\
Both of these limitations could in principle be circumvented by including the time in the state space at the expense of a larger state space. Although it is questionable whether such a construct would yield any interesting results, as then no state is visited twice. So it is of no use to an actor to learn the value of an action in a certain state without further assumptions.

To illustrate the uses of such a framework, I have selected a few examples from \textcite{whiteRealApplicationsMarkov1985}:
\begin{enumerate}
	\item Resource Management: The state is the resource level
	\begin{itemize}
		\item Inventory Management: The resource is the inventory, the possible action is to order resupply, influencing the inventory (state) together with the stochastic demand, and the reward is the profit. The essential trade-off is the cost of storage versus lost sales from a stock-out.
		\item Fishing: The resource is the amount of fish, the action is the amount fished, the reward is directly proportional to the amount fished, and the repopulation is the random element.
		\item Pumped storage Hydro-power: The state is the amount of water in the higher reservoir and the electricity price, the action is to use water to generate electricity or wait for higher prices.
		\item Beds in a hospital: How many empty beds are needed for emergencies?
	\end{itemize}
	\item Stock trading: The state is the price level and stock and liquidity owned.
	\item Maintenance: When does a car/road become too expensive to repair?
	\item Evacuation in response to flood forecasts
\end{enumerate}

\section{Model Formulation}
%countable state space
Most of the definitions in this chapter are adaptions from \textcite{szepesvariAlgorithmsReinforcementLearning2010}.
But to properly define the transition probabilities given an action in a certain state, let us define a probability kernel first.

\begin{definition}(Kernel)
	Let \((Y,\sigma_Y), (X,\sigma_X)\) be measure spaces.
	 \begin{align*}
	 &\lambda\colon X\times\sigma_Y\to \R \text{ is a \emph{(probability) kernel}}\\
	 &:\iff \begin{aligned}[t]
	 &\lambda(\cdot,A)\colon x\mapsto \lambda(x,A) \text{ measurable}\\
	 &\lambda(x,\cdot)\colon A\mapsto \lambda(x,A) \text{ a (probability) measure}
	 \end{aligned}
	  \end{align*}
	  Since we will interpret probability kernels as distributions over \(Y\) given a  certain condition \(x\in X\), the notation \(\lambda(\cdot\mid x) \coloneqq \lambda(x,\cdot)\) helps this intuition. 
\end{definition}

\begin{definition} \leavevmode \\ 
	\(\cM=(\cX,\cA,\cP_0) \) is called a \emph{(finite) Markov Decision Process} (MDP). Where:
	\par\begin{tabular}{l l}
		\(\cX\) & is a countable (finite) set of states.\\
		\(\cA\) & is a countable (finite) set of actions.
	\end{tabular}\\
	And \(\cP_0\colon (\cX\times\cA) \times \sigma_{\cX\times\R} \to \R\) is a probability kernel.\\ \\
	\(\cX\times\R\) represents the next state and the reward. So \(\cP_0(\cdot\mid x,a) \) represents the probability distribution over the next states and rewards given an action \(a\) in the state \(x\).
	\\ \\
	\(\quad\) \(\cP_0\) is called the \emph{transition probability kernel} or in short, transition kernel. 
\end{definition}
\begin{remark}
	Instead of a transition probability kernel \(\cP_0\), sometimes a \emph{transition function} f with a and an exogenous random element \(D_t\) (e.g. Demand) is used to define the next state and reward: \((X_{t+1},R_{t+1})=f(X_t,A_t,D_t)\)

	Some authors include a Time set \(T\) in the tuple \parencite[e.g.][]{putermanMarkovDecisionProcesses2014} this allows for finite horizons but not for continuous time, since the transition kernel is defined for discrete steps. Most authors split the transition kernel into a state transition kernel and a reward kernel \parencite[e.g.][]{putermanMarkovDecisionProcesses2014}. But since it is easier to define a marginal distribution from a joint distribution than vice versa, and since this notation is more compact I will stick to the definition from \textcite{szepesvariAlgorithmsReinforcementLearning2010}.
\end{remark}

According to \textcite{putermanMarkovDecisionProcesses2014} some authors call this tuple a Markov Decision Problem instead of Markov Decision Process, presumably to reserve the term Markov Decision Process for the resulting sequence of states, actions and rewards \((X_t,A_t,R_{t+1},t\in\N_0)\), aligning the Definition with the definition of a Markov process. Although this does not appear to be common practice.

I can find no explanation for this deviation from the notation of Markov processes. So I offer my own interpretation:

The objective of the theory of MDPs is to find an optimal action selection rule (behavior). And without a fixed behavior the sequence \((X_t, A_t, R_{t+1}, t\in\N_0 )\) is undefined, since the \((A_t,t\in\N_0)\) are not defined. But fixing the behavior defeats the purpose of modeling decisions. As it would not make sense to talk about optimal behaviors in an MDP if every behavior creates its own MDP.\vspace{1\baselineskip} 

Nevertheless we still need to construct a stochastic process from the MDP when we have an action selection rule. 

First we need to select the random variable \(X_0\) of the initial state. The initial state is not included in the definition of an MDP because later objects will be defined conditional on the current state. They are thus invariant to different starting distributions, as long as \(\Pr(X_0=x)>0\) holds for all \(x\in\cX\) ensuring that conditioning on every state is possible.

To inductively define a stochastic process we need an action selection rule, more formally:

\begin{definition} An \(A_t\) selection-rule \(\pi=(\pi_t,t\in\N_0)\) is called \emph{behavior}, where
	\[ 
		\pi_t\colon
		\begin{cases}
			[(\cX\times\cA\times\R)^t\times\cX]\times\sigma_\cA \to \R \\
			(y,A)\mapsto \pi_t(A\mid y)
		\end{cases} \text{ is a probability kernel,}
	\]
	and \(A_t\sim \pi_t(\cdot\mid (X_0,A_0,R_1), \dots,(X_{t-1},A_{t-1},R_t),X_t))\).\\
	Special cases:
	\begin{enumerate}
		\item \emph{Determinisitic stationary policies} specified with some abuse of notation:
		\[\leavevmode \pi\colon\cX\to\cA \text{ with }A_t=\pi(X_t)\]
		\item \emph{(Stochastic) stationary policies} specified by:
		\[\pi\colon \begin{cases}
		\cX\times\sigma_\cA\to\R\\
		(x,A)\mapsto \pi(A\mid x)
		\end{cases} \text{ with } A_t\sim\pi(\cdot\mid X_t) 
		\]
		\(A_t\) is here selected such that it has the markov property (well defined c.f. \ref{well defined markov property}), i.e.:
		\[\Pr[A_t=a\mid X_t]=\Pr[A_t=a\mid X_t, (X_{t-1},A_{t-1},R_t), \dots (X_0,A_0,R_1)] \]
	\end{enumerate}
	\(\Pi\) is the set of behaviors,\\
	\(\statPolicy\) is the set of (stochastic) stationary policies, \\
	\(\detPolicy\) is the set of deterministic stationary policies (note \(\detPolicy\subseteq\statPolicy\subseteq \Pi \))
\end{definition}

Now we define inductively: \((X_{t+1}, R_{t+1})\sim\cP_0(\cdot\mid X_t, A_t)\) with the Markov property (well defined c.f. \ref{well defined markov property}), i.e.:
\begin{align}
\label{X,R Markov}
	&\Pr[(X_{t+1}, R_{t+1})=(x,r)\mid (X_t,A_t)]\\
	&=\Pr[(X_{t+1},R_{t+1})=(x,r) \mid (X_t,A_t),(X_{t-1},A_{t-1},R_t), 
	\dots (X_0,A_0,R_1)] \nonumber
\end{align}
resulting in the stochastic process \(((X_t,A_t,R_{t+1}), t\in \N_0)\)
\begin{remark}\label{well defined markov property}
	\((X_{t+1}, R_{t+1})\sim\cP_0(\cdot\mid X_t, A_t)\) with the Markov property, is well defined i.e.:
	\begin{align*}
		&\exists (X_{t+1}, R_{t+1})\ \cX\times\R\text{-valued random variable}: \\ 
		&(X_{t+1}, R_{t+1})\sim\cP_0(\cdot\mid X_t, A_t) \text{ and satisfies the Markov property}
	\end{align*}
	(analogous \(A_t\) well defined)
\end{remark}
\begin{proof}
	\textcolor{red}{TODO}
\end{proof}


\begin{remark}\label{statPolicy induces time-homogeneous MC}
	A stationary policy \(\pi\) induces a \emph{time-homogeneous} Markov chain \((X_t,A_t,R_{t+1},t\in\N_0)\).
\end{remark}
\begin{proof} 
	\(\cH_s^t\coloneqq \{(X_t,A_t,R_{t+1})\in H_t,\dots,(X_s,A_s,R_{s+1})\in H_s \},\ H_i \in \sigma_{\cX\times\cA\times\R}\)
	Because of
	\[
		\Pr(A\cap B \mid C)
		=\frac{\Pr(A\cap B\cap C)}{\Pr(B\cap C)}\frac{\Pr(B\cap C)}{\Pr(C)} 
		= \Pr(A\mid B\cap C)\Pr(B\mid C) 
	\]
	we can show:
	\begin{align*}
		&\Pr\left[(X_t,A_t,R_{t+1})\in\{(x,a)\}\times U \mid \cH_0^{t-1} \right] \\
		&=\Pr\left[R_{t+1}\in U \mid (X_t, A_t)=(x,a), \cH_0^{t-1}\right]
		\Pr\left[(X_t, A_t)=(x,a)\mid\cH_0^{t-1}\right]\\
		&\lxeq{(\ref{X,R Markov})}\Pr\left[R_{t+1}\in U \mid (X_t, A_t)=(x,a)\right]
		\underbracket{\Pr\left[A_t=a \mid X_t=x\right]}_{=\pi(a\mid x)}
		\Pr\left[X_t=x\mid \cH_{t-1}^{t-1}\right]\\
		&\lxeq{(*)}\Pr\left[R_{t+1}\in U \mid (X_t, A_t)=(x,a), \cH_{t-1}^{t-1}\right]
		\Pr\left[(X_t, A_t)=(x,a)\mid\cH_{t-1}^{t-1}\right]\\
		&=\Pr\left[(X_t,A_t,R_{t+1})\in\{(x,a)\}\times U \mid \cH_{t-1}^{t-1} \right]
	\end{align*}
	\((*)\) \emph{Some} of the History is irrelevant if all of the History is irrelevant. (\textcolor{red}{appendix?, TODO: time-homogeneous})
\end{proof} 
\begin{remark} Sometimes not all actions make sense in all states. A simple fix would be to set the immediate reward functions for those actions very low, or (if possible) redirect them to the closest possible action. \\
A more formal approach would be to introduce an additional mapping, which assigns the set of admissible actions to each state \(\cX\to\cP(\cA)\) and define \({\cX\times\cA}\coloneqq\{(x,a): x\in\cX, a\in f(x) \}\).

If there is just one admissible action in every state, the MDP is equivalent to a normal Markov Process. 
Since then there is a mapping \(f\colon\cX\to\cA\) mapping the state to  the only admissible action. Which implies that \(A_t=f(X_t)\) which forces every behavior to be equal to f. And since f is a deterministic stationary behavior, \ref{statPolicy induces time-homogeneous MC} applies.  
\end{remark}

\begin{definition}
An MDP together with a discount factor \(\gamma\in[0,1]\) is a
\par\begin{tabular}{l l}
	\emph{discounted reward} MDP & for \(\gamma <1 \)\\
	\emph{undiscounted reward} MDP & for \(\gamma=1 \)
\end{tabular}\\
This allows us to define the \emph{return}:
\[\cR\coloneqq\sum_{t=0}^{\infty}\gamma^t R_{t+1}\]
\end{definition}

\begin{definition}
Let \((Y_{(x,a)}, R_{(x,a)})\sim \cP_0(\cdot\mid x,a) \) be a random variable.
\[r(x,a)\coloneqq \E[R_{(x,a)}] \quad\text{ is the \emph{immediate reward function}.}\]
\end{definition}

\begin{definition}\(\cM=(\cX,\cA,\cP_0)\) a MDP\\
	\(x\in\cX\) is a \emph{terminal (absorbing)} state \(:\iff \forall s\in\N: \Pr(X_{t+s}=x\mid X_t=x)=1\)\\
	An MDP with such states is called \emph{episodic}.\\
	An \emph{episode} is the random time period \((1,\dots,T)\) until a terminal state is reached.
\end{definition}
\begin{remark}\leavevmode
	\begin{itemize}
		\item The reward in a terminal state is by convention zero, i.e. \(x\) terminal state implies \(\forall a\in\cA: R_{(x,a)}=0\).
		\item Episodic MDPs are often undiscounted
	\end{itemize}	
\end{remark}

%\begin{definition}(Markov Reward Process - MRP)
%TODO Markov Reward Process 
%\end{definition}

To avoid clutter we will from now on we assume an underlying MDP with the accompanying definitions and notation.

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Value functions}
%%%%%%%%%%%%%%%%%%%%%%%%%
The goal in this section is to
\begin{itemize}
\item define Value functions which assign states (and actions) a value, which allow the agent to make a more nuanced decisions than comparing immediate rewards of different actions
\item explore the relation of different value functions
\item show uniqueness of optimal value functions with the Banach fixpoint theorem, yielding a simple approximation methode along the way
\item demonstrate that in MDPs deterministic stationary policies are generally a large enough set of policies to choose from
 \end{itemize}
\begin{definition} Let \(\pi\) be a behavior. Select \(X_0\) such that \(\forall x\in\cX:\Pr({X_0=x})>0\), be \(((X_t,A_t,R_{t+1}), t\in \N_0)\) the resulting stoch. process.
\begin{align*}
	&V^\pi\colon
	\begin{cases}
		\cX\to\R\\
		x\mapsto \E[\cR\mid X_0=x]
	\end{cases} 
	&& \text{is the \emph{value function} for } \pi \footnotemark[1]
	\\
	&Q^\pi\colon
	\begin{cases}
		\cX\times\cA\to\R\\
		(x,a)\mapsto \E[\cR\mid X_0=x, A_0=a]
	\end{cases}
	&& \parbox{10em}{is the \emph{action value function} for \(\pi\) \footnotemark[2]}
	\\
	&V^*\colon
	\begin{cases}
		\cX\to\R\\
		x\mapsto \sup\limits_{\pi\in\Pi} V^\pi(x)
	\end{cases} 
	&& \text{is the \emph{optimal value function}}
	\\
	&Q^*\colon
	\begin{cases}
		\cX\times\cA\to\R\\
		(x,a)\mapsto \sup\limits_{\pi\in\Pi} Q^\pi(x,a)
	\end{cases}
	&& \parbox{10em}{is the \emph{optimal action value function}}
\end{align*}
\footnotetext[1]{Well defined because \(\Pr(X_0=x)>0\)}
\footnotetext[2]{Well defined because \(A_1\sim \pi_1(\cdot\mid (x,a,r_0), x_1)\) is defined for all \(a\) regardless of \(\pi_0\)}
\(\pi\) is \emph{optimal} \(:\iff V^*=V^\pi\)
\end{definition}


\begin{remark}
With the distribution of \(X_0\) set (or \(X_0\) being realized with a fixed value \(x\)), the distribution of \(X_t, A_t,R_{t+1}\) is determined for all \(t\in\N_0\). The conditional expectation is thus unique for a given \(X_0=x\), for all possible realizations of the MDP with a given behavior. \\
This means \(V^\pi, Q^\pi\) are well defined.
\end{remark}

As mentioned previously, the marginal probability distribution of the state instead of the joint distribution with the reward, will now makes some notation shorter.  
\begin{definition}
	\[
	p\colon 
	\begin{cases}
		(\cX\times\cA)\times\sigma_\cX\to\R\\
		(x,a,Y)\mapsto \cP_0(Y\times\R\mid x,a)
	\end{cases}\text{ is the \emph{\underline{state} transition kernel}.}
	\] 
\emph{Notation:} \(p(y\mid x,a)\coloneqq p(\{y\}\mid x,a)\) with \((x,a,y)\in\cX\times\cA\times\cX\)
\end{definition}

Now we can start to explore the relation of \(V^\pi\) and \(Q^\pi\).

\begin{prop}\label{expand Q^pi} Let \(\pi\in\detPolicy\) be a deterministic stationary behavior, then:
	\[Q^\pi(x,a)=r(x,a)+\gamma\sum_{y\in\cX}p(y\mid x,a)V^\pi(y)	\]
\end{prop}

\begin{proof} \textcolor{red}{split equation or accept eq sticking out a bit?}
\begin{align*}
Q^\pi(x,a) &= \E[\cR(\pi)\mid X_0=x, A_0=a]\\
&=\E[R_1(\pi)\mid X_0=x,A_0=a]+\gamma\E\left[\sum_{t=0}^\infty\gamma^t R_{t+2}(\pi)\middle| X_0=x,A_0=a\right]\\
&=\E[R_{(x,a)}] 
 + \gamma \sum_{y\in\cX}\E\left[\sum_{t=0}^\infty\gamma^t R_{t+2}(\pi) \middle| X_0=x,A_0=a, X_1=y\right]p(y\mid x, a)\\
&\lxeq{\text{Markov}} r(x,a)
 + \gamma\sum_{y\in\cX}\underbracket{\E\left[\sum_{t=0}^\infty\gamma^t R_{t+2}(\pi)\middle| X_1=y, A_1=\pi(y)\right]}_{
 \begin{aligned}
 	&= \E\left[\sum_{t=0}^\infty\gamma^t R_{t+2}(\pi)\middle| X_1=y \right] \\
 	&\lxeq{(*)} \E\left[\sum_{t=0}^\infty\gamma^t \tilde{R}_{t+1}(\pi)\middle| \tilde{X}_0=y \right]=V^\pi(y)
 \end{aligned}
 }
 p(y\mid x, a)
\end{align*}
\((*)\) Rename: \(\tilde{X}_{t}\coloneqq X_{t+1}, \tilde{A}_t\coloneqq A_{t+1},\tilde{R}_{t}\coloneqq R_{t+1}\), then \((\tilde{X}_t,\tilde{A}_t,\tilde{R}_{t+1}, t\in\N_0)\) is an \textcolor{red}{"evaluation"/ "Markov Action Process"} of the MDP with the (stationary!) policy \(\pi\).
\end{proof}

\begin{corollary}\label{V^pi,Q^pi relation} For \(\pi\in\detPolicy\) this fix-point equation holds: 
\begin{align*}
	V^\pi(x)&=Q^\pi(x,\pi(x))\\
	 &=r(x,\pi(x))+\gamma\sum\limits_{y\in\cX}p(y\mid x,\pi(x))V^\pi(y)
\end{align*}
and this fix-point equation:
\[
	Q^\pi(x,a)=r(x,a)+\gamma\sum_{y\in\cX}p(y\mid x,a)Q^\pi(x,\pi(x))
\]
\end{corollary}

\begin{proof}
Since \(\pi\) is a deterministic stationary policy:
\[V^\pi(x)=\E[\cR(\pi)\mid X_0=x]=\E[\cR(\pi)\mid X_0=x, A_0=\pi(x)]=Q^\pi(x,\pi(x))\]
The rest follows from \ref{expand Q^pi}
\end{proof}

With this relation we can use the Banach fix-point theorem (BFT) for the first time. But to do that we first need to make an assumption.
\begin{assumption} 
	\(\forall (x,a)\in\cX\times\cA:\quad\E[|R_{(x,a)}|]\le R\in\R\)
\end{assumption}
This implies that the immediate reward is bounded:
\[
	\|r\|_{\infty}=\sup_{(x,a)\in\cX\times\cA}|\E[R_{(x,a)}]|\le R
\]
But more importantly:
\[
	\E[|\cR|]\le\E\left[\sum_{t=0}^\infty \gamma^t |R_{t+1}|\right]
	\le \frac{R}{1-\gamma} 
\]
which implies implies \(V^\pi,V^* \in B(\cX), Q^\pi,Q^* \in B(\cX\times\cA)\) with \(B(M)\coloneqq \{f\colon M\to\R : \|f\|_\infty <\infty \} \) the set of bounded functions on \(\cX\), which happens to be a complete metric space which we will need for the Banach fix-point Theorem to work. Bounded Value functions will also be neccessary for a lot of other arguments later on. In finite MDPs this assumption is of course always fulfilled. 


\begin{definition} For policy \(\pi\in\detPolicy\) is the mapping \(T^\pi\colon B(\cX)\to B(\cX)\) with:
	\[
	T^\pi V(x)\coloneqq r(x,\pi(x))+\gamma\sum_{y\in\cX}p(y\mid x,\pi(x)) V(y)\qquad V\in B(\cX), x\in\cX
	\]
called the \emph{Bellman Operator}. With some abuse of notation, define the mapping \(T^\pi\colon B(\cX\times\cA)\to B(\cX\times\cA)\) with:
	\[
	T^\pi Q(x,a)\coloneqq r(x,a)+\gamma\sum_{y\in\cX}p(y\mid x,a) Q(y,\pi(y))\quad V\in B(\cX), (x,a)\in\cX\times\cA 
	\]
\end{definition}


\begin{remark}
	\(\forall\pi\in\detPolicy : T^\pi V^\pi=V^\pi\) (c.f. \ref{V^pi,Q^pi relation})

	\(T^\pi\) meets the requirements of the Banach fixed-point theorem \textcolor{red}{appendix?} for \({\gamma<1}\), this implies that \(V^\pi\) for \(\pi\in\detPolicy\)
	is a \emph{unique} fixpoint and can be approximated with the canonical iteration
\end{remark}

\begin{proof}
\((B(\cX), \|\cdot\|_\infty)\) is a non-empty, complete metric space \textcolor{red}{appendix?} and the mapping maps onto itself. It is left to show, that \(T^\pi\) is a contraction. Be \(V,W\in B(\cX)\):
\begin{align*}
	\|T^\pi V -T^\pi W\|_\infty &=\|\gamma\sum_{y\in\cX} p(y\mid \cdot,\pi(\cdot)) (V(y)-W(y))\|_\infty\\
	&\le\gamma \sup_{x\in\cX}\left\{ \sum_{y\in\cX} p(y\mid x,\pi(x)) \|V-W\|_\infty \right\}\\
	&=\gamma \|V-W\|_\infty  \sup_{x\in\cX}\underbracket{\left\{ \sum_{y\in\cX} p(y\mid x,\pi(x)) \right\}}_{=1}\\
	&=\gamma \|V-W\|_\infty
\end{align*}
The proof for \(T^\pi\colon B(\cX\times\cA)\to B(\cX\times\cA)\) is analogous.
\end{proof}

\begin{remark}\label{properties T^pi} Some observations which will come in useful later:
	\begin{enumerate}
		\item \(T^\pi\) is an affine operator
		\item \(W_1,W_2\in B(\cX)\), write \(W_1 \le W_2\) for \(\forall x\in\cX:W_1(x)\le W_2(x)\), then:
		\[W_1\le W_2 \implies T^\pi W_1\le T^\pi W_2\]
	\end{enumerate}
\end{remark}

\begin{proof}
 Be \(W_1,W_2\in B(\cX)\), \(W_1\le W_2\) and \(x\in\cX\):
\[
	T^\pi W_2 (x) - T^\pi W_1(x) 
	= \gamma \sum_{y\in\cX} p(y\mid x,\pi(x)) \underbracket{(W_2(y)-W_1(y))}_{\ge 0} 
	\ge 0
	\qedhere
\]
\end{proof}

Now we get to the more interesting but also harder optimal value functions. We will later see that taking the supremum over all behaviors is the same as taking it just over the deterministic stationary behaviors. But for now we need to make the distinction:

\begin{definition}
\begin{align*}
	\tilde{V}(x)&\coloneqq \sup_{\pi\in\detPolicy} V^\pi(x)\\
	\tilde{Q}(x,a)&\coloneqq \sup_{\pi\in\detPolicy} Q^\pi(x,a)
\end{align*}
\end{definition}

The goal is to show that these two pairs of optimal value functions are actually the same using the uniquness of the fix-point of the BFT. 

The intuition for why this should be the case comes from the fact that the MDP is memory-less. So winding up in the same state results in the agent having the exact same decision problem. And if the agent decided for one optimal action in the past, then this action will again be optimal. For this reason the supremum over all behaviors should be equal to the supremum over stationary behaviors. 

Deterministic policies are enough, because if an optimal policy randomizes over different actions, then the values of theses actions must all be equally high. Which means that just picking one and sticking with it should be just as good.

But before we get to tackle this problem, we first need to adress another one. Notice how we take the supremum for every state \(x\) individually? 
\begin{equation*}
\tilde{V}(x)=\sup_{\pi\in\detPolicy}V^\pi(x) 
\end{equation*}
Since the stationary policy still allows us to condition on the state it is intuitive to assume that we do not need different sequences of policies to apporximate \(V^*\) for each state \(x\in\cX\). This will eventually turn out to be true using statements about the optimal value functions. We can not show this fact right now, since sequences approximating the different suprema are indexed by the possibly countable state space, what we would need is a single sequence of policies which matches all the policies in this countable set of sequences in every state. This turns out to be an impossible task.

\begin{example} (The genie cubicles)
Imagine an infinite (countable) amount of cubicles \(\N \subset \cX\), with a genie in every cubicle. You can wish for an arbitrary amount of money \(\cA=\N\), after that you have to leave (end up in the terminal state \(0\), i.e.: \(\cX=\N_0\)). Then of course \(V^*(x)=\infty\) for \(x\in\N\), is achived by the seqences of behaviors: \((\pi_x^{(n)}, n\in\N)\) for \(x\in\N\) with \(\pi_x^{(n)}(y)=x+n\quad \forall y\in\cX\). Then there is no policy \(\pi^{(n)}\) which can match every \(\pi_x^{(n)}\) for all \(x\in\N\).

Even if \(\tilde{V}\) was finite, we could modify the example by cutting gifts off above a certain threshold with behaviors approaching that threshold. 
\end{example}

So we can not match an infinite set of behaviors. Which means we will have to settle for a finite version. This version will help us to handle the optimal value functions. With the later facts about optimal value functions we can then define a sequence of policies which approach the optimal value functions uniformly confirming our earlier suspicions, that the suprema can be attained with a single sequence of policies. 

\begin{prop}\label{finite outmatching}
	Be \(n\in\N\) and \(\{\pi_1,\dots,\pi_n\}\subseteq \detPolicy \), then:
	\[\exists \hat{\pi}\in\detPolicy: \quad \max_{i=1,\dots,n} V^\pi(x)\le V^{\hat{\pi}}(x) \qquad \forall x\in\cX \]
\end{prop}
\begin{proof}
	The idea is to to pick the same action in state \(x\), as the policy which generates the most value out of this state, i.e. \(\max_{i=1,\dots,n}V^{\pi_i}(x)\). 
	\[
		\hat{\pi}\colon
	\begin{cases} 
		\cX\to\cA\\
		x\mapsto \pi_{\mathop{\text{argmax}}\limits_{i=1,\dots,n}V^{\pi_i}(x)}(x)
	\end{cases}
	\]
	One might be surprised that such an appearingly short sighted policy should surpass every policy in the finite set. At first it might seem that different policies achive their values of their state through different, maybe incompatible strategies. Would a strategy which takes a policy because it changes transition probabilities in a way that leads to a high-payoff state not be sabotaged by switching policies erratically?
	It is important to realize that if a policy A attaches a high value to a state because it provides easy access to states which can yield a high payoff, then the other policies will either exploit the high payoff later on as well, or the policy A will once again be the maximum. Either way it would result in \(\hat{\pi}\) exploiting the high payoff later on. 

	For the maximum value write: \(V(x)\coloneqq \max_{i=1,\dots, n}V^{\pi_i}(x)\) and be \(m(x) \coloneqq \arg\max_{i=1,\dots,n}V^{\pi_i}(x)\), then:
	\begin{align*}
		&T^{\hat{\pi}}V(x)=r(x,\pi_{m(x)}(x))
		+\gamma \sum{y\in\cX} p(y\mid x,\pi_{m(x)}(x))V(y)\\
		&\lxgq{V\ge V^{\pi_{m(x)}}} r(x,\pi_{m(x)}(x))
		+\gamma \sum{y\in\cX} p(y\mid x,\pi_{m(x)}(x))V^{\pi_{m(x)}}(y)\\
		&\lxeq{\ref{V^pi,Q^pi relation}} V^{\pi_{m(x)}}(x)=V(x)
	\end{align*}
	By using the monotonicity of \(T^{\hat{\pi}} \) (\ref{properties T^pi}) inductively with \((T^{\hat{\pi}})^1 V \ge (T^{\hat{\pi}})^0 V \), we get
	\( (T^{\hat{\pi}})^n V \ge (T^{\hat{\pi}})^{n-1} V \) thus:
	\[
		V^{\hat{\pi}}(x) = \lim_{n\to\infty} (T^{\hat{\pi}})^n V(x) 
		\ge V(x) \ge V^{\pi_i}(x)
		\qquad \forall i=1,\dots,n, \forall x\in\cX
		\qedhere
	\] 
\end{proof}

To prove that the value functions \(V^*\) (\(Q^*\)) and \(\tilde{V}\) (\(\tilde{Q}\))  are equal we need to show that they are both fix points of the following mapping. 
And that this mapping satisfies the requirements of the BFT. The value of stochastic stationary behaviors will be sandwiched in between, thus also equal. 

\begin{definition}
The mapping \(T^*\colon B(\cX)\to B(\cX)\) with:
	\[
	 T^* V(x)\coloneqq \sup_{a\in\cA}\left\{r(x,a)+\sum_{y\in\cX}p(y\mid x,a) V(y)\right\} \qquad V\in B(\cX), x\in\cX
	\]
is called the \emph{Bellman Optimality Operator}.
\end{definition}

We will be able to use the established relation of \(V^\pi\) and \(Q^\pi\) in case of deterministic stationary policies
It is quite important to remember that this relation is only defined for these policies though. So we need a different approach for the general behaviors. The advantage of these is, that one can condition on a finer level on previous states which will allow us to swap places of suprema and sums.


\begin{lemma}\label{V*,Q* relation}\(\cM=(\cX,\cA,\cP_0)\) MDP
\begin{enumerate}[label=\textbf{(\roman*)},font=\normalfont]
\item\label{i:1} \(\tilde{Q}(x,a)= r(x,a)+\gamma\sum\limits_{y\in\cX}p(y\mid x,a)\tilde{V}(y)\)
\item\label{i:2} \(\tilde{V}(x)= \sup\limits_{a\in\cA} \tilde{Q}(x,a)\)
\item\label{i:3} \(V^*(x)=\sup\limits_{a\in\cA}Q^*(x,a)\)
\item\label{i:4} \(Q^*(x,a)=r(x,a)+\gamma \sum\limits_{y\in\cX}p(y\mid x,a) V^*(y)\)
\end{enumerate}
\end{lemma}

\begin{proof} \ref{i:1} The smaller or equal part is easy:
	\begin{align*}
		\tilde{Q}(x,a)&= \sup_{\pi \in \detPolicy} \left\{r(x,a)
		+\gamma\sum_{y\in\cX}p(y\mid x,a) V^\pi(y)\right\}\\
		&\le r(x,a) +\gamma\sum_{y\in\cX}p(y\mid x,a) \underbracket{\sup_{\pi \in \detPolicy}V^\pi(y)}_{=\tilde{V}(y)}
	\end{align*}
	For the other direction we need to do a bit more work. Since the \(r(x,a)\) and \(\gamma\), are unaffected by the supremum what is left to show is:
	\[
		\sup_{\pi \in \detPolicy}\sum_{y\in\cX}p(y\mid x,a) V^\pi(y)
		\ge\sum_{y\in\cX}p(y\mid x,a)\tilde{V}(y)
	\]
	The set \(M_\delta\coloneqq \{y\in\cX : p(y\mid x,a)>\delta \} \) is finite for all \(\delta>0\), and
	\begin{align*}
		1&=p(\cX\mid x,a)=p\left(\bigcup_{\delta \to 0} M_\delta \mid x, a \right)
		= \lim_{\delta\to 0} p(M_\delta\mid x,a) \\
		&=\lim_{\delta\to 0} \sum_{y\in M_\delta} p(y\mid x,a)
	\end{align*}
	Therefore for all \(\varepsilon >0\) exists a \(\delta>0\) such that:
	\begin{align}
		\sum_{y\in M_\delta^\complement}p(y\mid x,a) < \frac{\varepsilon /4}{R/(1-\gamma)}
		\label{Mcomplement}
	\end{align}
	Be \((\pi_y^{(n)},n\in\N )\) with \(V^{\pi_y^{(n)}}(y) \nearrow \tilde{V}(y) \quad (n\to\infty) \). Since \(M_\delta\) is finite, there exists \(N\in\N \) such that:
	\begin{align}
		|\tilde{V}(y) -V^{\pi_y^{(n)}}(y)|< \varepsilon/2
		\qquad \forall n\ge N, \forall y\in M_\delta^\complement
		\label{convergenceToTilde}
	\end{align}
	And also because \(M_\delta \) is finite and \ref{finite outmatching} we know:
	\begin{align}
	\exists\hat{\pi}^{(n)}\in\detPolicy : 
	\quad {V^{\hat{\pi}^{(n)}}\ge V^{\pi_y^{(n)}}}
	\qquad \forall y\in M_\delta
	\label{piHat over finite set}
	\end{align}
	This finally implies:
	\begin{align*}
		&\sum_{y\in\cX}p(y\mid x,a)\tilde{V}(y) 
		- \sum_{y\in\cX}p(y\mid x,a) V^{\hat{\pi}^{(n)}}(y) \\
		&\le \sum_{y\in M_\delta}p(y\mid x,a) (\tilde{V}(y) - V^{\hat{\pi}^{(n)}}(y))
		+ \sum_{y\in M_\delta^\complement}p(y\mid x,a) 
		\underbracket{|\tilde{V}(y) - V^{\hat{\pi}^{(n)}}(y)|}_{
			\le \|\tilde{V}\|_\infty + \| V^{\hat{\pi}^{(n)}}\|_\infty
		} \\
		&\lxlq{(\ref{piHat over finite set})} \sum_{y\in M_\delta}p(y\mid x,a) 
		\underbracket{(\tilde{V}(y) - V^{\pi_y^{(n)}}(y))}_{
			<\varepsilon/2\quad(\ref{convergenceToTilde})
		}
		+ 2R/(1-\gamma) \underbracket{\sum_{y\in M_\delta^\complement}p(y\mid x,a)}_{
			< \frac{\varepsilon /4}{R/(1-\gamma)}\quad(\ref{Mcomplement})
		}\\
		&\le \varepsilon \qquad \forall n\ge N
	\end{align*}
	This results in:
	\begin{align*}
		\sum_{y\in\cX}p(y\mid x,a)\tilde{V}(y) 
		&\le \varepsilon + \sum_{y\in\cX}p(y\mid x,a) V^{\hat{\pi}^{(n)}}(y) \\
		&\le \varepsilon + \sum_{y\in\cX}p(y\mid x,a) \tilde{V}(y)
	\end{align*}
	This implies:
	\begin{align*}
		\sup
	\end{align*}
	\\
	\ref{i:2}
	By \ref{V^pi,Q^pi relation} we know \(V^\pi(x)=Q^\pi(x,\pi(x))\) thus:
	\begin{align}
		\tilde{V}(x)& =\sup_{\pi \in \detPolicy} V^\pi(x)
		=\sup_{\pi \in \detPolicy}Q^\pi(x,\pi(x)) 
		\nonumber\\
		&\le\sup_{a\in \cA} \sup_{\pi\in\detPolicy} Q^\pi(x,a)
		=\sup_{a\in \cA} \tilde{Q}(x,a)
		\label{inequality}
		\\
		%&= \sup_{a\in \cA} \sup_{\pi\in\detPolicy} \left\{r(x,a)+\gamma\sum\limits_{y\in\cX}p(y\mid x,a) V^\pi(y) \right\}
		%\nonumber\\
		&\lxeq{\ref{i:1}} \sup_{a\in \cA}\left\{ r(x,a)+\gamma\sum\limits_{y\in\cX}p(y\mid x,a) 
		\sup_{\pi\in\detPolicy} V^\pi(y) \right\} \label{last eq}
		%\\
		%&=\sup_{a\in \cA}\left\{ r(x,a)+\gamma\sum\limits_{y\in\cX}p(y\mid x,a) 
		% \tilde{V}(y) \right\}
	\end{align}
	Assume (\ref{inequality}) is a true inequality for some \(x\in\cX\), since the suprema in (\ref{last eq}) can be arbitrarily closely approximated:
	\[ \exists \pi, \exists a : \tilde{V}(x) < r(x,a) + \gamma\sum_{y\in\cX} p(y\mid x,a) V^\pi(y)\]
	Define a slightly changed deterministic policy with this \(\pi\) and \(a\):
	\begin{align*}
		\hat{\pi}\colon
		\begin{cases}
			\cX\to\cA\\
			y\mapsto
			\begin{cases}
				\pi(y) & y\neq x\\
				a & y=x
			\end{cases}
		\end{cases}
	\end{align*}
	Define \(W_n\coloneqq (T^{\hat{\pi}})^n V^\pi\), then:
	\begin{align*}
		W_1(y)&=T^{\hat{\pi}}V^\pi(y) \xeq{y\neq x} T^\pi V^\pi(y) = V^\pi(y)\\
		&\lxeq{y=x} r(x, \hat{\pi}(x)) 
		+ \gamma \sum_{z\in\cX} p(z \mid x, \hat{\pi}(x)) V^\pi(z)  \\
		&=r(x, a) + \gamma \sum_{z\in\cX} p(z \mid x, a) V^\pi(z) \\
		&> \tilde{V}(x) \ge V^\pi(x)
	\end{align*}
	In either case we get \(W_1(y)\ge V^\pi(y)=W_0(y) \).
	By induction with \ref{properties T^pi} we get: \(W_{n+1}=T^{\hat{\pi}}W_n \ge T^{\hat{\pi}}W_{n-1}=W_n\), thus:
	\begin{align*}
		V^{\hat{\pi}}(x)&=\lim_{n\to\infty}(T^{\hat{\pi}})^n V^\pi(x)=\lim_{n\to\infty} W_n(x)\ge W_1(x) \\
		&=r(x, a) + \gamma \sum_{z\in\cX} p(z \mid x, a) V^\pi(z) \\
		&> \tilde{V}(x) \contradiction{\hat{\pi}\in\detPolicy}
	\end{align*}
%TODO
\end{proof}


\begin{corollary}
	\begin{align*}
	&T^*\tilde{V}=\tilde{V}\\
	&T^*V^*=V^*
	\end{align*}
\end{corollary}

\begin{proof}
\begin{align*}
	V^*(x)\xeq{\ref{i:3}}\sup_{a\in\cA}Q^*(x,a)\xeq{\ref{i:4}}\sup_{a\in\cA}\left\{r(x,a)+\sum_{y\in\cX}p(y\mid x,a) V^*(y)\right\} =T^*V^*(x)
\end{align*}
\(\tilde{V}\) analogous
\end{proof}


\begin{thm}\(\cM=(\cX,\cA,\cP_0)\) MDP\\
\(T^*\) satisfies the requirements of the Banach fixpoint theorem, in particular:
	\[V^*(x)=\sup_{\pi\in\statPolicy}V^\pi(x)=\tilde{V}(x) \]
is the unique fixpoint of \(T^*\)
\end{thm}

\begin{lemma}(Blackwell's condition for contraction)
\end{lemma}

\begin{proof}
https://math.stackexchange.com/questions/1087885/blackwells-condition-for-a-contraction-why-is-boundedness-neccessary?rq=1
\end{proof}

\begin{proof}[Proof (Theorem)]
\end{proof}


\begin{prop}\label{sup is attained}\(\cM=(\cX,\cA,\cP_0)\) MDP\\
The following statements are equivalent:
\begin{enumerate}[label={(\roman*)},font=\normalfont]
\item \(\pi \in\statPolicy\) is optimal (\(V^*=V^\pi\))
\item \(\forall x\in\cX: V^*(x)=\sum\limits_{a\in\cA}\pi(a\mid x)Q^*(x,a)\)
\item\label{ii:3} \(\forall x\in\cX: \pi=\arg\max\limits_{\pi\in\statPolicy}\sum\limits_{a\in\cA}\pi(a\mid x)Q^*(x,a) \)
\item \(\pi(a\mid x)>0 \iff Q^*(x,a)=V^*(x) =\sup\limits_{b\in\cA}Q*(x,b)\) \\
	``actions are concentrated on the set of actions that maximize \(Q^*(x,\cdot)\)''\\
	(this also implies: \(Q^*(x,a)<V^*(x) \implies \pi(a\mid x)=0\))
\end{enumerate}
\end{prop}

\begin{proof}
%TODO
\end{proof}


\begin{definition}
	\(Q\colon\cX\times\cA\to\R\) an action value function, \(\tilde{\pi}\colon\cX\to\cA\) with:
	\[
	\tilde{\pi}(x)\coloneqq\arg\max_{\pi\in\statPolicy}\sum_{a\in\cA}\pi(a\mid x) Q(x,a)\qquad x\in\cX
	\]
	\(\tilde{\pi}(x)\) is called \emph{greedy} with respect to Q in \(x\in\cX\)\\
	\(\tilde{\pi}\) is called \emph{greedy} w.r.t. Q
\end{definition}


\begin{remark}\leavevmode
	\begin{itemize}
	\item \ref{sup is attained}\ref{ii:3} implies that greedy w.r.t. \(Q^*\) is optimal. 
	This means that knowledge of \(Q^*\) is sufficient to select the best action.
	\item \ref{V*,Q* relation} implies that knowledge of \(V^*,r,p\) is sufficient as well.
	\end{itemize}
\end{remark}
\endinput
